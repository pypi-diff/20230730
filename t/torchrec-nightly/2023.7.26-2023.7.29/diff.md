# Comparing `tmp/torchrec_nightly-2023.7.26-py38-none-any.whl.zip` & `tmp/torchrec_nightly-2023.7.29-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,150 +1,150 @@
-Zip file size: 371083 bytes, number of entries: 148
--rw-r--r--  2.0 unx      811 b- defN 23-Jul-26 11:17 torchrec/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 23-Jul-26 11:17 torchrec/streamable.py
--rw-r--r--  2.0 unx      854 b- defN 23-Jul-26 11:17 torchrec/types.py
--rw-r--r--  2.0 unx     1153 b- defN 23-Jul-26 11:17 torchrec/datasets/__init__.py
--rw-r--r--  2.0 unx    41469 b- defN 23-Jul-26 11:17 torchrec/datasets/criteo.py
--rw-r--r--  2.0 unx     4548 b- defN 23-Jul-26 11:17 torchrec/datasets/movielens.py
--rw-r--r--  2.0 unx     6539 b- defN 23-Jul-26 11:17 torchrec/datasets/random.py
--rw-r--r--  2.0 unx    10909 b- defN 23-Jul-26 11:17 torchrec/datasets/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 11:17 torchrec/datasets/scripts/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 23-Jul-26 11:17 torchrec/datasets/scripts/contiguous_preproc_criteo.py
--rw-r--r--  2.0 unx     2847 b- defN 23-Jul-26 11:17 torchrec/datasets/scripts/npy_preproc_criteo.py
--rw-r--r--  2.0 unx     3077 b- defN 23-Jul-26 11:17 torchrec/datasets/scripts/shuffle_preproc_criteo.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 11:17 torchrec/datasets/test_utils/__init__.py
--rw-r--r--  2.0 unx     5308 b- defN 23-Jul-26 11:17 torchrec/datasets/test_utils/criteo_test_utils.py
--rw-r--r--  2.0 unx     1912 b- defN 23-Jul-26 11:17 torchrec/distributed/__init__.py
--rw-r--r--  2.0 unx    37307 b- defN 23-Jul-26 11:17 torchrec/distributed/batched_embedding_kernel.py
--rw-r--r--  2.0 unx     2069 b- defN 23-Jul-26 11:17 torchrec/distributed/collective_utils.py
--rw-r--r--  2.0 unx     4988 b- defN 23-Jul-26 11:17 torchrec/distributed/comm.py
--rw-r--r--  2.0 unx    56136 b- defN 23-Jul-26 11:17 torchrec/distributed/comm_ops.py
--rw-r--r--  2.0 unx    37051 b- defN 23-Jul-26 11:17 torchrec/distributed/dist_data.py
--rw-r--r--  2.0 unx    32295 b- defN 23-Jul-26 11:17 torchrec/distributed/embedding.py
--rw-r--r--  2.0 unx     4947 b- defN 23-Jul-26 11:17 torchrec/distributed/embedding_kernel.py
--rw-r--r--  2.0 unx    29185 b- defN 23-Jul-26 11:17 torchrec/distributed/embedding_lookup.py
--rw-r--r--  2.0 unx    19035 b- defN 23-Jul-26 11:17 torchrec/distributed/embedding_sharding.py
--rw-r--r--  2.0 unx    36853 b- defN 23-Jul-26 11:17 torchrec/distributed/embedding_tower_sharding.py
--rw-r--r--  2.0 unx    15030 b- defN 23-Jul-26 11:17 torchrec/distributed/embedding_types.py
--rw-r--r--  2.0 unx    37369 b- defN 23-Jul-26 11:17 torchrec/distributed/embeddingbag.py
--rw-r--r--  2.0 unx     7373 b- defN 23-Jul-26 11:17 torchrec/distributed/fbgemm_qcomm_codec.py
--rw-r--r--  2.0 unx     6137 b- defN 23-Jul-26 11:17 torchrec/distributed/fp_embeddingbag.py
--rw-r--r--  2.0 unx     5243 b- defN 23-Jul-26 11:17 torchrec/distributed/fused_embedding.py
--rw-r--r--  2.0 unx     5080 b- defN 23-Jul-26 11:17 torchrec/distributed/fused_embeddingbag.py
--rw-r--r--  2.0 unx     2271 b- defN 23-Jul-26 11:17 torchrec/distributed/fused_params.py
--rw-r--r--  2.0 unx     3807 b- defN 23-Jul-26 11:17 torchrec/distributed/grouped_position_weighted.py
--rw-r--r--  2.0 unx    10997 b- defN 23-Jul-26 11:17 torchrec/distributed/mc_embeddingbag.py
--rw-r--r--  2.0 unx    19750 b- defN 23-Jul-26 11:17 torchrec/distributed/model_parallel.py
--rw-r--r--  2.0 unx    20748 b- defN 23-Jul-26 11:17 torchrec/distributed/quant_embedding.py
--rw-r--r--  2.0 unx    15112 b- defN 23-Jul-26 11:17 torchrec/distributed/quant_embedding_kernel.py
--rw-r--r--  2.0 unx    12621 b- defN 23-Jul-26 11:17 torchrec/distributed/quant_embeddingbag.py
--rw-r--r--  2.0 unx    13928 b- defN 23-Jul-26 11:17 torchrec/distributed/quant_state.py
--rw-r--r--  2.0 unx     9261 b- defN 23-Jul-26 11:17 torchrec/distributed/shard.py
--rw-r--r--  2.0 unx    19646 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding_plan.py
--rw-r--r--  2.0 unx    37840 b- defN 23-Jul-26 11:17 torchrec/distributed/train_pipeline.py
--rw-r--r--  2.0 unx    27865 b- defN 23-Jul-26 11:17 torchrec/distributed/types.py
--rw-r--r--  2.0 unx    15470 b- defN 23-Jul-26 11:17 torchrec/distributed/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 11:17 torchrec/distributed/composable/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 23-Jul-26 11:17 torchrec/distributed/composable/table_batched_embedding_slice.py
--rw-r--r--  2.0 unx     1025 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/__init__.py
--rw-r--r--  2.0 unx     3135 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/constants.py
--rw-r--r--  2.0 unx    11430 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/enumerators.py
--rw-r--r--  2.0 unx    12642 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/partitioners.py
--rw-r--r--  2.0 unx      835 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/perf_models.py
--rw-r--r--  2.0 unx    13530 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/planners.py
--rw-r--r--  2.0 unx    11236 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/proposers.py
--rw-r--r--  2.0 unx    41291 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/shard_estimators.py
--rw-r--r--  2.0 unx    23791 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/stats.py
--rw-r--r--  2.0 unx     9643 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/storage_reservations.py
--rw-r--r--  2.0 unx    14582 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/types.py
--rw-r--r--  2.0 unx     1119 b- defN 23-Jul-26 11:17 torchrec/distributed/planner/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/__init__.py
--rw-r--r--  2.0 unx     2479 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/cw_sequence_sharding.py
--rw-r--r--  2.0 unx    12945 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/cw_sharding.py
--rw-r--r--  2.0 unx     2802 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/dp_sequence_sharding.py
--rw-r--r--  2.0 unx     7681 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/dp_sharding.py
--rw-r--r--  2.0 unx     7640 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/rw_sequence_sharding.py
--rw-r--r--  2.0 unx    17911 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/rw_sharding.py
--rw-r--r--  2.0 unx     3627 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/sequence_sharding.py
--rw-r--r--  2.0 unx     7632 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/tw_sequence_sharding.py
--rw-r--r--  2.0 unx    16097 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/tw_sharding.py
--rw-r--r--  2.0 unx     1284 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/twcw_sharding.py
--rw-r--r--  2.0 unx    19871 b- defN 23-Jul-26 11:17 torchrec/distributed/sharding/twrw_sharding.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 11:17 torchrec/distributed/test_utils/__init__.py
--rw-r--r--  2.0 unx    11237 b- defN 23-Jul-26 11:17 torchrec/distributed/test_utils/infer_utils.py
--rw-r--r--  2.0 unx     4868 b- defN 23-Jul-26 11:17 torchrec/distributed/test_utils/multi_process.py
--rw-r--r--  2.0 unx    34091 b- defN 23-Jul-26 11:17 torchrec/distributed/test_utils/test_model.py
--rw-r--r--  2.0 unx    11197 b- defN 23-Jul-26 11:17 torchrec/distributed/test_utils/test_model_parallel.py
--rw-r--r--  2.0 unx    25310 b- defN 23-Jul-26 11:17 torchrec/distributed/test_utils/test_model_parallel_base.py
--rw-r--r--  2.0 unx    15367 b- defN 23-Jul-26 11:17 torchrec/distributed/test_utils/test_sharding.py
--rw-r--r--  2.0 unx      422 b- defN 23-Jul-26 11:17 torchrec/fx/__init__.py
--rw-r--r--  2.0 unx     6451 b- defN 23-Jul-26 11:17 torchrec/fx/tracer.py
--rw-r--r--  2.0 unx     4524 b- defN 23-Jul-26 11:17 torchrec/fx/utils.py
--rw-r--r--  2.0 unx     1223 b- defN 23-Jul-26 11:17 torchrec/inference/__init__.py
--rw-r--r--  2.0 unx     3614 b- defN 23-Jul-26 11:17 torchrec/inference/client.py
--rw-r--r--  2.0 unx     3957 b- defN 23-Jul-26 11:17 torchrec/inference/model_packager.py
--rw-r--r--  2.0 unx     8068 b- defN 23-Jul-26 11:17 torchrec/inference/modules.py
--rw-r--r--  2.0 unx     3797 b- defN 23-Jul-26 11:17 torchrec/inference/state_dict_transform.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 11:17 torchrec/metrics/__init__.py
--rw-r--r--  2.0 unx     4168 b- defN 23-Jul-26 11:17 torchrec/metrics/accuracy.py
--rw-r--r--  2.0 unx    12549 b- defN 23-Jul-26 11:17 torchrec/metrics/auc.py
--rw-r--r--  2.0 unx     3703 b- defN 23-Jul-26 11:17 torchrec/metrics/calibration.py
--rw-r--r--  2.0 unx     3465 b- defN 23-Jul-26 11:17 torchrec/metrics/ctr.py
--rw-r--r--  2.0 unx     3836 b- defN 23-Jul-26 11:17 torchrec/metrics/mae.py
--rw-r--r--  2.0 unx    17990 b- defN 23-Jul-26 11:17 torchrec/metrics/metric_module.py
--rw-r--r--  2.0 unx     6796 b- defN 23-Jul-26 11:17 torchrec/metrics/metrics_config.py
--rw-r--r--  2.0 unx     3731 b- defN 23-Jul-26 11:17 torchrec/metrics/metrics_namespace.py
--rw-r--r--  2.0 unx     3904 b- defN 23-Jul-26 11:17 torchrec/metrics/model_utils.py
--rw-r--r--  2.0 unx     4631 b- defN 23-Jul-26 11:17 torchrec/metrics/mse.py
--rw-r--r--  2.0 unx     5605 b- defN 23-Jul-26 11:17 torchrec/metrics/multiclass_recall.py
--rw-r--r--  2.0 unx     8735 b- defN 23-Jul-26 11:17 torchrec/metrics/ndcg.py
--rw-r--r--  2.0 unx     6811 b- defN 23-Jul-26 11:17 torchrec/metrics/ne.py
--rw-r--r--  2.0 unx    33554 b- defN 23-Jul-26 11:17 torchrec/metrics/rec_metric.py
--rw-r--r--  2.0 unx    10490 b- defN 23-Jul-26 11:17 torchrec/metrics/recall_session.py
--rw-r--r--  2.0 unx     6057 b- defN 23-Jul-26 11:17 torchrec/metrics/throughput.py
--rw-r--r--  2.0 unx    11160 b- defN 23-Jul-26 11:17 torchrec/metrics/tower_qps.py
--rw-r--r--  2.0 unx     2867 b- defN 23-Jul-26 11:17 torchrec/metrics/weighted_avg.py
--rw-r--r--  2.0 unx    16441 b- defN 23-Jul-26 11:17 torchrec/metrics/test_utils/__init__.py
--rw-r--r--  2.0 unx      913 b- defN 23-Jul-26 11:17 torchrec/models/__init__.py
--rw-r--r--  2.0 unx    11410 b- defN 23-Jul-26 11:17 torchrec/models/deepfm.py
--rw-r--r--  2.0 unx    29965 b- defN 23-Jul-26 11:17 torchrec/models/dlrm.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 11:17 torchrec/models/experimental/__init__.py
--rw-r--r--  2.0 unx     9825 b- defN 23-Jul-26 11:17 torchrec/models/experimental/test_transformerdlrm.py
--rw-r--r--  2.0 unx     7434 b- defN 23-Jul-26 11:17 torchrec/models/experimental/transformerdlrm.py
--rw-r--r--  2.0 unx     1179 b- defN 23-Jul-26 11:17 torchrec/modules/__init__.py
--rw-r--r--  2.0 unx     1456 b- defN 23-Jul-26 11:17 torchrec/modules/activation.py
--rw-r--r--  2.0 unx    14636 b- defN 23-Jul-26 11:17 torchrec/modules/crossnet.py
--rw-r--r--  2.0 unx     8415 b- defN 23-Jul-26 11:17 torchrec/modules/deepfm.py
--rw-r--r--  2.0 unx     6468 b- defN 23-Jul-26 11:17 torchrec/modules/embedding_configs.py
--rw-r--r--  2.0 unx    14021 b- defN 23-Jul-26 11:17 torchrec/modules/embedding_modules.py
--rw-r--r--  2.0 unx     4858 b- defN 23-Jul-26 11:17 torchrec/modules/embedding_tower.py
--rw-r--r--  2.0 unx    12360 b- defN 23-Jul-26 11:17 torchrec/modules/feature_processor.py
--rw-r--r--  2.0 unx     4832 b- defN 23-Jul-26 11:17 torchrec/modules/feature_processor_.py
--rw-r--r--  2.0 unx     4403 b- defN 23-Jul-26 11:17 torchrec/modules/fp_embedding_modules.py
--rw-r--r--  2.0 unx    31545 b- defN 23-Jul-26 11:17 torchrec/modules/fused_embedding_modules.py
--rw-r--r--  2.0 unx    10533 b- defN 23-Jul-26 11:17 torchrec/modules/lazy_extension.py
--rw-r--r--  2.0 unx     3143 b- defN 23-Jul-26 11:17 torchrec/modules/managed_collision_modules.py
--rw-r--r--  2.0 unx     5124 b- defN 23-Jul-26 11:17 torchrec/modules/mc_embedding_modules.py
--rw-r--r--  2.0 unx     6309 b- defN 23-Jul-26 11:17 torchrec/modules/mlp.py
--rw-r--r--  2.0 unx     3897 b- defN 23-Jul-26 11:17 torchrec/modules/utils.py
--rw-r--r--  2.0 unx     1639 b- defN 23-Jul-26 11:17 torchrec/optim/__init__.py
--rw-r--r--  2.0 unx     2012 b- defN 23-Jul-26 11:17 torchrec/optim/apply_optimizer_in_backward.py
--rw-r--r--  2.0 unx     1569 b- defN 23-Jul-26 11:17 torchrec/optim/clipping.py
--rw-r--r--  2.0 unx     1353 b- defN 23-Jul-26 11:17 torchrec/optim/fused.py
--rw-r--r--  2.0 unx    16069 b- defN 23-Jul-26 11:17 torchrec/optim/keyed.py
--rw-r--r--  2.0 unx     4420 b- defN 23-Jul-26 11:17 torchrec/optim/optimizers.py
--rw-r--r--  2.0 unx     7405 b- defN 23-Jul-26 11:17 torchrec/optim/rowwise_adagrad.py
--rw-r--r--  2.0 unx     4865 b- defN 23-Jul-26 11:17 torchrec/optim/warmup.py
--rw-r--r--  2.0 unx      560 b- defN 23-Jul-26 11:17 torchrec/optim/test_utils/__init__.py
--rw-r--r--  2.0 unx     1140 b- defN 23-Jul-26 11:17 torchrec/quant/__init__.py
--rw-r--r--  2.0 unx    26618 b- defN 23-Jul-26 11:17 torchrec/quant/embedding_modules.py
--rw-r--r--  2.0 unx     4292 b- defN 23-Jul-26 11:17 torchrec/quant/utils.py
--rw-r--r--  2.0 unx     1163 b- defN 23-Jul-26 11:17 torchrec/sparse/__init__.py
--rw-r--r--  2.0 unx    56412 b- defN 23-Jul-26 11:17 torchrec/sparse/jagged_tensor.py
--rw-r--r--  2.0 unx     1430 b- defN 23-Jul-26 11:17 torchrec/sparse/test_utils/__init__.py
--rw-r--r--  2.0 unx     5661 b- defN 23-Jul-26 11:17 torchrec/test_utils/__init__.py
--rw-r--r--  2.0 unx     1530 b- defN 23-Jul-26 11:22 torchrec_nightly-2023.7.26.dist-info/LICENSE
--rw-r--r--  2.0 unx     5012 b- defN 23-Jul-26 11:22 torchrec_nightly-2023.7.26.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-Jul-26 11:22 torchrec_nightly-2023.7.26.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-Jul-26 11:22 torchrec_nightly-2023.7.26.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    13823 b- defN 23-Jul-26 11:22 torchrec_nightly-2023.7.26.dist-info/RECORD
-148 files, 1526782 bytes uncompressed, 348973 bytes compressed:  77.1%
+Zip file size: 372383 bytes, number of entries: 148
+-rw-r--r--  2.0 unx      811 b- defN 23-Jul-29 11:17 torchrec/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 23-Jul-29 11:17 torchrec/streamable.py
+-rw-r--r--  2.0 unx      854 b- defN 23-Jul-29 11:17 torchrec/types.py
+-rw-r--r--  2.0 unx     1153 b- defN 23-Jul-29 11:17 torchrec/datasets/__init__.py
+-rw-r--r--  2.0 unx    41469 b- defN 23-Jul-29 11:17 torchrec/datasets/criteo.py
+-rw-r--r--  2.0 unx     4548 b- defN 23-Jul-29 11:17 torchrec/datasets/movielens.py
+-rw-r--r--  2.0 unx     6539 b- defN 23-Jul-29 11:17 torchrec/datasets/random.py
+-rw-r--r--  2.0 unx    10909 b- defN 23-Jul-29 11:17 torchrec/datasets/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-29 11:17 torchrec/datasets/scripts/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 23-Jul-29 11:17 torchrec/datasets/scripts/contiguous_preproc_criteo.py
+-rw-r--r--  2.0 unx     2847 b- defN 23-Jul-29 11:17 torchrec/datasets/scripts/npy_preproc_criteo.py
+-rw-r--r--  2.0 unx     3077 b- defN 23-Jul-29 11:17 torchrec/datasets/scripts/shuffle_preproc_criteo.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-29 11:17 torchrec/datasets/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5308 b- defN 23-Jul-29 11:17 torchrec/datasets/test_utils/criteo_test_utils.py
+-rw-r--r--  2.0 unx     1912 b- defN 23-Jul-29 11:17 torchrec/distributed/__init__.py
+-rw-r--r--  2.0 unx    37307 b- defN 23-Jul-29 11:17 torchrec/distributed/batched_embedding_kernel.py
+-rw-r--r--  2.0 unx     2069 b- defN 23-Jul-29 11:17 torchrec/distributed/collective_utils.py
+-rw-r--r--  2.0 unx     4988 b- defN 23-Jul-29 11:17 torchrec/distributed/comm.py
+-rw-r--r--  2.0 unx    56136 b- defN 23-Jul-29 11:17 torchrec/distributed/comm_ops.py
+-rw-r--r--  2.0 unx    36889 b- defN 23-Jul-29 11:17 torchrec/distributed/dist_data.py
+-rw-r--r--  2.0 unx    32295 b- defN 23-Jul-29 11:17 torchrec/distributed/embedding.py
+-rw-r--r--  2.0 unx     4947 b- defN 23-Jul-29 11:17 torchrec/distributed/embedding_kernel.py
+-rw-r--r--  2.0 unx    30169 b- defN 23-Jul-29 11:17 torchrec/distributed/embedding_lookup.py
+-rw-r--r--  2.0 unx    19035 b- defN 23-Jul-29 11:17 torchrec/distributed/embedding_sharding.py
+-rw-r--r--  2.0 unx    36853 b- defN 23-Jul-29 11:17 torchrec/distributed/embedding_tower_sharding.py
+-rw-r--r--  2.0 unx    15386 b- defN 23-Jul-29 11:17 torchrec/distributed/embedding_types.py
+-rw-r--r--  2.0 unx    37369 b- defN 23-Jul-29 11:17 torchrec/distributed/embeddingbag.py
+-rw-r--r--  2.0 unx     7373 b- defN 23-Jul-29 11:17 torchrec/distributed/fbgemm_qcomm_codec.py
+-rw-r--r--  2.0 unx     6219 b- defN 23-Jul-29 11:17 torchrec/distributed/fp_embeddingbag.py
+-rw-r--r--  2.0 unx     5243 b- defN 23-Jul-29 11:17 torchrec/distributed/fused_embedding.py
+-rw-r--r--  2.0 unx     5080 b- defN 23-Jul-29 11:17 torchrec/distributed/fused_embeddingbag.py
+-rw-r--r--  2.0 unx     2271 b- defN 23-Jul-29 11:17 torchrec/distributed/fused_params.py
+-rw-r--r--  2.0 unx     3807 b- defN 23-Jul-29 11:17 torchrec/distributed/grouped_position_weighted.py
+-rw-r--r--  2.0 unx    10997 b- defN 23-Jul-29 11:17 torchrec/distributed/mc_embeddingbag.py
+-rw-r--r--  2.0 unx    19750 b- defN 23-Jul-29 11:17 torchrec/distributed/model_parallel.py
+-rw-r--r--  2.0 unx    20748 b- defN 23-Jul-29 11:17 torchrec/distributed/quant_embedding.py
+-rw-r--r--  2.0 unx    15112 b- defN 23-Jul-29 11:17 torchrec/distributed/quant_embedding_kernel.py
+-rw-r--r--  2.0 unx    12621 b- defN 23-Jul-29 11:17 torchrec/distributed/quant_embeddingbag.py
+-rw-r--r--  2.0 unx    13928 b- defN 23-Jul-29 11:17 torchrec/distributed/quant_state.py
+-rw-r--r--  2.0 unx     9261 b- defN 23-Jul-29 11:17 torchrec/distributed/shard.py
+-rw-r--r--  2.0 unx    19646 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding_plan.py
+-rw-r--r--  2.0 unx    47947 b- defN 23-Jul-29 11:17 torchrec/distributed/train_pipeline.py
+-rw-r--r--  2.0 unx    27865 b- defN 23-Jul-29 11:17 torchrec/distributed/types.py
+-rw-r--r--  2.0 unx    15470 b- defN 23-Jul-29 11:17 torchrec/distributed/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-29 11:17 torchrec/distributed/composable/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 23-Jul-29 11:17 torchrec/distributed/composable/table_batched_embedding_slice.py
+-rw-r--r--  2.0 unx     1025 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/__init__.py
+-rw-r--r--  2.0 unx     3135 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/constants.py
+-rw-r--r--  2.0 unx    11430 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/enumerators.py
+-rw-r--r--  2.0 unx    12642 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/partitioners.py
+-rw-r--r--  2.0 unx      835 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/perf_models.py
+-rw-r--r--  2.0 unx    13530 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/planners.py
+-rw-r--r--  2.0 unx    11236 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/proposers.py
+-rw-r--r--  2.0 unx    41291 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/shard_estimators.py
+-rw-r--r--  2.0 unx    23791 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/stats.py
+-rw-r--r--  2.0 unx     9643 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/storage_reservations.py
+-rw-r--r--  2.0 unx    14582 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/types.py
+-rw-r--r--  2.0 unx     1119 b- defN 23-Jul-29 11:17 torchrec/distributed/planner/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/__init__.py
+-rw-r--r--  2.0 unx     2479 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/cw_sequence_sharding.py
+-rw-r--r--  2.0 unx    12945 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/cw_sharding.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/dp_sequence_sharding.py
+-rw-r--r--  2.0 unx     7681 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/dp_sharding.py
+-rw-r--r--  2.0 unx     7640 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/rw_sequence_sharding.py
+-rw-r--r--  2.0 unx    17911 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/rw_sharding.py
+-rw-r--r--  2.0 unx     3627 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/sequence_sharding.py
+-rw-r--r--  2.0 unx     7632 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/tw_sequence_sharding.py
+-rw-r--r--  2.0 unx    16097 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/tw_sharding.py
+-rw-r--r--  2.0 unx     1284 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/twcw_sharding.py
+-rw-r--r--  2.0 unx    19871 b- defN 23-Jul-29 11:17 torchrec/distributed/sharding/twrw_sharding.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-29 11:17 torchrec/distributed/test_utils/__init__.py
+-rw-r--r--  2.0 unx    11237 b- defN 23-Jul-29 11:17 torchrec/distributed/test_utils/infer_utils.py
+-rw-r--r--  2.0 unx     4868 b- defN 23-Jul-29 11:17 torchrec/distributed/test_utils/multi_process.py
+-rw-r--r--  2.0 unx    34091 b- defN 23-Jul-29 11:17 torchrec/distributed/test_utils/test_model.py
+-rw-r--r--  2.0 unx    11197 b- defN 23-Jul-29 11:17 torchrec/distributed/test_utils/test_model_parallel.py
+-rw-r--r--  2.0 unx    25310 b- defN 23-Jul-29 11:17 torchrec/distributed/test_utils/test_model_parallel_base.py
+-rw-r--r--  2.0 unx    15367 b- defN 23-Jul-29 11:17 torchrec/distributed/test_utils/test_sharding.py
+-rw-r--r--  2.0 unx      422 b- defN 23-Jul-29 11:17 torchrec/fx/__init__.py
+-rw-r--r--  2.0 unx     6451 b- defN 23-Jul-29 11:17 torchrec/fx/tracer.py
+-rw-r--r--  2.0 unx     4524 b- defN 23-Jul-29 11:17 torchrec/fx/utils.py
+-rw-r--r--  2.0 unx     1223 b- defN 23-Jul-29 11:17 torchrec/inference/__init__.py
+-rw-r--r--  2.0 unx     3614 b- defN 23-Jul-29 11:17 torchrec/inference/client.py
+-rw-r--r--  2.0 unx     3957 b- defN 23-Jul-29 11:17 torchrec/inference/model_packager.py
+-rw-r--r--  2.0 unx     8068 b- defN 23-Jul-29 11:17 torchrec/inference/modules.py
+-rw-r--r--  2.0 unx     3797 b- defN 23-Jul-29 11:17 torchrec/inference/state_dict_transform.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-29 11:17 torchrec/metrics/__init__.py
+-rw-r--r--  2.0 unx     4168 b- defN 23-Jul-29 11:17 torchrec/metrics/accuracy.py
+-rw-r--r--  2.0 unx    12549 b- defN 23-Jul-29 11:17 torchrec/metrics/auc.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Jul-29 11:17 torchrec/metrics/calibration.py
+-rw-r--r--  2.0 unx     3465 b- defN 23-Jul-29 11:17 torchrec/metrics/ctr.py
+-rw-r--r--  2.0 unx     3836 b- defN 23-Jul-29 11:17 torchrec/metrics/mae.py
+-rw-r--r--  2.0 unx    17990 b- defN 23-Jul-29 11:17 torchrec/metrics/metric_module.py
+-rw-r--r--  2.0 unx     6796 b- defN 23-Jul-29 11:17 torchrec/metrics/metrics_config.py
+-rw-r--r--  2.0 unx     3731 b- defN 23-Jul-29 11:17 torchrec/metrics/metrics_namespace.py
+-rw-r--r--  2.0 unx     3904 b- defN 23-Jul-29 11:17 torchrec/metrics/model_utils.py
+-rw-r--r--  2.0 unx     4631 b- defN 23-Jul-29 11:17 torchrec/metrics/mse.py
+-rw-r--r--  2.0 unx     5605 b- defN 23-Jul-29 11:17 torchrec/metrics/multiclass_recall.py
+-rw-r--r--  2.0 unx     8735 b- defN 23-Jul-29 11:17 torchrec/metrics/ndcg.py
+-rw-r--r--  2.0 unx     6811 b- defN 23-Jul-29 11:17 torchrec/metrics/ne.py
+-rw-r--r--  2.0 unx    33554 b- defN 23-Jul-29 11:17 torchrec/metrics/rec_metric.py
+-rw-r--r--  2.0 unx    10490 b- defN 23-Jul-29 11:17 torchrec/metrics/recall_session.py
+-rw-r--r--  2.0 unx     6057 b- defN 23-Jul-29 11:17 torchrec/metrics/throughput.py
+-rw-r--r--  2.0 unx    11160 b- defN 23-Jul-29 11:17 torchrec/metrics/tower_qps.py
+-rw-r--r--  2.0 unx     2867 b- defN 23-Jul-29 11:17 torchrec/metrics/weighted_avg.py
+-rw-r--r--  2.0 unx    16441 b- defN 23-Jul-29 11:17 torchrec/metrics/test_utils/__init__.py
+-rw-r--r--  2.0 unx      913 b- defN 23-Jul-29 11:17 torchrec/models/__init__.py
+-rw-r--r--  2.0 unx    11410 b- defN 23-Jul-29 11:17 torchrec/models/deepfm.py
+-rw-r--r--  2.0 unx    29965 b- defN 23-Jul-29 11:17 torchrec/models/dlrm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-29 11:17 torchrec/models/experimental/__init__.py
+-rw-r--r--  2.0 unx     9825 b- defN 23-Jul-29 11:17 torchrec/models/experimental/test_transformerdlrm.py
+-rw-r--r--  2.0 unx     7434 b- defN 23-Jul-29 11:17 torchrec/models/experimental/transformerdlrm.py
+-rw-r--r--  2.0 unx     1179 b- defN 23-Jul-29 11:17 torchrec/modules/__init__.py
+-rw-r--r--  2.0 unx     1456 b- defN 23-Jul-29 11:17 torchrec/modules/activation.py
+-rw-r--r--  2.0 unx    14636 b- defN 23-Jul-29 11:17 torchrec/modules/crossnet.py
+-rw-r--r--  2.0 unx     8415 b- defN 23-Jul-29 11:17 torchrec/modules/deepfm.py
+-rw-r--r--  2.0 unx     6468 b- defN 23-Jul-29 11:17 torchrec/modules/embedding_configs.py
+-rw-r--r--  2.0 unx    14021 b- defN 23-Jul-29 11:17 torchrec/modules/embedding_modules.py
+-rw-r--r--  2.0 unx     4858 b- defN 23-Jul-29 11:17 torchrec/modules/embedding_tower.py
+-rw-r--r--  2.0 unx    12360 b- defN 23-Jul-29 11:17 torchrec/modules/feature_processor.py
+-rw-r--r--  2.0 unx     4964 b- defN 23-Jul-29 11:17 torchrec/modules/feature_processor_.py
+-rw-r--r--  2.0 unx     4903 b- defN 23-Jul-29 11:17 torchrec/modules/fp_embedding_modules.py
+-rw-r--r--  2.0 unx    31545 b- defN 23-Jul-29 11:17 torchrec/modules/fused_embedding_modules.py
+-rw-r--r--  2.0 unx    10533 b- defN 23-Jul-29 11:17 torchrec/modules/lazy_extension.py
+-rw-r--r--  2.0 unx     3143 b- defN 23-Jul-29 11:17 torchrec/modules/managed_collision_modules.py
+-rw-r--r--  2.0 unx     5124 b- defN 23-Jul-29 11:17 torchrec/modules/mc_embedding_modules.py
+-rw-r--r--  2.0 unx     6309 b- defN 23-Jul-29 11:17 torchrec/modules/mlp.py
+-rw-r--r--  2.0 unx     3897 b- defN 23-Jul-29 11:17 torchrec/modules/utils.py
+-rw-r--r--  2.0 unx     1639 b- defN 23-Jul-29 11:17 torchrec/optim/__init__.py
+-rw-r--r--  2.0 unx     2012 b- defN 23-Jul-29 11:17 torchrec/optim/apply_optimizer_in_backward.py
+-rw-r--r--  2.0 unx     1569 b- defN 23-Jul-29 11:17 torchrec/optim/clipping.py
+-rw-r--r--  2.0 unx     1353 b- defN 23-Jul-29 11:17 torchrec/optim/fused.py
+-rw-r--r--  2.0 unx    16069 b- defN 23-Jul-29 11:17 torchrec/optim/keyed.py
+-rw-r--r--  2.0 unx     4420 b- defN 23-Jul-29 11:17 torchrec/optim/optimizers.py
+-rw-r--r--  2.0 unx     7405 b- defN 23-Jul-29 11:17 torchrec/optim/rowwise_adagrad.py
+-rw-r--r--  2.0 unx     4865 b- defN 23-Jul-29 11:17 torchrec/optim/warmup.py
+-rw-r--r--  2.0 unx      560 b- defN 23-Jul-29 11:17 torchrec/optim/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1140 b- defN 23-Jul-29 11:17 torchrec/quant/__init__.py
+-rw-r--r--  2.0 unx    26618 b- defN 23-Jul-29 11:17 torchrec/quant/embedding_modules.py
+-rw-r--r--  2.0 unx     4292 b- defN 23-Jul-29 11:17 torchrec/quant/utils.py
+-rw-r--r--  2.0 unx     1163 b- defN 23-Jul-29 11:17 torchrec/sparse/__init__.py
+-rw-r--r--  2.0 unx    56412 b- defN 23-Jul-29 11:17 torchrec/sparse/jagged_tensor.py
+-rw-r--r--  2.0 unx     1430 b- defN 23-Jul-29 11:17 torchrec/sparse/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5661 b- defN 23-Jul-29 11:17 torchrec/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1530 b- defN 23-Jul-29 11:22 torchrec_nightly-2023.7.29.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5012 b- defN 23-Jul-29 11:22 torchrec_nightly-2023.7.29.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-Jul-29 11:22 torchrec_nightly-2023.7.29.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Jul-29 11:22 torchrec_nightly-2023.7.29.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    13823 b- defN 23-Jul-29 11:22 torchrec_nightly-2023.7.29.dist-info/RECORD
+148 files, 1538781 bytes uncompressed, 350273 bytes compressed:  77.2%
```

## zipnote {}

```diff
@@ -423,23 +423,23 @@
 
 Filename: torchrec/sparse/test_utils/__init__.py
 Comment: 
 
 Filename: torchrec/test_utils/__init__.py
 Comment: 
 
-Filename: torchrec_nightly-2023.7.26.dist-info/LICENSE
+Filename: torchrec_nightly-2023.7.29.dist-info/LICENSE
 Comment: 
 
-Filename: torchrec_nightly-2023.7.26.dist-info/METADATA
+Filename: torchrec_nightly-2023.7.29.dist-info/METADATA
 Comment: 
 
-Filename: torchrec_nightly-2023.7.26.dist-info/WHEEL
+Filename: torchrec_nightly-2023.7.29.dist-info/WHEEL
 Comment: 
 
-Filename: torchrec_nightly-2023.7.26.dist-info/top_level.txt
+Filename: torchrec_nightly-2023.7.29.dist-info/top_level.txt
 Comment: 
 
-Filename: torchrec_nightly-2023.7.26.dist-info/RECORD
+Filename: torchrec_nightly-2023.7.29.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchrec/distributed/dist_data.py

```diff
@@ -722,17 +722,14 @@
             tensors (List[torch.Tensor]): list of embedding tensors.
 
         Returns:
             Awaitable[torch.Tensor]: awaitable of the merged embeddings.
         """
         assert len(tensors) == self._world_size
         is_target_device_cpu: bool = self._device.type == "cpu"
-        if is_target_device_cpu and tensors[0].device.type == "cpu":
-            # assume all tensors on cpu
-            return torch.cat(tensors, self._cat_dim)
 
         non_cat_size = tensors[0].size(1 - self._cat_dim)
         # if src device is cuda, target device is cpu:
         # 1. merge on first tensor device
         # 2. move to cpu
         device = self._device if not is_target_device_cpu else tensors[0].device
         merge = torch.ops.fbgemm.merge_pooled_embeddings(
```

## torchrec/distributed/embedding_lookup.py

```diff
@@ -282,14 +282,17 @@
         scale_weight_gradients: bool = True,
     ) -> None:
         # TODO rename to _create_embedding_kernel
         def _create_lookup(
             config: GroupedEmbeddingConfig,
             device: Optional[torch.device] = None,
         ) -> BaseEmbedding:
+            for table in config.embedding_tables:
+                if table.compute_kernel == EmbeddingComputeKernel.FUSED_UVM_CACHING:
+                    self._need_prefetch = True
             if config.compute_kernel == EmbeddingComputeKernel.DENSE:
                 return BatchedDenseEmbeddingBag(
                     config=config,
                     pg=pg,
                     device=device,
                 )
             elif config.compute_kernel == EmbeddingComputeKernel.FUSED:
@@ -301,14 +304,15 @@
             else:
                 raise ValueError(
                     f"Compute kernel not supported {config.compute_kernel}"
                 )
 
         super().__init__()
         self._emb_modules: nn.ModuleList = nn.ModuleList()
+        self._need_prefetch = False
         for config in grouped_configs:
             self._emb_modules.append(_create_lookup(config, device))
 
         self._feature_splits: List[int] = []
         for config in grouped_configs:
             self._feature_splits.append(config.num_features())
 
@@ -328,14 +332,34 @@
 
         self._scale_gradient_factor: int = (
             dist.get_world_size(pg)
             if scale_weight_gradients and get_gradient_division()
             else 1
         )
 
+    def prefetch(
+        self,
+        sparse_features: KeyedJaggedTensor,
+        forward_stream: Optional[torch.cuda.Stream] = None,
+    ) -> None:
+        if not self._need_prefetch:
+            return
+        if len(self._emb_modules) > 0:
+            assert sparse_features is not None
+            features_by_group = sparse_features.split(
+                self._feature_splits,
+            )
+            for emb_op, features in zip(self._emb_modules, features_by_group):
+                if hasattr(emb_op.emb_module, "prefetch"):
+                    emb_op.emb_module.prefetch(
+                        indices=features.values(),
+                        offsets=features.offsets(),
+                        forward_stream=forward_stream,
+                    )
+
     def forward(
         self,
         sparse_features: KeyedJaggedTensor,
     ) -> torch.Tensor:
         embeddings: List[torch.Tensor] = []
         if len(self._emb_modules) > 0:
             assert sparse_features is not None
```

## torchrec/distributed/embedding_types.py

```diff
@@ -291,14 +291,24 @@
     ) -> None:
         super().__init__(qcomm_codecs_registry)
 
         self._input_dists: List[nn.Module] = []
         self._lookups: List[nn.Module] = []
         self._output_dists: List[nn.Module] = []
 
+    def prefetch(
+        self, dist_input: KJTList, forward_stream: Optional[torch.cuda.Stream] = None
+    ) -> None:
+        """
+        Prefetch input features for each lookup module.
+        """
+
+        for feature, emb_lookup in zip(dist_input, self._lookups):
+            emb_lookup.prefetch(sparse_features=feature, forward_stream=forward_stream)
+
     def extra_repr(self) -> str:
         """
         Pretty prints representation of the module's lookup modules, input_dists and output_dists
         """
 
         def loop(key: str, modules: List[nn.Module]) -> List[str]:
             child_lines = []
```

## torchrec/distributed/fp_embeddingbag.py

```diff
@@ -60,14 +60,16 @@
                 module._embedding_bag_collection,
                 table_name_to_parameter_sharding,
                 env=env,
                 device=device,
             )
         )
 
+        self._lookups: List[nn.Module] = self._embedding_bag_collection._lookups
+
         self._feature_processors: Union[nn.ModuleDict, FeatureProcessorsCollection]
         if isinstance(module._feature_processors, FeatureProcessorsCollection):
             self._feature_processors = module._feature_processors.to(device)
         else:
             self._feature_processors = torch.nn.ModuleDict(
                 {key: fp.to(device) for key, fp in module._feature_processors.items()}
             )
```

## torchrec/distributed/train_pipeline.py

```diff
@@ -22,23 +22,25 @@
     Dict,
     Generic,
     Iterator,
     List,
     Optional,
     Set,
     Tuple,
+    Type,
     TypeVar,
     Union,
 )
 
 import torch
 from torch import distributed as dist
 from torch.autograd.profiler import record_function
 from torch.fx.node import Node
 from torchrec.distributed.dist_data import KJTAllToAll, KJTAllToAllTensorsAwaitable
+from torchrec.distributed.embedding_lookup import GroupedPooledEmbeddingsLookup
 from torchrec.distributed.embedding_sharding import (
     KJTListAwaitable,
     KJTListSplitsAwaitable,
 )
 from torchrec.distributed.model_parallel import DistributedModelParallel, ShardedModule
 from torchrec.distributed.types import Awaitable
 from torchrec.modules.feature_processor import BaseGroupedFeatureProcessor
@@ -362,14 +364,22 @@
         Tuple[List[str], FusedKJTListSplitsAwaitable]
     ] = field(default_factory=list)
     # pyre-ignore [4]
     feature_processor_forwards: List[Any] = field(default_factory=list)
 
 
 @dataclass
+class PrefetchTrainPipelineContext(TrainPipelineContext):
+    module_input_post_prefetch: Dict[str, Multistreamable] = field(default_factory=dict)
+    module_contexts_post_prefetch: Dict[str, Multistreamable] = field(
+        default_factory=dict
+    )
+
+
+@dataclass
 class ArgInfo:
     """
     Representation of args from a node.
 
     Attributes:
         input_attrs (List[str]): attributes of input batch,
             e.g. `batch.attr1.attr2` will produce ["attr1", "attr2"].
@@ -379,44 +389,54 @@
     """
 
     input_attrs: List[str]
     is_getitems: List[bool]
     name: Optional[str]
 
 
-class PipelinedForward:
+class BaseForward:
     def __init__(
         self,
         name: str,
         args: List[ArgInfo],
         module: ShardedModule,
         context: TrainPipelineContext,
-        dist_stream: Optional[torch.cuda.streams.Stream],
+        stream: Optional[torch.cuda.streams.Stream],
     ) -> None:
         self._name = name
         self._args = args
         self._module = module
         self._context = context
-        self._dist_stream = dist_stream
+        self._stream = stream
 
+    @property
+    def name(self) -> str:
+        return self._name
+
+    @property
+    def args(self) -> List[ArgInfo]:
+        return self._args
+
+
+class PipelinedForward(BaseForward):
     # pyre-ignore [2, 24]
     def __call__(self, *input, **kwargs) -> Awaitable:
         assert self._name in self._context.input_dist_tensors_requests
         request = self._context.input_dist_tensors_requests[self._name]
         assert isinstance(request, Awaitable)
         with record_function("## wait_sparse_data_dist ##"):
             # Finish waiting on the dist_stream,
             # in case some delayed stream scheduling happens during the wait() call.
-            with torch.cuda.stream(self._dist_stream):
+            with torch.cuda.stream(self._stream):
                 data = request.wait()
 
         # Make sure that both result of input_dist and context
         # are properly transferred to the current stream.
-        if self._dist_stream is not None:
-            torch.cuda.current_stream().wait_stream(self._dist_stream)
+        if self._stream is not None:
+            torch.cuda.current_stream().wait_stream(self._stream)
             cur_stream = torch.cuda.current_stream()
 
             assert isinstance(
                 data, (torch.Tensor, Multistreamable)
             ), f"{type(data)} must implement Multistreamable interface"
             # pyre-fixme[6]: For 1st param expected `Stream` but got `Stream`.
             data.record_stream(cur_stream)
@@ -430,21 +450,63 @@
                     for fp_forward in self._context.feature_processor_forwards:
                         data[i] = fp_forward(sparse_feature)
 
         return self._module.compute_and_output_dist(
             self._context.module_contexts[self._name], data
         )
 
-    @property
-    def name(self) -> str:
-        return self._name
 
-    @property
-    def args(self) -> List[ArgInfo]:
-        return self._args
+class PrefetchPipelinedForward(BaseForward):
+    def __init__(
+        self,
+        name: str,
+        args: List[ArgInfo],
+        module: ShardedModule,
+        context: PrefetchTrainPipelineContext,
+        prefetch_stream: Optional[torch.cuda.streams.Stream],
+    ) -> None:
+        super().__init__(
+            name=name,
+            args=args,
+            module=module,
+            context=context,
+            stream=prefetch_stream,
+        )
+        self._context: PrefetchTrainPipelineContext = self._context
+
+    # pyre-ignore [2, 24]
+    def __call__(self, *input, **kwargs) -> Awaitable:
+        assert self._name in self._context.module_input_post_prefetch
+        data = self._context.module_input_post_prefetch[self._name]
+
+        # Make sure that both result of input_dist and context
+        # are properly transferred to the current stream.
+        if self._stream is not None:
+            torch.cuda.current_stream().wait_stream(self._stream)
+            cur_stream = torch.cuda.current_stream()
+
+            assert isinstance(
+                data, (torch.Tensor, Multistreamable)
+            ), f"{type(data)} must implement Multistreamable interface"
+            data.record_stream(cur_stream)
+
+            ctx = self._context.module_contexts_post_prefetch[self._name]
+            ctx.record_stream(cur_stream)
+
+        if len(self._context.feature_processor_forwards) > 0:
+            with record_function("## feature_processor ##"):
+                # pyre-ignore[6]
+                for i, sparse_feature in enumerate(data):
+                    for fp_forward in self._context.feature_processor_forwards:
+                        # pyre-ignore[16]
+                        data[i] = fp_forward(sparse_feature)
+
+        return self._module.compute_and_output_dist(
+            self._context.module_contexts_post_prefetch[self._name], data
+        )
 
 
 class KJTAllToAllForward:
     def __init__(
         self, pg: dist.ProcessGroup, splits: List[int], stagger: int = 1
     ) -> None:
         self._pg = pg
@@ -713,14 +775,15 @@
 
 def _rewrite_model(  # noqa C901
     model: torch.nn.Module,
     context: TrainPipelineContext,
     dist_stream: Optional[torch.cuda.streams.Stream],
     batch: Optional[In] = None,
     apply_jit: bool = False,
+    pipelined_forward: Type[BaseForward] = PipelinedForward,
 ) -> Tuple[List[ShardedModule], torch.nn.Module]:
     input_model = model
     # Get underlying nn.Module
     if isinstance(model, DistributedModelParallel):
         model = model.module
 
     # Collect feature processors.
@@ -772,15 +835,15 @@
                 assert num_found == len(
                     arg_info_list
                 ), "the number of args doesn't match the fx trace arg info provided in the model"
 
             if num_found == total_num_args:
                 logger.info(f"Module '{node.target}'' will be pipelined")
                 child = sharded_modules[node.target]
-                child.forward = PipelinedForward(
+                child.forward = pipelined_forward(
                     node.target,
                     arg_info_list,
                     child,
                     context,
                     dist_stream,
                 )
                 pipelined_forwards.append(child)
@@ -988,7 +1051,197 @@
                 self._context.module_contexts = (
                     self._context.module_contexts_next_batch.copy()
                 )
                 self._context.input_dist_tensors_requests.clear()
                 for names, awaitable in self._context.fused_splits_awaitables:
                     for name, request in zip(names, awaitable.wait()):
                         self._context.input_dist_tensors_requests[name] = request
+
+
+class PrefetchTrainPipelineSparseDist(TrainPipelineSparseDist[In, Out]):
+    """
+    This pipeline overlaps device transfer, `ShardedModule.input_dist()`, and cache
+    prefetching with forward and backward. This helps hide the all2all latency while
+    preserving the training forward / backward ordering.
+
+    stage 4: forward, backward - uses default CUDA stream
+    stage 3: prefetch - uses prefetch CUDA stream
+    stage 2: ShardedModule.input_dist() - uses data_dist CUDA stream
+    stage 1: device transfer - uses memcpy CUDA stream
+
+    `ShardedModule.input_dist()` is only done for top-level modules in the call graph.
+    To be considered a top-level module, a module can only depend on 'getattr' calls on
+    input.
+
+    Input model must be symbolically traceable with the exception of `ShardedModule` and
+    `DistributedDataParallel` modules.
+
+    Args:
+        model (torch.nn.Module): model to pipeline.
+        optimizer (torch.optim.Optimizer): optimizer to use.
+        device (torch.device): device where device transfer, sparse data dist, prefetch,
+            and forward/backward pass will happen.
+        execute_all_batches (bool): executes remaining batches in pipeline after
+            exhausting dataloader iterator.
+        apply_jit (bool): apply torch.jit.script to non-pipelined (unsharded) modules.
+    """
+
+    def __init__(
+        self,
+        model: torch.nn.Module,
+        optimizer: torch.optim.Optimizer,
+        device: torch.device,
+        execute_all_batches: bool = True,
+        apply_jit: bool = False,
+    ) -> None:
+        super().__init__(
+            model=model,
+            optimizer=optimizer,
+            device=device,
+            execute_all_batches=execute_all_batches,
+            apply_jit=apply_jit,
+        )
+        self._context = PrefetchTrainPipelineContext()
+        if self._device.type == "cuda":
+            self._prefetch_stream: Optional[
+                torch.cuda.streams.Stream
+            ] = torch.cuda.Stream()
+            self._default_stream: Optional[
+                torch.cuda.streams.Stream
+            ] = torch.cuda.current_stream()
+        else:
+            self._prefetch_stream: Optional[torch.cuda.streams.Stream] = None
+            self._default_stream: Optional[torch.cuda.streams.Stream] = None
+        self._batch_ip3: Optional[In] = None
+
+    def _fill_pipeline(self, dataloader_iter: Iterator[In]) -> None:
+        # pipeline is already filled
+        if self._batch_i and self._batch_ip1 and self._batch_ip2:
+            return
+        # executes last batch in pipeline
+        if self._execute_all_batches and (self._batch_i or self._batch_ip1):
+            return
+
+        # batch 1
+        self._batch_i = self._copy_batch_to_gpu(dataloader_iter)
+        if self._batch_i is None:
+            raise StopIteration
+
+        self._init_pipelined_modules(self._batch_i)
+        self._start_sparse_data_dist(self._batch_i)
+        self._wait_sparse_data_dist()
+        self._prefetch(self._batch_i)
+
+        # batch 2
+        self._batch_ip1 = self._copy_batch_to_gpu(dataloader_iter)
+        self._start_sparse_data_dist(self._batch_ip1)
+        self._wait_sparse_data_dist()
+
+        # batch 3
+        self._batch_ip2 = self._copy_batch_to_gpu(dataloader_iter)
+
+    def progress(self, dataloader_iter: Iterator[In]) -> Out:
+        self._fill_pipeline(dataloader_iter)
+
+        if self._model.training:
+            with record_function("## zero_grad ##"):
+                self._optimizer.zero_grad()
+
+        with record_function("## wait_for_batch ##"):
+            _wait_for_batch(cast(In, self._batch_i), self._prefetch_stream)
+
+        self._start_sparse_data_dist(self._batch_ip2)
+
+        self._batch_ip3 = self._copy_batch_to_gpu(dataloader_iter)
+
+        # forward
+        with record_function("## forward ##"):
+            losses, output = cast(Tuple[torch.Tensor, Out], self._model(self._batch_i))
+
+        self._prefetch(self._batch_ip1)
+
+        self._wait_sparse_data_dist()
+
+        if self._model.training:
+            # backward
+            with record_function("## backward ##"):
+                torch.sum(losses, dim=0).backward()
+
+            # update
+            with record_function("## optimizer ##"):
+                self._optimizer.step()
+
+        self._batch_i = self._batch_ip1
+        self._batch_ip1 = self._batch_ip2
+        self._batch_ip2 = self._batch_ip3
+
+        return output
+
+    def _init_pipelined_modules(self, batch: In) -> None:
+        """
+        Retrieves the pipelined modules after overriding their forwards, initializes the
+        modules' input dists, and overrides the input dist forwards to support fusing
+        the splits collective in the input dist.
+        """
+        if self._pipelined_modules:
+            return
+        self._pipelined_modules, self._model = _rewrite_model(
+            model=self._model,
+            context=self._context,
+            dist_stream=self._data_dist_stream,
+            batch=self._batch_i,
+            apply_jit=self._apply_jit,
+            pipelined_forward=PrefetchPipelinedForward,
+        )
+
+        # initializes input dist, so we can override input dist forwards
+        self._start_sparse_data_dist(self._batch_i)
+        _override_input_dist_forwards(self._pipelined_modules)
+
+    def _prefetch(self, batch: Optional[In]) -> None:
+        """
+        Waits for input dist to finish, then prefetches data.
+        """
+        if batch is None:
+            return
+        self._context.module_input_post_prefetch.clear()
+        self._context.module_contexts_post_prefetch.clear()
+
+        with record_function("## sharded_module_prefetch ##"):
+            with torch.cuda.stream(self._prefetch_stream):
+                batch.record_stream(torch.cuda.current_stream())
+                for sharded_module in self._pipelined_modules:
+                    forward = sharded_module.forward
+                    assert isinstance(forward, PrefetchPipelinedForward)
+
+                    assert forward._name in self._context.input_dist_tensors_requests
+                    request = self._context.input_dist_tensors_requests[forward._name]
+                    assert isinstance(request, Awaitable)
+                    with record_function("## wait_sparse_data_dist ##"):
+                        # Finish waiting on the dist_stream,
+                        # in case some delayed stream scheduling happens during the wait() call.
+                        with torch.cuda.stream(self._data_dist_stream):
+                            data = request.wait()
+
+                    # Make sure that both result of input_dist and context
+                    # are properly transferred to the current stream.
+                    if self._data_dist_stream is not None:
+                        torch.cuda.current_stream().wait_stream(self._data_dist_stream)
+                        cur_stream = torch.cuda.current_stream()
+
+                        assert isinstance(
+                            data, (torch.Tensor, Multistreamable)
+                        ), f"{type(data)} must implement Multistreamable interface"
+                        data.record_stream(cur_stream)
+                        data.record_stream(self._default_stream)
+
+                        ctx = self._context.module_contexts[forward._name]
+                        ctx.record_stream(cur_stream)
+                        ctx.record_stream(self._default_stream)
+
+                    sharded_module.prefetch(
+                        dist_input=data, forward_stream=self._default_stream
+                    )
+                    self._context.module_input_post_prefetch[forward._name] = data
+                    self._context.module_contexts_post_prefetch[
+                        forward._name
+                    ] = self._context.module_contexts[forward._name]
```

## torchrec/modules/feature_processor_.py

```diff
@@ -117,59 +117,59 @@
 
         Returns:
             JaggedTensor: modified JT
         """
         pass
 
 
+@torch.fx.wrap
+def get_weights_list(
+    cat_seq: torch.Tensor,
+    features: KeyedJaggedTensor,
+    position_weights: Dict[str, nn.Parameter],
+) -> Optional[torch.Tensor]:
+    weights_list = []
+    seqs = torch.split(cat_seq, features.length_per_key())
+    for key, seq in zip(features.keys(), seqs):
+        if key in position_weights.keys():
+            weights_list.append(torch.gather(position_weights[key], dim=0, index=seq))
+        else:
+            weights_list.append(
+                torch.ones(seq.shape[0], device=features.values().device)
+            )
+    return torch.cat(weights_list) if weights_list else features.weights_or_none()
+
+
 class PositionWeightedModuleCollection(FeatureProcessorsCollection):
     def __init__(
         self, max_feature_lengths: Dict[str, int], device: Optional[torch.device] = None
     ) -> None:
         super().__init__()
         self.max_feature_lengths = max_feature_lengths
         for length in self.max_feature_lengths.values():
             if length <= 0:
                 raise
+
         self.position_weights: nn.ParameterDict = nn.ParameterDict()
+        # needed since nn.ParameterDict isn't torchscriptable (get_items)
+        self.position_weights_dict: Dict[str, nn.Parameter] = {}
+
         for key, length in max_feature_lengths.items():
             self.position_weights[key] = nn.Parameter(
                 torch.empty([length], device=device).fill_(1.0)
             )
-        self.register_buffer(
-            "_dummy_weights",
-            torch.tensor(
-                max(self.max_feature_lengths.values()),
-                device=device,
-            ).fill_(1.0),
-        )
+            self.position_weights_dict[key] = self.position_weights[key]
 
     def forward(self, features: KeyedJaggedTensor) -> KeyedJaggedTensor:
-        if len(features.keys()) == 0:
-            return features
-
         cat_seq = torch.ops.fbgemm.offsets_range(
             features.offsets().long(), torch.numel(features.values())
         )
 
-        seqs = torch.split(cat_seq, features.length_per_key())
-        weights_list = []
-        for key, seq in zip(features.keys(), seqs):
-            if key in self.position_weights:
-                weights_list.append(
-                    torch.gather(self.position_weights[key], dim=0, index=seq)
-                )
-            else:
-                weights_list.append(
-                    torch.ones(seq.shape[0], device=features.values().device)
-                )
-
-        weights = torch.cat(weights_list)
         return KeyedJaggedTensor(
             keys=features.keys(),
             values=features.values(),
-            weights=weights,
+            weights=get_weights_list(cat_seq, features, self.position_weights_dict),
             lengths=features.lengths(),
             offsets=features.offsets(),
             stride=features.stride(),
             length_per_key=features.length_per_key(),
         )
```

## torchrec/modules/fp_embedding_modules.py

```diff
@@ -1,50 +1,49 @@
 #!/usr/bin/env python3
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
-from typing import Dict, Union
+from typing import Dict, List, Set, Union
 
 import torch
 import torch.nn as nn
 from torchrec.modules.embedding_modules import EmbeddingBagCollection
 from torchrec.modules.feature_processor_ import (
     FeatureProcessor,
     FeatureProcessorsCollection,
 )
 from torchrec.sparse.jagged_tensor import KeyedJaggedTensor, KeyedTensor
 
 
 @torch.fx.wrap
 def apply_feature_processors_to_kjt(
     features: KeyedJaggedTensor,
-    feature_processors: nn.ModuleDict,
+    feature_processors: Dict[str, nn.Module],
 ) -> KeyedJaggedTensor:
 
-    if len(features.keys()) == 0:
-        return features
-
     processed_weights = []
     features_dict = features.to_dict()
 
     for key in features.keys():
         jt = features_dict[key]
         if key in feature_processors:
             fp_jt = feature_processors[key](jt)
+            processed_weights.append(fp_jt.weights())
         else:
-            fp_jt = jt
-        processed_weights.append(fp_jt.weights())
+            torch.ones(jt.values().shape[0], device=jt.values().device)
 
     return KeyedJaggedTensor(
         keys=features.keys(),
         values=features.values(),
-        weights=torch.cat(processed_weights),
+        weights=torch.cat(processed_weights)
+        if processed_weights
+        else features.weights_or_none(),
         lengths=features.lengths(),
         offsets=features._offsets,
         stride=features._stride,
         length_per_key=features._length_per_key,
         offset_per_key=features._offset_per_key,
         index_per_key=features._index_per_key,
     )
@@ -94,14 +93,15 @@
         self._embedding_bag_collection = embedding_bag_collection
         self._feature_processors: Union[nn.ModuleDict, FeatureProcessorsCollection]
 
         if isinstance(feature_processors, FeatureProcessorsCollection):
             self._feature_processors = feature_processors
         else:
             self._feature_processors = nn.ModuleDict(feature_processors)
+
             assert set(
                 sum(
                     [
                         config.feature_names
                         for config in self._embedding_bag_collection.embedding_bag_configs()
                     ],
                     [],
@@ -110,14 +110,19 @@
                 feature_processors.keys()
             ), "Passed in feature processors do not match feature names of embedding bag"
 
         assert (
             embedding_bag_collection.is_weighted()
         ), "EmbeddingBagCollection must accept weighted inputs for feature processor"
 
+        feature_names_set: Set[str] = set()
+        for table_config in self._embedding_bag_collection.embedding_bag_configs():
+            feature_names_set.update(table_config.feature_names)
+        self._feature_names: List[str] = list(feature_names_set)
+
     def forward(
         self,
         features: KeyedJaggedTensor,
     ) -> KeyedTensor:
         """
         Args:
             features (KeyedJaggedTensor): KJT of form [F X B X L].
@@ -125,11 +130,14 @@
         Returns:
             KeyedTensor
         """
 
         if isinstance(self._feature_processors, FeatureProcessorsCollection):
             fp_features = self._feature_processors(features)
         else:
+            # TODO: This path isn't currently scriptable. May be hard to support Dict[nn.Module]. Workaround is to always use FP-Collections
             fp_features = apply_feature_processors_to_kjt(
-                features, self._feature_processors
+                features,
+                self._feature_processors,
             )
+
         return self._embedding_bag_collection(fp_features)
```

## Comparing `torchrec_nightly-2023.7.26.dist-info/LICENSE` & `torchrec_nightly-2023.7.29.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchrec_nightly-2023.7.26.dist-info/METADATA` & `torchrec_nightly-2023.7.29.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchrec-nightly
-Version: 2023.7.26
+Version: 2023.7.29
 Summary: Pytorch domain library for recommendation systems
 Home-page: https://github.com/pytorch/torchrec
 Author: TorchRec Team
 Author-email: packages@pytorch.org
 License: BSD-3
 Keywords: pytorch,recommendation systems,sharding
 Classifier: Development Status :: 4 - Beta
```

## Comparing `torchrec_nightly-2023.7.26.dist-info/RECORD` & `torchrec_nightly-2023.7.29.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -13,37 +13,37 @@
 torchrec/datasets/test_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/datasets/test_utils/criteo_test_utils.py,sha256=Ob2fJniGOsfbWNF_Gy2RJhrGAVnLAFPSlUTJOp2kay4,5308
 torchrec/distributed/__init__.py,sha256=VCy8GKOM-1dejxUWNSA3gozG3HQ4x5-Y9c9-WFbAMGg,1912
 torchrec/distributed/batched_embedding_kernel.py,sha256=uBFpJTxGTKKaye-kCA4Ou-sh6rpg_6K0JAfiZhqAyvw,37307
 torchrec/distributed/collective_utils.py,sha256=r7Aawq-KSVC-HjjEd6U8k0vNnRMx_-8_sAhYdElGaJw,2069
 torchrec/distributed/comm.py,sha256=21Std9n_HJCF3Nsw9O4yQQOJuL2DUVzZzoRhH3M6my8,4988
 torchrec/distributed/comm_ops.py,sha256=1GOJQEoRPystHePHLDSf-qavv84MkVwSqLVgYSdg6LU,56136
-torchrec/distributed/dist_data.py,sha256=qGIhMurMuSK45hfGCZBvTUIdOJ4YPrnNlzYrHrQ_oIo,37051
+torchrec/distributed/dist_data.py,sha256=bsRALYTMWxDTqj3DpRctnA6g9pyCx-iIE5dKkCQIA6M,36889
 torchrec/distributed/embedding.py,sha256=ctrHHtWFq5ITPOucAbO2UQ2cRdeDPKRFyHd04lhYmPo,32295
 torchrec/distributed/embedding_kernel.py,sha256=9OA5PDofQZ-ZRORNR4Ca1pJoEZk5Oj2gE6axYwcXIds,4947
-torchrec/distributed/embedding_lookup.py,sha256=-rExTGufuH2qcRzeR2FSSdB9TfqsnVXdz-lghXceVZs,29185
+torchrec/distributed/embedding_lookup.py,sha256=Dk8bDu84T5IAUFlV8kkQa6F4w0n8c-e2WKMknWDxR0U,30169
 torchrec/distributed/embedding_sharding.py,sha256=X088TGuzLb3ou2LQWokn3MNYvo3bNpZT5RzdT1bz9JI,19035
 torchrec/distributed/embedding_tower_sharding.py,sha256=eDHPHzjCstJJRJlE-n8a9lC54lCDsi1iAt4SmkLfza0,36853
-torchrec/distributed/embedding_types.py,sha256=QbDVqeT2wb1RpnGZxrFtFdHYDsOHKW7SH8fLWmN_d0E,15030
+torchrec/distributed/embedding_types.py,sha256=z0LHKSOTNL-18X1zG4eliIUuzVupn3rBCkr8XClxSSA,15386
 torchrec/distributed/embeddingbag.py,sha256=3r_m4bnmyNWPI82_QDZ6vuZVuXnn8siPTDdMmhpX68o,37369
 torchrec/distributed/fbgemm_qcomm_codec.py,sha256=StYltKC6Eq6SE_YiX6GsVW3ZF0VyqTcGHXuCYmPAFlU,7373
-torchrec/distributed/fp_embeddingbag.py,sha256=sC7ZIECzJtwn5LNRrUzf0OH7phI3kQHqQ6wBPaGGvPQ,6137
+torchrec/distributed/fp_embeddingbag.py,sha256=mM4Dnb1_nE5pH-VqwCs0DBYhi87VxjWC8JlMC3sa1qY,6219
 torchrec/distributed/fused_embedding.py,sha256=1VJeW5Dl7EFMvyOfhBvDKZlp39GYucBo8vNFJY2alFI,5243
 torchrec/distributed/fused_embeddingbag.py,sha256=tG_BrUlCdsek87jgHPKbxg3-z13sAgSWPNRBArf2_ss,5080
 torchrec/distributed/fused_params.py,sha256=YEbH5KUphcSWkwLi-JzXUR9SRDm2OykFxP46CJj6nTM,2271
 torchrec/distributed/grouped_position_weighted.py,sha256=q-QE0U306BiPkXIAlJGIQ80EUDZj-FXTbWwjz3EyvLI,3807
 torchrec/distributed/mc_embeddingbag.py,sha256=3_cm7HF53Lj12DJ271yfN1cbjbdxxYRpBCQUWySNsWc,10997
 torchrec/distributed/model_parallel.py,sha256=VV9Vsyas0VKNHr8sX9pq1iT_xJ2b9xjenY3pYAY5HYw,19750
 torchrec/distributed/quant_embedding.py,sha256=9hTS4oUYkoCEHInYO7r0IQjfAJi4fsw5WNiTOHGUJuM,20748
 torchrec/distributed/quant_embedding_kernel.py,sha256=VsROr4bXBkYysS0H7NnlZzF7IvGgZFtgws5jsnPf6g4,15112
 torchrec/distributed/quant_embeddingbag.py,sha256=SJPY-nL95LKozfT8iR4SHt1BNLx4OcoAfFvqKzy1Gf4,12621
 torchrec/distributed/quant_state.py,sha256=PDQ7qUwhFt-q1WvTkoSbWE3rotiFOYyKNFsQ5vJQ36U,13928
 torchrec/distributed/shard.py,sha256=4Dr5ixWCoMEFEuL5WN4fL2gIdl9wmSUjZWsiF-kdCdQ,9261
 torchrec/distributed/sharding_plan.py,sha256=DftThfyzPvfFzg7SvqsNo7X5n-1rxWhp1RlcQCcpq2o,19646
-torchrec/distributed/train_pipeline.py,sha256=BK2EWb8tqSZ406S0NU-_KczHSpvCV2rHRp9Rh8u6A88,37840
+torchrec/distributed/train_pipeline.py,sha256=LKDMhKSddd1pM5gHJlgY0DQR4V_yOnTwrWLsYcnWHIA,47947
 torchrec/distributed/types.py,sha256=aqJ4ipEsMTy9WoTWCRfXg0KhHNcXYC-QRdBp0Db2rHY,27865
 torchrec/distributed/utils.py,sha256=dwNuXu5_OOK0YsGZhGPIdtZu4GzvT6X1MPsARINy10U,15470
 torchrec/distributed/composable/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/distributed/composable/table_batched_embedding_slice.py,sha256=x439M8TTXQtzoihan1OKKbmGYkqJlAxxTHCDz5295RY,3207
 torchrec/distributed/planner/__init__.py,sha256=UWnxb-SuE211uJGdwtSkKRVADT3plQozB2l6fvs6Ve0,1025
 torchrec/distributed/planner/constants.py,sha256=MkeVqYO2QGg57i6fs29lZb2dScaaR9mdQVsee4NxyFc,3135
 torchrec/distributed/planner/enumerators.py,sha256=JoQ1WcrU12Fyy57aBHitcKIpBTASJVnyLhkOxN7V4J4,11430
@@ -113,16 +113,16 @@
 torchrec/modules/activation.py,sha256=o3rGyy40bmmsplyxQy3dDMe8kITRERLJQF7oa5VTKfE,1456
 torchrec/modules/crossnet.py,sha256=nkl21C9_MZf7R4IOTbFGK6EIfQlOBNp2LXV2kQ5ZXsE,14636
 torchrec/modules/deepfm.py,sha256=05ZM2sB1YMVliZwrLHvtMYf3eWGguLFopaZwGZK16bs,8415
 torchrec/modules/embedding_configs.py,sha256=5lb9NeX-hSx6HTwiwV9at70mvAKGjfaKiSpKfx37rBw,6468
 torchrec/modules/embedding_modules.py,sha256=ezRcQ2BqgEEdDF-86Aeribm44YHtaA21NrgOvfr_KYM,14021
 torchrec/modules/embedding_tower.py,sha256=rHiMEUUMoMZUxIPKBE2HyI7I5uNthGQsZIxH038TmoI,4858
 torchrec/modules/feature_processor.py,sha256=UqDELZQ7cRpAQb_6LyxwVByJDdM5NPDfAyEk4qYQmKM,12360
-torchrec/modules/feature_processor_.py,sha256=y6OSfvgmNSvhe00Xwy69Yqu5mwkF24nO2I4G5iu_ONc,4832
-torchrec/modules/fp_embedding_modules.py,sha256=AdUxhEODvE-h0FbqQoD0o-haLBgMRhXl2O0gvcstxI0,4403
+torchrec/modules/feature_processor_.py,sha256=yQckTbhOmDiOISi8kA9wEqdIyul1hwVn_GhYIksye5o,4964
+torchrec/modules/fp_embedding_modules.py,sha256=mfdvqxBMGktVqcGu6zcb-J4bPcxdf8ZbNzCoVzpQIXQ,4903
 torchrec/modules/fused_embedding_modules.py,sha256=4nyD7aJYasPXqQElg8u3Wmh1n_t-BSe3rd2qxN4WO-k,31545
 torchrec/modules/lazy_extension.py,sha256=EOCfDLP0x-n3iEpKYaPPBgWP3nbaOCv-oG087dqwtGo,10533
 torchrec/modules/managed_collision_modules.py,sha256=Au4LOxNEanXBtbOzlEi-xOnHKlj7NccZB9y7sHELRRM,3143
 torchrec/modules/mc_embedding_modules.py,sha256=z7l0cXKE9uy4GeDYsDCsdfJcPWJK5C4KMTRTUeLW37o,5124
 torchrec/modules/mlp.py,sha256=lV3vCEmNLI1kDtF4eGlCaGvtG3FQ8UEMs4Hcc6Nf3pM,6309
 torchrec/modules/utils.py,sha256=4xCBc_dTQ8CFNLdGtJcCa3Ei0zScfkZCSmnSxOq51vk,3897
 torchrec/optim/__init__.py,sha256=4-nuv6JsmuWBtzyUvCKJ0b9-m7oFi8QB2b3krFF0KgQ,1639
@@ -137,12 +137,12 @@
 torchrec/quant/__init__.py,sha256=A6NIA6ztq6iP1JTLRLNzlgnCcd-LaN8efnxGub3Ii4A,1140
 torchrec/quant/embedding_modules.py,sha256=-_5_HaoNpt3AmL7fapO2Wj2r6WPjbq2VvATNmGhrai0,26618
 torchrec/quant/utils.py,sha256=rcyo5LDcLK49VLs6ZFxOHeutblWZunDAM_T-0NsraDE,4292
 torchrec/sparse/__init__.py,sha256=dLqSye4Jo6obnNNTUKdPDxPQb9sL2U4weemSn-DjpYk,1163
 torchrec/sparse/jagged_tensor.py,sha256=__iBD2tBQE9ZZpxcOn8RVmDF9gognnyUlmMjedHxJ4E,56412
 torchrec/sparse/test_utils/__init__.py,sha256=BLxfGKJvwjjCiQM64O5wGAA_Cea0sG-buw9lTDWuqug,1430
 torchrec/test_utils/__init__.py,sha256=JncJcXS4N3gI7-fsizQ2-qiWM6MhIrpvskF_9gDf0Go,5661
-torchrec_nightly-2023.7.26.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
-torchrec_nightly-2023.7.26.dist-info/METADATA,sha256=56FhvteEkVGvZe78Xt0ziEt7zaJw2WRHOk4-_YfCrfY,5012
-torchrec_nightly-2023.7.26.dist-info/WHEEL,sha256=LFkqBJk4I7hCv0noSqfZr_dqF1ghJkoTPb1XzfEVHuI,93
-torchrec_nightly-2023.7.26.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
-torchrec_nightly-2023.7.26.dist-info/RECORD,,
+torchrec_nightly-2023.7.29.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
+torchrec_nightly-2023.7.29.dist-info/METADATA,sha256=52fqsDMcS01SyzmulwCbdMPfB-Hwi0lFNNS1nEOcwaE,5012
+torchrec_nightly-2023.7.29.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
+torchrec_nightly-2023.7.29.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
+torchrec_nightly-2023.7.29.dist-info/RECORD,,
```


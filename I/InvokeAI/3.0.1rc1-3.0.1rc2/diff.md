# Comparing `tmp/InvokeAI-3.0.1rc1.tar.gz` & `tmp/InvokeAI-3.0.1rc2.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "InvokeAI-3.0.1rc1.tar", last modified: Thu Jul 27 04:29:47 2023, max compression
+gzip compressed data, was "InvokeAI-3.0.1rc2.tar", last modified: Thu Jul 27 19:57:14 2023, max compression
```

## Comparing `InvokeAI-3.0.1rc1.tar` & `InvokeAI-3.0.1rc2.tar`

### file list

```diff
@@ -1,261 +1,261 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.683507 InvokeAI-3.0.1rc1/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.647506 InvokeAI-3.0.1rc1/InvokeAI.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    30363 2023-07-27 04:29:47.000000 InvokeAI-3.0.1rc1/InvokeAI.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     9449 2023-07-27 04:29:47.000000 InvokeAI-3.0.1rc1/InvokeAI.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-27 04:29:47.000000 InvokeAI-3.0.1rc1/InvokeAI.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      838 2023-07-27 04:29:47.000000 InvokeAI-3.0.1rc1/InvokeAI.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)     1155 2023-07-27 04:29:47.000000 InvokeAI-3.0.1rc1/InvokeAI.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        9 2023-07-27 04:29:47.000000 InvokeAI-3.0.1rc1/InvokeAI.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)    10146 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)    14555 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/LICENSE-SD1+SD2.txt
--rw-r--r--   0 runner    (1001) docker     (123)    14213 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/LICENSE-SDXL.txt
--rw-r--r--   0 runner    (1001) docker     (123)    30363 2023-07-27 04:29:47.683507 InvokeAI-3.0.1rc1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    17058 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.647506 InvokeAI-3.0.1rc1/invokeai/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.647506 InvokeAI-3.0.1rc1/invokeai/app/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.647506 InvokeAI-3.0.1rc1/invokeai/app/api/
--rw-r--r--   0 runner    (1001) docker     (123)     5268 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/dependencies.py
--rw-r--r--   0 runner    (1001) docker     (123)     1641 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/events.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.647506 InvokeAI-3.0.1rc1/invokeai/app/api/routers/
--rw-r--r--   0 runner    (1001) docker     (123)     3475 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/routers/app_info.py
--rw-r--r--   0 runner    (1001) docker     (123)     1884 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/routers/board_images.py
--rw-r--r--   0 runner    (1001) docker     (123)     5457 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/routers/boards.py
--rw-r--r--   0 runner    (1001) docker     (123)     8845 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/routers/images.py
--rw-r--r--   0 runner    (1001) docker     (123)    17094 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/routers/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     9818 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/routers/sessions.py
--rw-r--r--   0 runner    (1001) docker     (123)     1237 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api/sockets.py
--rw-r--r--   0 runner    (1001) docker     (123)     7604 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/api_app.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.651506 InvokeAI-3.0.1rc1/invokeai/app/cli/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10153 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/cli/commands.py
--rw-r--r--   0 runner    (1001) docker     (123)     5835 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/cli/completer.py
--rw-r--r--   0 runner    (1001) docker     (123)    18442 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/cli_app.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.651506 InvokeAI-3.0.1rc1/invokeai/app/invocations/
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4286 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/baseinvocation.py
--rw-r--r--   0 runner    (1001) docker     (123)     4899 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/collections.py
--rw-r--r--   0 runner    (1001) docker     (123)    27374 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/compel.py
--rw-r--r--   0 runner    (1001) docker     (123)    30040 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/controlnet_image_processors.py
--rw-r--r--   0 runner    (1001) docker     (123)     2477 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/cv.py
--rw-r--r--   0 runner    (1001) docker     (123)     9864 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/generate.py
--rw-r--r--   0 runner    (1001) docker     (123)    23887 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/image.py
--rw-r--r--   0 runner    (1001) docker     (123)     8041 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/infill.py
--rw-r--r--   0 runner    (1001) docker     (123)    30373 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/latent.py
--rw-r--r--   0 runner    (1001) docker     (123)     3982 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/math.py
--rw-r--r--   0 runner    (1001) docker     (123)     7335 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)    10843 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/model.py
--rw-r--r--   0 runner    (1001) docker     (123)     3608 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/noise.py
--rw-r--r--   0 runner    (1001) docker     (123)    11390 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/param_easing.py
--rw-r--r--   0 runner    (1001) docker     (123)     2039 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/params.py
--rw-r--r--   0 runner    (1001) docker     (123)     4192 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/prompt.py
--rw-r--r--   0 runner    (1001) docker     (123)    32559 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/sdxl.py
--rw-r--r--   0 runner    (1001) docker     (123)     4321 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/invocations/upscale.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.651506 InvokeAI-3.0.1rc1/invokeai/app/models/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)       83 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/models/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/models/image.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.655506 InvokeAI-3.0.1rc1/invokeai/app/services/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8414 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/board_image_record_storage.py
--rw-r--r--   0 runner    (1001) docker     (123)     3593 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/board_images.py
--rw-r--r--   0 runner    (1001) docker     (123)     9364 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/board_record_storage.py
--rw-r--r--   0 runner    (1001) docker     (123)     5505 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/boards.py
--rw-r--r--   0 runner    (1001) docker     (123)    23173 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3560 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/default_graphs.py
--rw-r--r--   0 runner    (1001) docker     (123)     6113 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/events.py
--rw-r--r--   0 runner    (1001) docker     (123)    47629 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     6777 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/image_file_storage.py
--rw-r--r--   0 runner    (1001) docker     (123)    18445 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/image_record_storage.py
--rw-r--r--   0 runner    (1001) docker     (123)    14890 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/images.py
--rw-r--r--   0 runner    (1001) docker     (123)     2204 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/invocation_queue.py
--rw-r--r--   0 runner    (1001) docker     (123)     2642 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/invocation_services.py
--rw-r--r--   0 runner    (1001) docker     (123)     2880 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/invoker.py
--rw-r--r--   0 runner    (1001) docker     (123)     2228 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/item_storage.py
--rw-r--r--   0 runner    (1001) docker     (123)     3030 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/latent_storage.py
--rw-r--r--   0 runner    (1001) docker     (123)    24186 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/model_manager_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.655506 InvokeAI-3.0.1rc1/invokeai/app/services/models/
--rw-r--r--   0 runner    (1001) docker     (123)     2314 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/models/board_record.py
--rw-r--r--   0 runner    (1001) docker     (123)     5408 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/models/image_record.py
--rw-r--r--   0 runner    (1001) docker     (123)     7480 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/processor.py
--rw-r--r--   0 runner    (1001) docker     (123)      756 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/resource_name.py
--rw-r--r--   0 runner    (1001) docker     (123)     4795 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/sqlite.py
--rw-r--r--   0 runner    (1001) docker     (123)      804 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/services/urls.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.659506 InvokeAI-3.0.1rc1/invokeai/app/util/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/util/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13179 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/util/controlnet_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1561 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/util/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      319 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/util/metaenum.py
--rw-r--r--   0 runner    (1001) docker     (123)      497 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/util/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)     4851 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/util/step_callback.py
--rw-r--r--   0 runner    (1001) docker     (123)      467 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/app/util/thumbnails.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.659506 InvokeAI-3.0.1rc1/invokeai/backend/
--rw-r--r--   0 runner    (1001) docker     (123)      362 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.659506 InvokeAI-3.0.1rc1/invokeai/backend/generator/
--rw-r--r--   0 runner    (1001) docker     (123)      248 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/generator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    21883 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/generator/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3180 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/generator/img2img.py
--rw-r--r--   0 runner    (1001) docker     (123)    13438 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/generator/inpaint.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.659506 InvokeAI-3.0.1rc1/invokeai/backend/image_util/
--rw-r--r--   0 runner    (1001) docker     (123)      675 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/image_util/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1266 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/image_util/invisible_watermark.py
--rw-r--r--   0 runner    (1001) docker     (123)     1408 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/image_util/patchmatch.py
--rw-r--r--   0 runner    (1001) docker     (123)     4360 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/image_util/pngwriter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2389 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/image_util/safety_checker.py
--rw-r--r--   0 runner    (1001) docker     (123)     2242 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/image_util/seamless.py
--rw-r--r--   0 runner    (1001) docker     (123)     4789 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/image_util/txt2mask.py
--rw-r--r--   0 runner    (1001) docker     (123)     2364 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/image_util/util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.659506 InvokeAI-3.0.1rc1/invokeai/backend/install/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/install/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/install/check_root.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    29082 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/install/invokeai_configure.py
--rw-r--r--   0 runner    (1001) docker     (123)    11559 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/install/legacy_arg_parsing.py
--rw-r--r--   0 runner    (1001) docker     (123)    24063 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/install/migrate_to_3.py
--rw-r--r--   0 runner    (1001) docker     (123)    20401 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/install/model_install_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.663506 InvokeAI-3.0.1rc1/invokeai/backend/model_management/
--rw-r--r--   0 runner    (1001) docker     (123)      384 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    74478 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/convert_ckpt_to_diffusers.py
--rw-r--r--   0 runner    (1001) docker     (123)    21816 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/lora.py
--rw-r--r--   0 runner    (1001) docker     (123)    15688 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_cache.py
--rw-r--r--   0 runner    (1001) docker     (123)    39638 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     6870 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_merge.py
--rw-r--r--   0 runner    (1001) docker     (123)    20118 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_probe.py
--rw-r--r--   0 runner    (1001) docker     (123)     3378 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_search.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.663506 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/
--rw-r--r--   0 runner    (1001) docker     (123)     4329 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14224 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5026 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/controlnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     2490 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/lora.py
--rw-r--r--   0 runner    (1001) docker     (123)     4368 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/sdxl.py
--rw-r--r--   0 runner    (1001) docker     (123)    11849 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/stable_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (123)     2487 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/textual_inversion.py
--rw-r--r--   0 runner    (1001) docker     (123)     5602 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/vae.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.663506 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/
--rw-r--r--   0 runner    (1001) docker     (123)      394 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    40437 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusers_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.663506 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusion/
--rw-r--r--   0 runner    (1001) docker     (123)      270 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    30667 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusion/cross_attention_control.py
--rw-r--r--   0 runner    (1001) docker     (123)     4223 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusion/cross_attention_map_saving.py
--rw-r--r--   0 runner    (1001) docker     (123)    25101 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusion/shared_invokeai_diffusion.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.663506 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/image_degradation/
--rw-r--r--   0 runner    (1001) docker     (123)      226 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/image_degradation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    26273 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/image_degradation/bsrgan.py
--rw-r--r--   0 runner    (1001) docker     (123)    23118 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/image_degradation/bsrgan_light.py
--rw-r--r--   0 runner    (1001) docker     (123)    29836 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/image_degradation/utils_image.py
--rw-r--r--   0 runner    (1001) docker     (123)     8139 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/offloading.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.663506 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/schedulers/
--rw-r--r--   0 runner    (1001) docker     (123)       37 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/schedulers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1866 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/schedulers/schedulers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.667506 InvokeAI-3.0.1rc1/invokeai/backend/training/
--rw-r--r--   0 runner    (1001) docker     (123)      140 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    36344 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/training/textual_inversion_training.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.667506 InvokeAI-3.0.1rc1/invokeai/backend/util/
--rw-r--r--   0 runner    (1001) docker     (123)      369 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/util/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2176 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/util/devices.py
--rw-r--r--   0 runner    (1001) docker     (123)    30638 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/util/hotfixes.py
--rw-r--r--   0 runner    (1001) docker     (123)     2396 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/util/log.py
--rw-r--r--   0 runner    (1001) docker     (123)    14189 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/util/logging.py
--rw-r--r--   0 runner    (1001) docker     (123)     8099 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/util/mps_fixes.py
--rw-r--r--   0 runner    (1001) docker     (123)    11958 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/util/util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.667506 InvokeAI-3.0.1rc1/invokeai/backend/web/
--rw-r--r--   0 runner    (1001) docker     (123)      101 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/web/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    69421 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/web/invoke_ai_web_server.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.667506 InvokeAI-3.0.1rc1/invokeai/backend/web/modules/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/web/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1599 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/web/modules/create_cmd_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     3716 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/web/modules/get_canvas_generation_mode.py
--rw-r--r--   0 runner    (1001) docker     (123)     2830 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/web/modules/parameters.py
--rw-r--r--   0 runner    (1001) docker     (123)     1224 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/backend/web/modules/parse_seed_weights.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.667506 InvokeAI-3.0.1rc1/invokeai/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     4516 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/INITIAL_MODELS.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1791 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/models.yaml.example
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.671507 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/
--rw-r--r--   0 runner    (1001) docker     (123)     3246 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/sd_xl_base.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2948 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/sd_xl_refiner.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2803 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-finetune.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2664 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-finetune_style.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2291 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-inference.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2409 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-inpainting-inference.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2791 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-m1-finetune.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1905 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v2-inference-v.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1879 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v2-inference.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     4475 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v2-inpainting-inference-v.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     4449 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v2-inpainting-inference.yaml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.671507 InvokeAI-3.0.1rc1/invokeai/frontend/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.671507 InvokeAI-3.0.1rc1/invokeai/frontend/CLI/
--rw-r--r--   0 runner    (1001) docker     (123)      111 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/CLI/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)       50 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.671507 InvokeAI-3.0.1rc1/invokeai/frontend/install/
--rw-r--r--   0 runner    (1001) docker     (123)      227 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/install/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      121 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/install/invokeai_configure.py
--rw-r--r--   0 runner    (1001) docker     (123)     4179 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/install/invokeai_update.py
--rw-r--r--   0 runner    (1001) docker     (123)    30362 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/install/model_install.py
--rw-r--r--   0 runner    (1001) docker     (123)    13834 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/install/widgets.py
--rw-r--r--   0 runner    (1001) docker     (123)      441 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/legacy_launch_invokeai.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.671507 InvokeAI-3.0.1rc1/invokeai/frontend/merge/
--rw-r--r--   0 runner    (1001) docker     (123)      119 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/merge/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14377 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/merge/merge_diffusers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.671507 InvokeAI-3.0.1rc1/invokeai/frontend/training/
--rw-r--r--   0 runner    (1001) docker     (123)      125 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/training/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    15781 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/training/textual_inversion.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.671507 InvokeAI-3.0.1rc1/invokeai/frontend/web/
--rw-r--r--   0 runner    (1001) docker     (123)       54 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.671507 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.679507 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/
--rw-r--r--   0 runner    (1001) docker     (123)     7277 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/App-6125620a.css
--rw-r--r--   0 runner    (1001) docker     (123)   924409 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/App-69e5ea36.js
--rw-r--r--   0 runner    (1001) docker     (123)    38129 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/MantineProvider-8184f020.js
--rw-r--r--   0 runner    (1001) docker     (123)    15813 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/ThemeLocaleProvider-5b992bc7.css
--rw-r--r--   0 runner    (1001) docker     (123)    80546 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/ThemeLocaleProvider-9ac72450.js
--rw-r--r--   0 runner    (1001) docker     (123)   118734 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/favicon-0d253ced.ico
--rw-r--r--   0 runner    (1001) docker     (123)  1210266 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/index-89941396.js
--rw-r--r--   0 runner    (1001) docker     (123)    26692 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-cyrillic-ext-wght-normal-848492d3.woff2
--rw-r--r--   0 runner    (1001) docker     (123)    17076 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-cyrillic-wght-normal-262a1054.woff2
--rw-r--r--   0 runner    (1001) docker     (123)    11952 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-greek-ext-wght-normal-fe977ddb.woff2
--rw-r--r--   0 runner    (1001) docker     (123)    22000 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-greek-wght-normal-89b4a3fe.woff2
--rw-r--r--   0 runner    (1001) docker     (123)    56968 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-latin-ext-wght-normal-45606f83.woff2
--rw-r--r--   0 runner    (1001) docker     (123)    37924 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-latin-wght-normal-450f3ba4.woff2
--rw-r--r--   0 runner    (1001) docker     (123)     8640 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-vietnamese-wght-normal-ac4e131c.woff2
--rw-r--r--   0 runner    (1001) docker     (123)    44115 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/logo-13003d72.png
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/index.html
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.683507 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/
--rw-r--r--   0 runner    (1001) docker     (123)    31054 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ar.json
--rw-r--r--   0 runner    (1001) docker     (123)    27068 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/de.json
--rw-r--r--   0 runner    (1001) docker     (123)    33140 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/en.json
--rw-r--r--   0 runner    (1001) docker     (123)    33031 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/es.json
--rw-r--r--   0 runner    (1001) docker     (123)     5926 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/fi.json
--rw-r--r--   0 runner    (1001) docker     (123)    28008 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/fr.json
--rw-r--r--   0 runner    (1001) docker     (123)    34036 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/he.json
--rw-r--r--   0 runner    (1001) docker     (123)    33035 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/it.json
--rw-r--r--   0 runner    (1001) docker     (123)    24998 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ja.json
--rw-r--r--   0 runner    (1001) docker     (123)     4276 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ko.json
--rw-r--r--   0 runner    (1001) docker     (123)        3 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/mn.json
--rw-r--r--   0 runner    (1001) docker     (123)    31594 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/nl.json
--rw-r--r--   0 runner    (1001) docker     (123)    22184 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/pl.json
--rw-r--r--   0 runner    (1001) docker     (123)    31239 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/pt.json
--rw-r--r--   0 runner    (1001) docker     (123)    29993 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/pt_BR.json
--rw-r--r--   0 runner    (1001) docker     (123)        3 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ro.json
--rw-r--r--   0 runner    (1001) docker     (123)    42731 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ru.json
--rw-r--r--   0 runner    (1001) docker     (123)    10330 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/sv.json
--rw-r--r--   0 runner    (1001) docker     (123)     3203 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/tr.json
--rw-r--r--   0 runner    (1001) docker     (123)    42582 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/uk.json
--rw-r--r--   0 runner    (1001) docker     (123)        3 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/vi.json
--rw-r--r--   0 runner    (1001) docker     (123)    20391 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/zh_CN.json
--rw-r--r--   0 runner    (1001) docker     (123)     1325 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/zh_Hant.json
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.647506 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.683507 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/
--rw-r--r--   0 runner    (1001) docker     (123)     1150 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/favicon.ico
--rw-r--r--   0 runner    (1001) docker     (123)     3063 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/index.css
--rw-r--r--   0 runner    (1001) docker     (123)     8921 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/index.html
--rw-r--r--   0 runner    (1001) docker     (123)    12465 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/index.js
--rw-r--r--   0 runner    (1001) docker     (123)     9927 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/test.html
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.683507 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/
--rw-r--r--   0 runner    (1001) docker     (123)     1150 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/favicon.ico
--rw-r--r--   0 runner    (1001) docker     (123)     2472 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/index.css
--rw-r--r--   0 runner    (1001) docker     (123)     7705 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/index.html
--rw-r--r--   0 runner    (1001) docker     (123)     7135 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/index.js
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 04:29:47.683507 InvokeAI-3.0.1rc1/invokeai/version/
--rw-r--r--   0 runner    (1001) docker     (123)      497 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/version/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)       25 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/invokeai/version/invokeai_version.py
--rw-r--r--   0 runner    (1001) docker     (123)     5900 2023-07-27 04:29:30.000000 InvokeAI-3.0.1rc1/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-07-27 04:29:47.683507 InvokeAI-3.0.1rc1/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.960403 InvokeAI-3.0.1rc2/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.920403 InvokeAI-3.0.1rc2/InvokeAI.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    30368 2023-07-27 19:57:14.000000 InvokeAI-3.0.1rc2/InvokeAI.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     9449 2023-07-27 19:57:14.000000 InvokeAI-3.0.1rc2/InvokeAI.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-27 19:57:14.000000 InvokeAI-3.0.1rc2/InvokeAI.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      838 2023-07-27 19:57:14.000000 InvokeAI-3.0.1rc2/InvokeAI.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1157 2023-07-27 19:57:14.000000 InvokeAI-3.0.1rc2/InvokeAI.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        9 2023-07-27 19:57:14.000000 InvokeAI-3.0.1rc2/InvokeAI.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    10146 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)    14555 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/LICENSE-SD1+SD2.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    14213 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/LICENSE-SDXL.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    30368 2023-07-27 19:57:14.960403 InvokeAI-3.0.1rc2/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    17063 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.920403 InvokeAI-3.0.1rc2/invokeai/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.920403 InvokeAI-3.0.1rc2/invokeai/app/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.920403 InvokeAI-3.0.1rc2/invokeai/app/api/
+-rw-r--r--   0 runner    (1001) docker     (123)     5216 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/dependencies.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1641 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/events.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.924403 InvokeAI-3.0.1rc2/invokeai/app/api/routers/
+-rw-r--r--   0 runner    (1001) docker     (123)     3406 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/routers/app_info.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1883 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/routers/board_images.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5357 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/routers/boards.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8681 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/routers/images.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15498 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/routers/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9735 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/routers/sessions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1215 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api/sockets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7823 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/api_app.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.924403 InvokeAI-3.0.1rc2/invokeai/app/cli/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10161 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/cli/commands.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5831 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/cli/completer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18340 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/cli_app.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.928403 InvokeAI-3.0.1rc2/invokeai/app/invocations/
+-rw-r--r--   0 runner    (1001) docker     (123)      229 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4264 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/baseinvocation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4534 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26470 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/compel.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28543 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/controlnet_image_processors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2431 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/cv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9737 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/generate.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23033 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/image.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7797 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/infill.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30600 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/latent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3752 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/math.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7098 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10739 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3608 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/noise.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11203 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/param_easing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1913 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/params.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4110 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/prompt.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32529 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4239 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/invocations/upscale.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.928403 InvokeAI-3.0.1rc2/invokeai/app/models/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)       84 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/models/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4616 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/models/image.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.932403 InvokeAI-3.0.1rc2/invokeai/app/services/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8392 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/board_image_record_storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3565 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/board_images.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9320 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/board_record_storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5262 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/boards.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23491 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4009 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/default_graphs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6067 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/events.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45997 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6711 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/image_file_storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18390 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/image_record_storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14668 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/images.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2225 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/invocation_queue.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2642 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/invocation_services.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2867 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/invoker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2218 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/item_storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3013 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/latent_storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23739 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/model_manager_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.932403 InvokeAI-3.0.1rc2/invokeai/app/services/models/
+-rw-r--r--   0 runner    (1001) docker     (123)     2244 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/models/board_record.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5296 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/models/image_record.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7120 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)      756 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/resource_name.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4647 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/sqlite.py
+-rw-r--r--   0 runner    (1001) docker     (123)      804 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/services/urls.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.932403 InvokeAI-3.0.1rc2/invokeai/app/util/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/util/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13013 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/util/controlnet_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1561 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/util/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      319 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/util/metaenum.py
+-rw-r--r--   0 runner    (1001) docker     (123)      497 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/util/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4823 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/util/step_callback.py
+-rw-r--r--   0 runner    (1001) docker     (123)      467 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/app/util/thumbnails.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.932403 InvokeAI-3.0.1rc2/invokeai/backend/
+-rw-r--r--   0 runner    (1001) docker     (123)      322 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.932403 InvokeAI-3.0.1rc2/invokeai/backend/generator/
+-rw-r--r--   0 runner    (1001) docker     (123)      248 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/generator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20982 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/generator/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3102 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/generator/img2img.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13130 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/generator/inpaint.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.936403 InvokeAI-3.0.1rc2/invokeai/backend/image_util/
+-rw-r--r--   0 runner    (1001) docker     (123)      669 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/image_util/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1243 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/image_util/invisible_watermark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1410 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/image_util/patchmatch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4316 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/image_util/pngwriter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2318 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/image_util/safety_checker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2134 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/image_util/seamless.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4602 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/image_util/txt2mask.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2364 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/image_util/util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.936403 InvokeAI-3.0.1rc2/invokeai/backend/install/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/install/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1455 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/install/check_root.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    28561 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/install/invokeai_configure.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11431 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/install/legacy_arg_parsing.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22785 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/install/migrate_to_3.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20234 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/install/model_install_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.936403 InvokeAI-3.0.1rc2/invokeai/backend/model_management/
+-rw-r--r--   0 runner    (1001) docker     (123)      412 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    74600 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/convert_ckpt_to_diffusers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21815 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/lora.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15703 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_cache.py
+-rw-r--r--   0 runner    (1001) docker     (123)    39447 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6703 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_merge.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19986 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_probe.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3482 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_search.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.940403 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/
+-rw-r--r--   0 runner    (1001) docker     (123)     4466 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14291 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4967 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/controlnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2495 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/lora.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4358 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11654 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/stable_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2491 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/textual_inversion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5577 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/vae.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.940403 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (123)      394 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    39975 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusers_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.940403 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusion/
+-rw-r--r--   0 runner    (1001) docker     (123)      270 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29639 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusion/cross_attention_control.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4125 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusion/cross_attention_map_saving.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24890 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusion/shared_invokeai_diffusion.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.940403 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/image_degradation/
+-rw-r--r--   0 runner    (1001) docker     (123)      226 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/image_degradation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25969 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/image_degradation/bsrgan.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22836 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/image_degradation/bsrgan_light.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29531 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/image_degradation/utils_image.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7955 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/offloading.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.940403 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/schedulers/
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/schedulers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1910 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/schedulers/schedulers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.940403 InvokeAI-3.0.1rc2/invokeai/backend/training/
+-rw-r--r--   0 runner    (1001) docker     (123)      140 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35304 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/training/textual_inversion_training.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.940403 InvokeAI-3.0.1rc2/invokeai/backend/util/
+-rw-r--r--   0 runner    (1001) docker     (123)      343 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/util/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2177 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/util/devices.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30611 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/util/hotfixes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2396 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/util/log.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14210 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/util/logging.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8324 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/util/mps_fixes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11733 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/util/util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.944403 InvokeAI-3.0.1rc2/invokeai/backend/web/
+-rw-r--r--   0 runner    (1001) docker     (123)      101 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/web/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    66455 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/web/invoke_ai_web_server.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.944403 InvokeAI-3.0.1rc2/invokeai/backend/web/modules/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/web/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1599 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/web/modules/create_cmd_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3692 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/web/modules/get_canvas_generation_mode.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2800 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/web/modules/parameters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1224 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/backend/web/modules/parse_seed_weights.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.944403 InvokeAI-3.0.1rc2/invokeai/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     4329 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/INITIAL_MODELS.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1791 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/models.yaml.example
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.944403 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/
+-rw-r--r--   0 runner    (1001) docker     (123)     3246 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/sd_xl_base.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2948 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/sd_xl_refiner.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2803 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-finetune.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2664 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-finetune_style.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2291 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-inference.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2409 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-inpainting-inference.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2791 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-m1-finetune.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1905 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v2-inference-v.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1879 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v2-inference.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     4475 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v2-inpainting-inference-v.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     4449 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v2-inpainting-inference.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.944403 InvokeAI-3.0.1rc2/invokeai/frontend/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.944403 InvokeAI-3.0.1rc2/invokeai/frontend/CLI/
+-rw-r--r--   0 runner    (1001) docker     (123)      111 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/CLI/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)       50 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.948403 InvokeAI-3.0.1rc2/invokeai/frontend/install/
+-rw-r--r--   0 runner    (1001) docker     (123)      227 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/install/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/install/invokeai_configure.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4230 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/install/invokeai_update.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29786 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/install/model_install.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13763 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/install/widgets.py
+-rw-r--r--   0 runner    (1001) docker     (123)      446 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/legacy_launch_invokeai.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.948403 InvokeAI-3.0.1rc2/invokeai/frontend/merge/
+-rw-r--r--   0 runner    (1001) docker     (123)      118 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/merge/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14230 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/merge/merge_diffusers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.948403 InvokeAI-3.0.1rc2/invokeai/frontend/training/
+-rw-r--r--   0 runner    (1001) docker     (123)      125 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/training/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    15426 2023-07-27 19:57:00.000000 InvokeAI-3.0.1rc2/invokeai/frontend/training/textual_inversion.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.948403 InvokeAI-3.0.1rc2/invokeai/frontend/web/
+-rw-r--r--   0 runner    (1001) docker     (123)       54 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.948403 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.952403 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/
+-rw-r--r--   0 runner    (1001) docker     (123)   924409 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/App-58b095d3.js
+-rw-r--r--   0 runner    (1001) docker     (123)     7277 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/App-6125620a.css
+-rw-r--r--   0 runner    (1001) docker     (123)    38129 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/MantineProvider-ea42d3d1.js
+-rw-r--r--   0 runner    (1001) docker     (123)    80546 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/ThemeLocaleProvider-13e3db3d.js
+-rw-r--r--   0 runner    (1001) docker     (123)    15813 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/ThemeLocaleProvider-5b992bc7.css
+-rw-r--r--   0 runner    (1001) docker     (123)   118734 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/favicon-0d253ced.ico
+-rw-r--r--   0 runner    (1001) docker     (123)  1210646 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/index-5a784cdd.js
+-rw-r--r--   0 runner    (1001) docker     (123)    26692 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-cyrillic-ext-wght-normal-848492d3.woff2
+-rw-r--r--   0 runner    (1001) docker     (123)    17076 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-cyrillic-wght-normal-262a1054.woff2
+-rw-r--r--   0 runner    (1001) docker     (123)    11952 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-greek-ext-wght-normal-fe977ddb.woff2
+-rw-r--r--   0 runner    (1001) docker     (123)    22000 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-greek-wght-normal-89b4a3fe.woff2
+-rw-r--r--   0 runner    (1001) docker     (123)    56968 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-latin-ext-wght-normal-45606f83.woff2
+-rw-r--r--   0 runner    (1001) docker     (123)    37924 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-latin-wght-normal-450f3ba4.woff2
+-rw-r--r--   0 runner    (1001) docker     (123)     8640 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-vietnamese-wght-normal-ac4e131c.woff2
+-rw-r--r--   0 runner    (1001) docker     (123)    44115 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/logo-13003d72.png
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/index.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.956403 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/
+-rw-r--r--   0 runner    (1001) docker     (123)    31054 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ar.json
+-rw-r--r--   0 runner    (1001) docker     (123)    27068 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/de.json
+-rw-r--r--   0 runner    (1001) docker     (123)    33140 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/en.json
+-rw-r--r--   0 runner    (1001) docker     (123)    33031 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/es.json
+-rw-r--r--   0 runner    (1001) docker     (123)     5926 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/fi.json
+-rw-r--r--   0 runner    (1001) docker     (123)    28008 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/fr.json
+-rw-r--r--   0 runner    (1001) docker     (123)    34036 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/he.json
+-rw-r--r--   0 runner    (1001) docker     (123)    33035 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/it.json
+-rw-r--r--   0 runner    (1001) docker     (123)    24998 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ja.json
+-rw-r--r--   0 runner    (1001) docker     (123)     4276 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ko.json
+-rw-r--r--   0 runner    (1001) docker     (123)        3 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/mn.json
+-rw-r--r--   0 runner    (1001) docker     (123)    31594 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/nl.json
+-rw-r--r--   0 runner    (1001) docker     (123)    22184 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/pl.json
+-rw-r--r--   0 runner    (1001) docker     (123)    31239 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/pt.json
+-rw-r--r--   0 runner    (1001) docker     (123)    29993 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/pt_BR.json
+-rw-r--r--   0 runner    (1001) docker     (123)        3 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ro.json
+-rw-r--r--   0 runner    (1001) docker     (123)    42731 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ru.json
+-rw-r--r--   0 runner    (1001) docker     (123)    10330 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/sv.json
+-rw-r--r--   0 runner    (1001) docker     (123)     3203 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/tr.json
+-rw-r--r--   0 runner    (1001) docker     (123)    42582 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/uk.json
+-rw-r--r--   0 runner    (1001) docker     (123)        3 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/vi.json
+-rw-r--r--   0 runner    (1001) docker     (123)    20391 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/zh_CN.json
+-rw-r--r--   0 runner    (1001) docker     (123)     1325 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/zh_Hant.json
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.920403 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.960403 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/
+-rw-r--r--   0 runner    (1001) docker     (123)     1150 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/favicon.ico
+-rw-r--r--   0 runner    (1001) docker     (123)     3063 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/index.css
+-rw-r--r--   0 runner    (1001) docker     (123)     8921 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/index.html
+-rw-r--r--   0 runner    (1001) docker     (123)    12465 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/index.js
+-rw-r--r--   0 runner    (1001) docker     (123)     9927 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/test.html
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.960403 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/
+-rw-r--r--   0 runner    (1001) docker     (123)     1150 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/favicon.ico
+-rw-r--r--   0 runner    (1001) docker     (123)     2472 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/index.css
+-rw-r--r--   0 runner    (1001) docker     (123)     7705 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/index.html
+-rw-r--r--   0 runner    (1001) docker     (123)     7135 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/index.js
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-27 19:57:14.960403 InvokeAI-3.0.1rc2/invokeai/version/
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/version/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)       25 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/invokeai/version/invokeai_version.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5925 2023-07-27 19:57:01.000000 InvokeAI-3.0.1rc2/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-07-27 19:57:14.960403 InvokeAI-3.0.1rc2/setup.cfg
```

### Comparing `InvokeAI-3.0.1rc1/InvokeAI.egg-info/PKG-INFO` & `InvokeAI-3.0.1rc2/InvokeAI.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: InvokeAI
-Version: 3.0.1rc1
+Version: 3.0.1rc2
 Summary: An implementation of Stable Diffusion which provides various new features and options to aid the image generation process
 Author-email: The InvokeAI Project <lincoln.stein@gmail.com>
 License:                                  Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
            TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
@@ -200,15 +200,15 @@
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Topic :: Artistic Software
 Classifier: Topic :: Internet :: WWW/HTTP :: WSGI :: Application
 Classifier: Topic :: Internet :: WWW/HTTP :: WSGI :: Server
 Classifier: Topic :: Multimedia :: Graphics
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Image Processing
-Requires-Python: <3.11,>=3.9
+Requires-Python: <3.12,>=3.9
 Description-Content-Type: text/markdown
 Provides-Extra: dist
 Provides-Extra: docs
 Provides-Extra: dev
 Provides-Extra: test
 Provides-Extra: xformers
 License-File: LICENSE
@@ -336,15 +336,15 @@
 minute or two for Stable Diffusion to start up, then open your browser
 and go to http://localhost:9090.
 
 10. Type `banana sushi` in the box on the top left and click `Invoke`
 
 ### Command-Line Installation (for developers and users familiar with Terminals)
 
-You must have Python 3.9 or 3.10 installed on your machine. Earlier or
+You must have Python 3.9 through 3.11 installed on your machine. Earlier or
 later versions are not supported.
 Node.js also needs to be installed along with yarn (can be installed with
 the command `npm install -g yarn` if needed)
 
 1. Open a command-line window on your machine. The PowerShell is recommended for Windows.
 2. Create a directory to install InvokeAI into. You'll need at least 15 GB of free space:
```

### Comparing `InvokeAI-3.0.1rc1/InvokeAI.egg-info/SOURCES.txt` & `InvokeAI-3.0.1rc2/InvokeAI.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -163,21 +163,21 @@
 invokeai/frontend/install/widgets.py
 invokeai/frontend/merge/__init__.py
 invokeai/frontend/merge/merge_diffusers.py
 invokeai/frontend/training/__init__.py
 invokeai/frontend/training/textual_inversion.py
 invokeai/frontend/web/__init__.py
 invokeai/frontend/web/dist/index.html
+invokeai/frontend/web/dist/assets/App-58b095d3.js
 invokeai/frontend/web/dist/assets/App-6125620a.css
-invokeai/frontend/web/dist/assets/App-69e5ea36.js
-invokeai/frontend/web/dist/assets/MantineProvider-8184f020.js
+invokeai/frontend/web/dist/assets/MantineProvider-ea42d3d1.js
+invokeai/frontend/web/dist/assets/ThemeLocaleProvider-13e3db3d.js
 invokeai/frontend/web/dist/assets/ThemeLocaleProvider-5b992bc7.css
-invokeai/frontend/web/dist/assets/ThemeLocaleProvider-9ac72450.js
 invokeai/frontend/web/dist/assets/favicon-0d253ced.ico
-invokeai/frontend/web/dist/assets/index-89941396.js
+invokeai/frontend/web/dist/assets/index-5a784cdd.js
 invokeai/frontend/web/dist/assets/inter-cyrillic-ext-wght-normal-848492d3.woff2
 invokeai/frontend/web/dist/assets/inter-cyrillic-wght-normal-262a1054.woff2
 invokeai/frontend/web/dist/assets/inter-greek-ext-wght-normal-fe977ddb.woff2
 invokeai/frontend/web/dist/assets/inter-greek-wght-normal-89b4a3fe.woff2
 invokeai/frontend/web/dist/assets/inter-latin-ext-wght-normal-45606f83.woff2
 invokeai/frontend/web/dist/assets/inter-latin-wght-normal-450f3ba4.woff2
 invokeai/frontend/web/dist/assets/inter-vietnamese-wght-normal-ac4e131c.woff2
```

### Comparing `InvokeAI-3.0.1rc1/InvokeAI.egg-info/entry_points.txt` & `InvokeAI-3.0.1rc2/InvokeAI.egg-info/entry_points.txt`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/InvokeAI.egg-info/requires.txt` & `InvokeAI-3.0.1rc2/InvokeAI.egg-info/requires.txt`

 * *Files 20% similar despite different names*

```diff
@@ -1,60 +1,60 @@
-accelerate~=0.16
+accelerate~=0.21.0
 albumentations
 click
 clip_anytorch
-compel==2.0.0
+compel~=2.0.0
 controlnet-aux>=0.0.6
 timm==0.6.13
 datasets
-diffusers[torch]~=0.18.1
-dnspython==2.2.1
+diffusers[torch]~=0.19.0
+dnspython~=2.4.0
 dynamicprompts
 easing-functions
 einops
 eventlet
 facexlib
 fastapi==0.88.0
 fastapi-events==0.8.0
 fastapi-socketio==0.0.10
 flask==2.1.3
 flask_cors==3.0.10
 flask_socketio==5.3.0
 flaskwebgui==1.0.3
-gfpgan==1.3.8
 huggingface-hub>=0.11.1
-invisible-watermark>=0.2.0
+invisible-watermark~=0.2.0
 matplotlib
 mediapipe
 npyscreen
-numpy<1.24
+numpy==1.24.4
 omegaconf
 opencv-python
 picklescan
 pillow
 prompt-toolkit
-pympler==1.0.1
+pydantic==1.10.10
+pympler~=1.0.1
 pypatchmatch
 pyperclip
 pyreadline3
-python-multipart==0.0.6
-pytorch-lightning==1.7.7
+python-multipart
+pytorch-lightning
 realesrgan
-requests==2.28.2
+requests~=2.28.2
 rich~=13.3
 safetensors~=0.3.0
-scikit-image>=0.19
+scikit-image~=0.21.0
 send2trash
-test-tube>=0.7.5
-torch~=2.0.0
-torchvision>=0.14.1
-torchmetrics==0.11.4
-torchsde==0.2.5
+test-tube~=0.7.5
+torch~=2.0.1
+torchvision~=0.15.2
+torchmetrics~=1.0.1
+torchsde~=0.2.5
 transformers~=4.31.0
-uvicorn[standard]==0.21.1
+uvicorn[standard]~=0.21.1
 
 [:sys_platform == "win32"]
 windows-curses
 
 [dev]
 pudb
 
@@ -67,14 +67,15 @@
 mkdocs-material<9.0
 mkdocs-git-revision-date-localized-plugin
 mkdocs-redirects==1.2.0
 
 [test]
 pytest>6.0.0
 pytest-cov
+black
 
 [xformers]
 
 [xformers:sys_platform != "darwin"]
 xformers~=0.0.19
 
 [xformers:sys_platform == "linux"]
```

### Comparing `InvokeAI-3.0.1rc1/LICENSE` & `InvokeAI-3.0.1rc2/LICENSE`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/LICENSE-SD1+SD2.txt` & `InvokeAI-3.0.1rc2/LICENSE-SD1+SD2.txt`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/LICENSE-SDXL.txt` & `InvokeAI-3.0.1rc2/LICENSE-SDXL.txt`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/PKG-INFO` & `InvokeAI-3.0.1rc2/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: InvokeAI
-Version: 3.0.1rc1
+Version: 3.0.1rc2
 Summary: An implementation of Stable Diffusion which provides various new features and options to aid the image generation process
 Author-email: The InvokeAI Project <lincoln.stein@gmail.com>
 License:                                  Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
            TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
@@ -200,15 +200,15 @@
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Topic :: Artistic Software
 Classifier: Topic :: Internet :: WWW/HTTP :: WSGI :: Application
 Classifier: Topic :: Internet :: WWW/HTTP :: WSGI :: Server
 Classifier: Topic :: Multimedia :: Graphics
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Image Processing
-Requires-Python: <3.11,>=3.9
+Requires-Python: <3.12,>=3.9
 Description-Content-Type: text/markdown
 Provides-Extra: dist
 Provides-Extra: docs
 Provides-Extra: dev
 Provides-Extra: test
 Provides-Extra: xformers
 License-File: LICENSE
@@ -336,15 +336,15 @@
 minute or two for Stable Diffusion to start up, then open your browser
 and go to http://localhost:9090.
 
 10. Type `banana sushi` in the box on the top left and click `Invoke`
 
 ### Command-Line Installation (for developers and users familiar with Terminals)
 
-You must have Python 3.9 or 3.10 installed on your machine. Earlier or
+You must have Python 3.9 through 3.11 installed on your machine. Earlier or
 later versions are not supported.
 Node.js also needs to be installed along with yarn (can be installed with
 the command `npm install -g yarn` if needed)
 
 1. Open a command-line window on your machine. The PowerShell is recommended for Windows.
 2. Create a directory to install InvokeAI into. You'll need at least 15 GB of free space:
```

### Comparing `InvokeAI-3.0.1rc1/README.md` & `InvokeAI-3.0.1rc2/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -119,15 +119,15 @@
 minute or two for Stable Diffusion to start up, then open your browser
 and go to http://localhost:9090.
 
 10. Type `banana sushi` in the box on the top left and click `Invoke`
 
 ### Command-Line Installation (for developers and users familiar with Terminals)
 
-You must have Python 3.9 or 3.10 installed on your machine. Earlier or
+You must have Python 3.9 through 3.11 installed on your machine. Earlier or
 later versions are not supported.
 Node.js also needs to be installed along with yarn (can be installed with
 the command `npm install -g yarn` if needed)
 
 1. Open a command-line window on your machine. The PowerShell is recommended for Windows.
 2. Create a directory to install InvokeAI into. You'll need at least 15 GB of free space:
```

#### html2text {}

```diff
@@ -68,21 +68,21 @@
 contain launcher scripts named `invoke.sh` and `invoke.bat`. 8. On Windows
 systems, double-click on the `invoke.bat` file. On macOS, open a Terminal
 window, drag `invoke.sh` from the folder into the Terminal, and press return.
 On Linux, run `invoke.sh` 9. Press 2 to open the "browser-based UI", press
 enter/return, wait a minute or two for Stable Diffusion to start up, then open
 your browser and go to http://localhost:9090. 10. Type `banana sushi` in the
 box on the top left and click `Invoke` ### Command-Line Installation (for
-developers and users familiar with Terminals) You must have Python 3.9 or 3.10
-installed on your machine. Earlier or later versions are not supported. Node.js
-also needs to be installed along with yarn (can be installed with the command
-`npm install -g yarn` if needed) 1. Open a command-line window on your machine.
-The PowerShell is recommended for Windows. 2. Create a directory to install
-InvokeAI into. You'll need at least 15 GB of free space: ```terminal mkdir
-invokeai ```` 3. Create a virtual environment named `.venv` inside this
+developers and users familiar with Terminals) You must have Python 3.9 through
+3.11 installed on your machine. Earlier or later versions are not supported.
+Node.js also needs to be installed along with yarn (can be installed with the
+command `npm install -g yarn` if needed) 1. Open a command-line window on your
+machine. The PowerShell is recommended for Windows. 2. Create a directory to
+install InvokeAI into. You'll need at least 15 GB of free space: ```terminal
+mkdir invokeai ```` 3. Create a virtual environment named `.venv` inside this
 directory and activate it: ```terminal cd invokeai python -m venv .venv --
 prompt InvokeAI ``` 4. Activate the virtual environment (do it every time you
 run InvokeAI) _For Linux/Mac users:_ ```sh source .venv/bin/activate ``` _For
 Windows users:_ ```ps .venv\Scripts\activate ``` 5. Install the InvokeAI module
 and its dependencies. Choose the command suited for your platform & GPU. _For
 Windows/Linux with an NVIDIA GPU:_ ```terminal pip install "InvokeAI[xformers]"
 --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 ``` _For
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/dependencies.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/dependencies.py`

 * *Files 1% similar despite different names*

```diff
@@ -74,17 +74,15 @@
             filename=db_location, table_name="graph_executions"
         )
 
         urls = LocalUrlService()
         image_record_storage = SqliteImageRecordStorage(db_location)
         image_file_storage = DiskImageFileStorage(f"{output_folder}/images")
         names = SimpleNameService()
-        latents = ForwardCacheLatentsStorage(
-            DiskLatentsStorage(f"{output_folder}/latents")
-        )
+        latents = ForwardCacheLatentsStorage(DiskLatentsStorage(f"{output_folder}/latents"))
 
         board_record_storage = SqliteBoardRecordStorage(db_location)
         board_image_record_storage = SqliteBoardImageRecordStorage(db_location)
 
         boards = BoardService(
             services=BoardServiceDependencies(
                 board_image_record_storage=board_image_record_storage,
@@ -121,17 +119,15 @@
             model_manager=ModelManagerService(config, logger),
             events=events,
             latents=latents,
             images=images,
             boards=boards,
             board_images=board_images,
             queue=MemoryInvocationQueue(),
-            graph_library=SqliteItemStorage[LibraryGraph](
-                filename=db_location, table_name="graphs"
-            ),
+            graph_library=SqliteItemStorage[LibraryGraph](filename=db_location, table_name="graphs"),
             graph_execution_manager=graph_execution_manager,
             processor=DefaultInvocationProcessor(),
             configuration=config,
             logger=logger,
         )
 
         create_system_graphs(services.graph_library)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/events.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/events.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/routers/app_info.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/routers/app_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,26 +11,29 @@
 from invokeai.app.invocations.upscale import ESRGAN_MODELS
 
 from invokeai.version import __version__
 
 from ..dependencies import ApiDependencies
 from invokeai.backend.util.logging import logging
 
+
 class LogLevel(int, Enum):
     NotSet = logging.NOTSET
     Debug = logging.DEBUG
     Info = logging.INFO
     Warning = logging.WARNING
     Error = logging.ERROR
     Critical = logging.CRITICAL
 
+
 class Upscaler(BaseModel):
     upscaling_method: str = Field(description="Name of upscaling method")
     upscaling_models: list[str] = Field(description="List of upscaling models for this method")
-    
+
+
 app_router = APIRouter(prefix="/v1/app", tags=["app"])
 
 
 class AppVersion(BaseModel):
     """App Version Response"""
 
     version: str = Field(description="App version")
@@ -41,69 +44,62 @@
 
     infill_methods: list[str] = Field(description="List of available infill methods")
     upscaling_methods: list[Upscaler] = Field(description="List of upscaling methods")
     nsfw_methods: list[str] = Field(description="List of NSFW checking methods")
     watermarking_methods: list[str] = Field(description="List of invisible watermark methods")
 
 
-@app_router.get(
-    "/version", operation_id="app_version", status_code=200, response_model=AppVersion
-)
+@app_router.get("/version", operation_id="app_version", status_code=200, response_model=AppVersion)
 async def get_version() -> AppVersion:
     return AppVersion(version=__version__)
 
 
-@app_router.get(
-    "/config", operation_id="get_config", status_code=200, response_model=AppConfig
-)
+@app_router.get("/config", operation_id="get_config", status_code=200, response_model=AppConfig)
 async def get_config() -> AppConfig:
-    infill_methods = ['tile']
+    infill_methods = ["tile"]
     if PatchMatch.patchmatch_available():
-        infill_methods.append('patchmatch')
-        
+        infill_methods.append("patchmatch")
 
     upscaling_models = []
     for model in typing.get_args(ESRGAN_MODELS):
         upscaling_models.append(str(Path(model).stem))
-    upscaler = Upscaler(
-        upscaling_method = 'esrgan',
-        upscaling_models = upscaling_models
-    )
-    
+    upscaler = Upscaler(upscaling_method="esrgan", upscaling_models=upscaling_models)
+
     nsfw_methods = []
     if SafetyChecker.safety_checker_available():
-        nsfw_methods.append('nsfw_checker')
+        nsfw_methods.append("nsfw_checker")
 
     watermarking_methods = []
     if InvisibleWatermark.invisible_watermark_available():
-        watermarking_methods.append('invisible_watermark')
-        
+        watermarking_methods.append("invisible_watermark")
+
     return AppConfig(
         infill_methods=infill_methods,
         upscaling_methods=[upscaler],
         nsfw_methods=nsfw_methods,
         watermarking_methods=watermarking_methods,
     )
 
+
 @app_router.get(
     "/logging",
     operation_id="get_log_level",
-    responses={200: {"description" : "The operation was successful"}},
-    response_model = LogLevel,
+    responses={200: {"description": "The operation was successful"}},
+    response_model=LogLevel,
 )
-async def get_log_level(
-) -> LogLevel:
+async def get_log_level() -> LogLevel:
     """Returns the log level"""
     return LogLevel(ApiDependencies.invoker.services.logger.level)
 
+
 @app_router.post(
     "/logging",
     operation_id="set_log_level",
-    responses={200: {"description" : "The operation was successful"}},
-    response_model = LogLevel,
+    responses={200: {"description": "The operation was successful"}},
+    response_model=LogLevel,
 )
 async def set_log_level(
-        level: LogLevel = Body(description="New log verbosity level"),
+    level: LogLevel = Body(description="New log verbosity level"),
 ) -> LogLevel:
     """Sets the log verbosity level"""
     ApiDependencies.invoker.services.logger.setLevel(level)
     return LogLevel(ApiDependencies.invoker.services.logger.level)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/routers/board_images.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/routers/board_images.py`

 * *Files 0% similar despite different names*

```diff
@@ -48,8 +48,7 @@
     try:
         result = ApiDependencies.invoker.services.board_images.remove_image_from_board(
             board_id=board_id, image_name=image_name
         )
         return result
     except Exception as e:
         raise HTTPException(status_code=500, detail="Failed to update board")
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/routers/boards.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/routers/boards.py`

 * *Files 7% similar despite different names*

```diff
@@ -14,17 +14,15 @@
 
 
 class DeleteBoardResult(BaseModel):
     board_id: str = Field(description="The id of the board that was deleted.")
     deleted_board_images: list[str] = Field(
         description="The image names of the board-images relationships that were deleted."
     )
-    deleted_images: list[str] = Field(
-        description="The names of the images that were deleted."
-    )
+    deleted_images: list[str] = Field(description="The names of the images that were deleted.")
 
 
 @boards_router.post(
     "/",
     operation_id="create_board",
     responses={
         201: {"description": "The board was created successfully"},
@@ -69,40 +67,32 @@
 )
 async def update_board(
     board_id: str = Path(description="The id of board to update"),
     changes: BoardChanges = Body(description="The changes to apply to the board"),
 ) -> BoardDTO:
     """Updates a board"""
     try:
-        result = ApiDependencies.invoker.services.boards.update(
-            board_id=board_id, changes=changes
-        )
+        result = ApiDependencies.invoker.services.boards.update(board_id=board_id, changes=changes)
         return result
     except Exception as e:
         raise HTTPException(status_code=500, detail="Failed to update board")
 
 
-@boards_router.delete(
-    "/{board_id}", operation_id="delete_board", response_model=DeleteBoardResult
-)
+@boards_router.delete("/{board_id}", operation_id="delete_board", response_model=DeleteBoardResult)
 async def delete_board(
     board_id: str = Path(description="The id of board to delete"),
-    include_images: Optional[bool] = Query(
-        description="Permanently delete all images on the board", default=False
-    ),
+    include_images: Optional[bool] = Query(description="Permanently delete all images on the board", default=False),
 ) -> DeleteBoardResult:
     """Deletes a board"""
     try:
         if include_images is True:
             deleted_images = ApiDependencies.invoker.services.board_images.get_all_board_image_names_for_board(
                 board_id=board_id
             )
-            ApiDependencies.invoker.services.images.delete_images_on_board(
-                board_id=board_id
-            )
+            ApiDependencies.invoker.services.images.delete_images_on_board(board_id=board_id)
             ApiDependencies.invoker.services.boards.delete(board_id=board_id)
             return DeleteBoardResult(
                 board_id=board_id,
                 deleted_board_images=[],
                 deleted_images=deleted_images,
             )
         else:
@@ -123,17 +113,15 @@
     "/",
     operation_id="list_boards",
     response_model=Union[OffsetPaginatedResults[BoardDTO], list[BoardDTO]],
 )
 async def list_boards(
     all: Optional[bool] = Query(default=None, description="Whether to list all boards"),
     offset: Optional[int] = Query(default=None, description="The page offset"),
-    limit: Optional[int] = Query(
-        default=None, description="The number of boards per page"
-    ),
+    limit: Optional[int] = Query(default=None, description="The number of boards per page"),
 ) -> Union[OffsetPaginatedResults[BoardDTO], list[BoardDTO]]:
     """Gets a list of boards"""
     if all:
         return ApiDependencies.invoker.services.boards.get_all()
     elif offset is not None and limit is not None:
         return ApiDependencies.invoker.services.boards.get_many(
             offset,
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/routers/images.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/routers/images.py`

 * *Files 6% similar despite different names*

```diff
@@ -36,23 +36,17 @@
 )
 async def upload_image(
     file: UploadFile,
     request: Request,
     response: Response,
     image_category: ImageCategory = Query(description="The category of the image"),
     is_intermediate: bool = Query(description="Whether this is an intermediate image"),
-    board_id: Optional[str] = Query(
-        default=None, description="The board to add this image to, if any"
-    ),
-    session_id: Optional[str] = Query(
-        default=None, description="The session ID associated with this upload, if any"
-    ),
-    crop_visible: Optional[bool] = Query(
-        default=False, description="Whether to crop the image"
-    ),
+    board_id: Optional[str] = Query(default=None, description="The board to add this image to, if any"),
+    session_id: Optional[str] = Query(default=None, description="The session ID associated with this upload, if any"),
+    crop_visible: Optional[bool] = Query(default=False, description="Whether to crop the image"),
 ) -> ImageDTO:
     """Uploads an image"""
     if not file.content_type.startswith("image"):
         raise HTTPException(status_code=415, detail="Not an image")
 
     contents = await file.read()
 
@@ -111,17 +105,15 @@
 @images_router.patch(
     "/{image_name}",
     operation_id="update_image",
     response_model=ImageDTO,
 )
 async def update_image(
     image_name: str = Path(description="The name of the image to update"),
-    image_changes: ImageRecordChanges = Body(
-        description="The changes to apply to the image"
-    ),
+    image_changes: ImageRecordChanges = Body(description="The changes to apply to the image"),
 ) -> ImageDTO:
     """Updates an image"""
 
     try:
         return ApiDependencies.invoker.services.images.update(image_name, image_changes)
     except Exception as e:
         raise HTTPException(status_code=400, detail="Failed to update image")
@@ -208,23 +200,19 @@
 )
 async def get_image_thumbnail(
     image_name: str = Path(description="The name of thumbnail image file to get"),
 ) -> FileResponse:
     """Gets a thumbnail image file"""
 
     try:
-        path = ApiDependencies.invoker.services.images.get_path(
-            image_name, thumbnail=True
-        )
+        path = ApiDependencies.invoker.services.images.get_path(image_name, thumbnail=True)
         if not ApiDependencies.invoker.services.images.validate_path(path):
             raise HTTPException(status_code=404)
 
-        response = FileResponse(
-            path, media_type="image/webp", content_disposition_type="inline"
-        )
+        response = FileResponse(path, media_type="image/webp", content_disposition_type="inline")
         response.headers["Cache-Control"] = f"max-age={IMAGE_MAX_AGE}"
         return response
     except Exception as e:
         raise HTTPException(status_code=404)
 
 
 @images_router.get(
@@ -235,17 +223,15 @@
 async def get_image_urls(
     image_name: str = Path(description="The name of the image whose URL to get"),
 ) -> ImageUrlsDTO:
     """Gets an image and thumbnail URL"""
 
     try:
         image_url = ApiDependencies.invoker.services.images.get_url(image_name)
-        thumbnail_url = ApiDependencies.invoker.services.images.get_url(
-            image_name, thumbnail=True
-        )
+        thumbnail_url = ApiDependencies.invoker.services.images.get_url(image_name, thumbnail=True)
         return ImageUrlsDTO(
             image_name=image_name,
             image_url=image_url,
             thumbnail_url=thumbnail_url,
         )
     except Exception as e:
         raise HTTPException(status_code=404)
@@ -253,23 +239,17 @@
 
 @images_router.get(
     "/",
     operation_id="list_image_dtos",
     response_model=OffsetPaginatedResults[ImageDTO],
 )
 async def list_image_dtos(
-    image_origin: Optional[ResourceOrigin] = Query(
-        default=None, description="The origin of images to list."
-    ),
-    categories: Optional[list[ImageCategory]] = Query(
-        default=None, description="The categories of image to include."
-    ),
-    is_intermediate: Optional[bool] = Query(
-        default=None, description="Whether to list intermediate images."
-    ),
+    image_origin: Optional[ResourceOrigin] = Query(default=None, description="The origin of images to list."),
+    categories: Optional[list[ImageCategory]] = Query(default=None, description="The categories of image to include."),
+    is_intermediate: Optional[bool] = Query(default=None, description="Whether to list intermediate images."),
     board_id: Optional[str] = Query(
         default=None,
         description="The board id to filter by. Use 'none' to find images without a board.",
     ),
     offset: int = Query(default=0, description="The page offset"),
     limit: int = Query(default=10, description="The number of images per page"),
 ) -> OffsetPaginatedResults[ImageDTO]:
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/routers/models.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/routers/models.py`

 * *Files 5% similar despite different names*

```diff
@@ -24,92 +24,94 @@
 
 UpdateModelResponse = Union[tuple(OPENAPI_MODEL_CONFIGS)]
 ImportModelResponse = Union[tuple(OPENAPI_MODEL_CONFIGS)]
 ConvertModelResponse = Union[tuple(OPENAPI_MODEL_CONFIGS)]
 MergeModelResponse = Union[tuple(OPENAPI_MODEL_CONFIGS)]
 ImportModelAttributes = Union[tuple(OPENAPI_MODEL_CONFIGS)]
 
+
 class ModelsList(BaseModel):
     models: list[Union[tuple(OPENAPI_MODEL_CONFIGS)]]
 
+
 @models_router.get(
     "/",
     operation_id="list_models",
-    responses={200: {"model": ModelsList }},
+    responses={200: {"model": ModelsList}},
 )
 async def list_models(
     base_models: Optional[List[BaseModelType]] = Query(default=None, description="Base models to include"),
     model_type: Optional[ModelType] = Query(default=None, description="The type of model to get"),
 ) -> ModelsList:
     """Gets a list of models"""
-    if base_models and len(base_models)>0:
+    if base_models and len(base_models) > 0:
         models_raw = list()
         for base_model in base_models:
             models_raw.extend(ApiDependencies.invoker.services.model_manager.list_models(base_model, model_type))
     else:
         models_raw = ApiDependencies.invoker.services.model_manager.list_models(None, model_type)
-    models = parse_obj_as(ModelsList, { "models": models_raw })
+    models = parse_obj_as(ModelsList, {"models": models_raw})
     return models
 
+
 @models_router.patch(
     "/{base_model}/{model_type}/{model_name}",
     operation_id="update_model",
-    responses={200: {"description" : "The model was updated successfully"},
-               400: {"description" : "Bad request"},
-               404: {"description" : "The model could not be found"},
-               409: {"description" : "There is already a model corresponding to the new name"},
-               },
-    status_code = 200,
-    response_model = UpdateModelResponse,
+    responses={
+        200: {"description": "The model was updated successfully"},
+        400: {"description": "Bad request"},
+        404: {"description": "The model could not be found"},
+        409: {"description": "There is already a model corresponding to the new name"},
+    },
+    status_code=200,
+    response_model=UpdateModelResponse,
 )
 async def update_model(
-        base_model: BaseModelType = Path(description="Base model"),
-        model_type: ModelType = Path(description="The type of model"),
-        model_name: str = Path(description="model name"),
-        info: Union[tuple(OPENAPI_MODEL_CONFIGS)] = Body(description="Model configuration"),
+    base_model: BaseModelType = Path(description="Base model"),
+    model_type: ModelType = Path(description="The type of model"),
+    model_name: str = Path(description="model name"),
+    info: Union[tuple(OPENAPI_MODEL_CONFIGS)] = Body(description="Model configuration"),
 ) -> UpdateModelResponse:
-    """ Update model contents with a new config. If the model name or base fields are changed, then the model is renamed. """
+    """Update model contents with a new config. If the model name or base fields are changed, then the model is renamed."""
     logger = ApiDependencies.invoker.services.logger
 
-    
     try:
         previous_info = ApiDependencies.invoker.services.model_manager.list_model(
             model_name=model_name,
             base_model=base_model,
             model_type=model_type,
         )
 
         # rename operation requested
         if info.model_name != model_name or info.base_model != base_model:
             ApiDependencies.invoker.services.model_manager.rename_model(
-                base_model = base_model,
-                model_type = model_type,
-                model_name = model_name,
-                new_name = info.model_name,
-                new_base = info.base_model,
+                base_model=base_model,
+                model_type=model_type,
+                model_name=model_name,
+                new_name=info.model_name,
+                new_base=info.base_model,
             )
-            logger.info(f'Successfully renamed {base_model}/{model_name}=>{info.base_model}/{info.model_name}')
+            logger.info(f"Successfully renamed {base_model.value}/{model_name}=>{info.base_model}/{info.model_name}")
             # update information to support an update of attributes
             model_name = info.model_name
             base_model = info.base_model
             new_info = ApiDependencies.invoker.services.model_manager.list_model(
                 model_name=model_name,
                 base_model=base_model,
                 model_type=model_type,
             )
-            if new_info.get('path') != previous_info.get('path'):  # model manager moved model path during rename - don't overwrite it
-                info.path = new_info.get('path')
-            
+            if new_info.get("path") != previous_info.get(
+                "path"
+            ):  # model manager moved model path during rename - don't overwrite it
+                info.path = new_info.get("path")
+
         ApiDependencies.invoker.services.model_manager.update_model(
-            model_name=model_name,
-            base_model=base_model,
-            model_type=model_type,
-            model_attributes=info.dict()
+            model_name=model_name, base_model=base_model, model_type=model_type, model_attributes=info.dict()
         )
-            
+
         model_raw = ApiDependencies.invoker.services.model_manager.list_model(
             model_name=model_name,
             base_model=base_model,
             model_type=model_type,
         )
         model_response = parse_obj_as(UpdateModelResponse, model_raw)
     except ModelNotFoundException as e:
@@ -119,258 +121,263 @@
         raise HTTPException(status_code=409, detail=str(e))
     except Exception as e:
         logger.error(str(e))
         raise HTTPException(status_code=400, detail=str(e))
 
     return model_response
 
+
 @models_router.post(
     "/import",
     operation_id="import_model",
-    responses= {
-        201: {"description" : "The model imported successfully"},
-        404: {"description" : "The model could not be found"},
-        415: {"description" : "Unrecognized file/folder format"},
-        424: {"description" : "The model appeared to import successfully, but could not be found in the model manager"},
-        409: {"description" : "There is already a model corresponding to this path or repo_id"},
+    responses={
+        201: {"description": "The model imported successfully"},
+        404: {"description": "The model could not be found"},
+        415: {"description": "Unrecognized file/folder format"},
+        424: {"description": "The model appeared to import successfully, but could not be found in the model manager"},
+        409: {"description": "There is already a model corresponding to this path or repo_id"},
     },
     status_code=201,
-    response_model=ImportModelResponse
+    response_model=ImportModelResponse,
 )
 async def import_model(
-        location: str = Body(description="A model path, repo_id or URL to import"),
-        prediction_type: Optional[Literal['v_prediction','epsilon','sample']] = \
-                Body(description='Prediction type for SDv2 checkpoint files', default="v_prediction"),
+    location: str = Body(description="A model path, repo_id or URL to import"),
+    prediction_type: Optional[Literal["v_prediction", "epsilon", "sample"]] = Body(
+        description="Prediction type for SDv2 checkpoint files", default="v_prediction"
+    ),
 ) -> ImportModelResponse:
-    """ Add a model using its local path, repo_id, or remote URL. Model characteristics will be probed and configured automatically """
-    
+    """Add a model using its local path, repo_id, or remote URL. Model characteristics will be probed and configured automatically"""
+
     items_to_import = {location}
-    prediction_types = { x.value: x for x in SchedulerPredictionType }
+    prediction_types = {x.value: x for x in SchedulerPredictionType}
     logger = ApiDependencies.invoker.services.logger
 
     try:
         installed_models = ApiDependencies.invoker.services.model_manager.heuristic_import(
-            items_to_import = items_to_import,
-            prediction_type_helper = lambda x: prediction_types.get(prediction_type)
+            items_to_import=items_to_import, prediction_type_helper=lambda x: prediction_types.get(prediction_type)
         )
         info = installed_models.get(location)
 
         if not info:
             logger.error("Import failed")
             raise HTTPException(status_code=415)
-        
-        logger.info(f'Successfully imported {location}, got {info}')
+
+        logger.info(f"Successfully imported {location}, got {info}")
         model_raw = ApiDependencies.invoker.services.model_manager.list_model(
-            model_name=info.name,
-            base_model=info.base_model,
-            model_type=info.model_type
+            model_name=info.name, base_model=info.base_model, model_type=info.model_type
         )
         return parse_obj_as(ImportModelResponse, model_raw)
-    
+
     except ModelNotFoundException as e:
         logger.error(str(e))
         raise HTTPException(status_code=404, detail=str(e))
     except InvalidModelException as e:
         logger.error(str(e))
         raise HTTPException(status_code=415)
     except ValueError as e:
         logger.error(str(e))
         raise HTTPException(status_code=409, detail=str(e))
-        
+
+
 @models_router.post(
     "/add",
     operation_id="add_model",
-    responses= {
-        201: {"description" : "The model added successfully"},
-        404: {"description" : "The model could not be found"},
-        424: {"description" : "The model appeared to add successfully, but could not be found in the model manager"},
-        409: {"description" : "There is already a model corresponding to this path or repo_id"},
+    responses={
+        201: {"description": "The model added successfully"},
+        404: {"description": "The model could not be found"},
+        424: {"description": "The model appeared to add successfully, but could not be found in the model manager"},
+        409: {"description": "There is already a model corresponding to this path or repo_id"},
     },
     status_code=201,
-    response_model=ImportModelResponse
+    response_model=ImportModelResponse,
 )
 async def add_model(
-        info: Union[tuple(OPENAPI_MODEL_CONFIGS)] = Body(description="Model configuration"),
+    info: Union[tuple(OPENAPI_MODEL_CONFIGS)] = Body(description="Model configuration"),
 ) -> ImportModelResponse:
-    """ Add a model using the configuration information appropriate for its type. Only local models can be added by path"""
-    
+    """Add a model using the configuration information appropriate for its type. Only local models can be added by path"""
+
     logger = ApiDependencies.invoker.services.logger
 
     try:
         ApiDependencies.invoker.services.model_manager.add_model(
-            info.model_name,
-            info.base_model,
-            info.model_type,
-            model_attributes = info.dict()
+            info.model_name, info.base_model, info.model_type, model_attributes=info.dict()
         )
-        logger.info(f'Successfully added {info.model_name}')
+        logger.info(f"Successfully added {info.model_name}")
         model_raw = ApiDependencies.invoker.services.model_manager.list_model(
-            model_name=info.model_name,
-            base_model=info.base_model,
-            model_type=info.model_type
+            model_name=info.model_name, base_model=info.base_model, model_type=info.model_type
         )
         return parse_obj_as(ImportModelResponse, model_raw)
     except ModelNotFoundException as e:
         logger.error(str(e))
         raise HTTPException(status_code=404, detail=str(e))
     except ValueError as e:
         logger.error(str(e))
         raise HTTPException(status_code=409, detail=str(e))
 
-    
+
 @models_router.delete(
     "/{base_model}/{model_type}/{model_name}",
     operation_id="del_model",
-    responses={
-        204: { "description": "Model deleted successfully" }, 
-        404: { "description": "Model not found" }
-    },
-    status_code = 204,
-    response_model = None,
+    responses={204: {"description": "Model deleted successfully"}, 404: {"description": "Model not found"}},
+    status_code=204,
+    response_model=None,
 )
 async def delete_model(
-        base_model: BaseModelType = Path(description="Base model"),
-        model_type: ModelType = Path(description="The type of model"),
-        model_name: str = Path(description="model name"),
+    base_model: BaseModelType = Path(description="Base model"),
+    model_type: ModelType = Path(description="The type of model"),
+    model_name: str = Path(description="model name"),
 ) -> Response:
     """Delete Model"""
     logger = ApiDependencies.invoker.services.logger
-    
+
     try:
-        ApiDependencies.invoker.services.model_manager.del_model(model_name,
-                                                                 base_model = base_model,
-                                                                 model_type = model_type
-                                                                 )
+        ApiDependencies.invoker.services.model_manager.del_model(
+            model_name, base_model=base_model, model_type=model_type
+        )
         logger.info(f"Deleted model: {model_name}")
         return Response(status_code=204)
     except ModelNotFoundException as e:
         logger.error(str(e))
         raise HTTPException(status_code=404, detail=str(e))
 
+
 @models_router.put(
     "/convert/{base_model}/{model_type}/{model_name}",
     operation_id="convert_model",
     responses={
-        200: { "description": "Model converted successfully" },
-        400: {"description" : "Bad request"  },
-        404: { "description": "Model not found"  },
+        200: {"description": "Model converted successfully"},
+        400: {"description": "Bad request"},
+        404: {"description": "Model not found"},
     },
-    status_code = 200,
-    response_model = ConvertModelResponse,
+    status_code=200,
+    response_model=ConvertModelResponse,
 )
 async def convert_model(
-        base_model: BaseModelType = Path(description="Base model"),
-        model_type: ModelType = Path(description="The type of model"),
-        model_name: str = Path(description="model name"),
-        convert_dest_directory: Optional[str] = Query(default=None, description="Save the converted model to the designated directory"),
+    base_model: BaseModelType = Path(description="Base model"),
+    model_type: ModelType = Path(description="The type of model"),
+    model_name: str = Path(description="model name"),
+    convert_dest_directory: Optional[str] = Query(
+        default=None, description="Save the converted model to the designated directory"
+    ),
 ) -> ConvertModelResponse:
     """Convert a checkpoint model into a diffusers model, optionally saving to the indicated destination directory, or `models` if none."""
     logger = ApiDependencies.invoker.services.logger
     try:
         logger.info(f"Converting model: {model_name}")
         dest = pathlib.Path(convert_dest_directory) if convert_dest_directory else None
-        ApiDependencies.invoker.services.model_manager.convert_model(model_name,
-                                                                     base_model = base_model,
-                                                                     model_type = model_type,
-                                                                     convert_dest_directory = dest,
-                                                                     )
-        model_raw = ApiDependencies.invoker.services.model_manager.list_model(model_name,
-                                                                              base_model = base_model,
-                                                                              model_type = model_type)
+        ApiDependencies.invoker.services.model_manager.convert_model(
+            model_name,
+            base_model=base_model,
+            model_type=model_type,
+            convert_dest_directory=dest,
+        )
+        model_raw = ApiDependencies.invoker.services.model_manager.list_model(
+            model_name, base_model=base_model, model_type=model_type
+        )
         response = parse_obj_as(ConvertModelResponse, model_raw)
     except ModelNotFoundException as e:
         raise HTTPException(status_code=404, detail=f"Model '{model_name}' not found: {str(e)}")
     except ValueError as e:
         raise HTTPException(status_code=400, detail=str(e))
     return response
 
+
 @models_router.get(
     "/search",
     operation_id="search_for_models",
     responses={
-        200: { "description": "Directory searched successfully" },
-        404: { "description": "Invalid directory path"  },
+        200: {"description": "Directory searched successfully"},
+        404: {"description": "Invalid directory path"},
     },
-    status_code = 200,
-    response_model = List[pathlib.Path]
+    status_code=200,
+    response_model=List[pathlib.Path],
 )
 async def search_for_models(
-        search_path: pathlib.Path = Query(description="Directory path to search for models")
-)->List[pathlib.Path]:
+    search_path: pathlib.Path = Query(description="Directory path to search for models"),
+) -> List[pathlib.Path]:
     if not search_path.is_dir():
-        raise HTTPException(status_code=404, detail=f"The search path '{search_path}' does not exist or is not directory")
+        raise HTTPException(
+            status_code=404, detail=f"The search path '{search_path}' does not exist or is not directory"
+        )
     return ApiDependencies.invoker.services.model_manager.search_for_models(search_path)
 
+
 @models_router.get(
     "/ckpt_confs",
     operation_id="list_ckpt_configs",
     responses={
-        200: { "description" : "paths retrieved successfully" },
+        200: {"description": "paths retrieved successfully"},
     },
-    status_code = 200,
-    response_model = List[pathlib.Path]
+    status_code=200,
+    response_model=List[pathlib.Path],
 )
-async def list_ckpt_configs(
-)->List[pathlib.Path]:
+async def list_ckpt_configs() -> List[pathlib.Path]:
     """Return a list of the legacy checkpoint configuration files stored in `ROOT/configs/stable-diffusion`, relative to ROOT."""
     return ApiDependencies.invoker.services.model_manager.list_checkpoint_configs()
-    
-        
+
+
 @models_router.post(
     "/sync",
     operation_id="sync_to_config",
     responses={
-        201: { "description": "synchronization successful" },
+        201: {"description": "synchronization successful"},
     },
-    status_code = 201,
-    response_model = bool
+    status_code=201,
+    response_model=bool,
 )
-async def sync_to_config(
-)->bool:
+async def sync_to_config() -> bool:
     """Call after making changes to models.yaml, autoimport directories or models directory to synchronize
     in-memory data structures with disk data structures."""
     ApiDependencies.invoker.services.model_manager.sync_to_config()
     return True
-        
+
+
 @models_router.put(
     "/merge/{base_model}",
     operation_id="merge_models",
     responses={
-        200: { "description": "Model converted successfully" },
-        400: { "description": "Incompatible models"  },
-        404: { "description": "One or more models not found"  },
+        200: {"description": "Model converted successfully"},
+        400: {"description": "Incompatible models"},
+        404: {"description": "One or more models not found"},
     },
-    status_code = 200,
-    response_model = MergeModelResponse,
+    status_code=200,
+    response_model=MergeModelResponse,
 )
 async def merge_models(
-        base_model: BaseModelType                  = Path(description="Base model"),
-        model_names: List[str]                     = Body(description="model name", min_items=2, max_items=3),
-        merged_model_name: Optional[str]           = Body(description="Name of destination model"),
-        alpha: Optional[float]                     = Body(description="Alpha weighting strength to apply to 2d and 3d models", default=0.5),
-        interp: Optional[MergeInterpolationMethod] = Body(description="Interpolation method"),
-        force: Optional[bool]                      = Body(description="Force merging of models created with different versions of diffusers", default=False),
-        merge_dest_directory: Optional[str]       = Body(description="Save the merged model to the designated directory (with 'merged_model_name' appended)", default=None)
+    base_model: BaseModelType = Path(description="Base model"),
+    model_names: List[str] = Body(description="model name", min_items=2, max_items=3),
+    merged_model_name: Optional[str] = Body(description="Name of destination model"),
+    alpha: Optional[float] = Body(description="Alpha weighting strength to apply to 2d and 3d models", default=0.5),
+    interp: Optional[MergeInterpolationMethod] = Body(description="Interpolation method"),
+    force: Optional[bool] = Body(
+        description="Force merging of models created with different versions of diffusers", default=False
+    ),
+    merge_dest_directory: Optional[str] = Body(
+        description="Save the merged model to the designated directory (with 'merged_model_name' appended)",
+        default=None,
+    ),
 ) -> MergeModelResponse:
     """Convert a checkpoint model into a diffusers model"""
     logger = ApiDependencies.invoker.services.logger
     try:
         logger.info(f"Merging models: {model_names} into {merge_dest_directory or '<MODELS>'}/{merged_model_name}")
         dest = pathlib.Path(merge_dest_directory) if merge_dest_directory else None
-        result = ApiDependencies.invoker.services.model_manager.merge_models(model_names,
-                                                                             base_model,
-                                                                             merged_model_name=merged_model_name or "+".join(model_names),
-                                                                             alpha=alpha,
-                                                                             interp=interp,
-                                                                             force=force,
-                                                                             merge_dest_directory = dest
-                                                                             )
-        model_raw = ApiDependencies.invoker.services.model_manager.list_model(result.name,
-                                                                              base_model = base_model,
-                                                                              model_type = ModelType.Main,
-                                                                              )
+        result = ApiDependencies.invoker.services.model_manager.merge_models(
+            model_names,
+            base_model,
+            merged_model_name=merged_model_name or "+".join(model_names),
+            alpha=alpha,
+            interp=interp,
+            force=force,
+            merge_dest_directory=dest,
+        )
+        model_raw = ApiDependencies.invoker.services.model_manager.list_model(
+            result.name,
+            base_model=base_model,
+            model_type=ModelType.Main,
+        )
         response = parse_obj_as(ConvertModelResponse, model_raw)
     except ModelNotFoundException:
         raise HTTPException(status_code=404, detail=f"One or more of the models '{model_names}' not found")
     except ValueError as e:
         raise HTTPException(status_code=400, detail=str(e))
     return response
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/routers/sessions.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/routers/sessions.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,17 +26,15 @@
     operation_id="create_session",
     responses={
         200: {"model": GraphExecutionState},
         400: {"description": "Invalid json"},
     },
 )
 async def create_session(
-    graph: Optional[Graph] = Body(
-        default=None, description="The graph to initialize the session with"
-    )
+    graph: Optional[Graph] = Body(default=None, description="The graph to initialize the session with")
 ) -> GraphExecutionState:
     """Creates a new session, optionally initializing it with an invocation graph"""
     session = ApiDependencies.invoker.create_execution_state(graph)
     return session
 
 
 @session_router.get(
@@ -47,21 +45,17 @@
 async def list_sessions(
     page: int = Query(default=0, description="The page of results to get"),
     per_page: int = Query(default=10, description="The number of results per page"),
     query: str = Query(default="", description="The query string to search for"),
 ) -> PaginatedResults[GraphExecutionState]:
     """Gets a list of sessions, optionally searching"""
     if query == "":
-        result = ApiDependencies.invoker.services.graph_execution_manager.list(
-            page, per_page
-        )
+        result = ApiDependencies.invoker.services.graph_execution_manager.list(page, per_page)
     else:
-        result = ApiDependencies.invoker.services.graph_execution_manager.search(
-            query, page, per_page
-        )
+        result = ApiDependencies.invoker.services.graph_execution_manager.search(query, page, per_page)
     return result
 
 
 @session_router.get(
     "/{session_id}",
     operation_id="get_session",
     responses={
@@ -87,17 +81,17 @@
         200: {"model": str},
         400: {"description": "Invalid node or link"},
         404: {"description": "Session not found"},
     },
 )
 async def add_node(
     session_id: str = Path(description="The id of the session"),
-    node: Annotated[
-        Union[BaseInvocation.get_invocations()], Field(discriminator="type") # type: ignore
-    ] = Body(description="The node to add"),
+    node: Annotated[Union[BaseInvocation.get_invocations()], Field(discriminator="type")] = Body(  # type: ignore
+        description="The node to add"
+    ),
 ) -> str:
     """Adds a node to the graph"""
     session = ApiDependencies.invoker.services.graph_execution_manager.get(session_id)
     if session is None:
         raise HTTPException(status_code=404)
 
     try:
@@ -120,17 +114,17 @@
         400: {"description": "Invalid node or link"},
         404: {"description": "Session not found"},
     },
 )
 async def update_node(
     session_id: str = Path(description="The id of the session"),
     node_path: str = Path(description="The path to the node in the graph"),
-    node: Annotated[
-        Union[BaseInvocation.get_invocations()], Field(discriminator="type") # type: ignore
-    ] = Body(description="The new node"),
+    node: Annotated[Union[BaseInvocation.get_invocations()], Field(discriminator="type")] = Body(  # type: ignore
+        description="The new node"
+    ),
 ) -> GraphExecutionState:
     """Updates a node in the graph and removes all linked edges"""
     session = ApiDependencies.invoker.services.graph_execution_manager.get(session_id)
     if session is None:
         raise HTTPException(status_code=404)
 
     try:
@@ -226,15 +220,15 @@
     session = ApiDependencies.invoker.services.graph_execution_manager.get(session_id)
     if session is None:
         raise HTTPException(status_code=404)
 
     try:
         edge = Edge(
             source=EdgeConnection(node_id=from_node_id, field=from_field),
-            destination=EdgeConnection(node_id=to_node_id, field=to_field)
+            destination=EdgeConnection(node_id=to_node_id, field=to_field),
         )
         session.delete_edge(edge)
         ApiDependencies.invoker.services.graph_execution_manager.set(
             session
         )  # TODO: can this be done automatically, or add node through an API?
         return session
     except NodeAlreadyExecutedError:
@@ -251,17 +245,15 @@
         202: {"description": "The invocation is queued"},
         400: {"description": "The session has no invocations ready to invoke"},
         404: {"description": "Session not found"},
     },
 )
 async def invoke_session(
     session_id: str = Path(description="The id of the session to invoke"),
-    all: bool = Query(
-        default=False, description="Whether or not to invoke all remaining invocations"
-    ),
+    all: bool = Query(default=False, description="Whether or not to invoke all remaining invocations"),
 ) -> Response:
     """Invokes a session"""
     session = ApiDependencies.invoker.services.graph_execution_manager.get(session_id)
     if session is None:
         raise HTTPException(status_code=404)
 
     if session.is_complete():
@@ -270,17 +262,15 @@
     ApiDependencies.invoker.invoke(session, invoke_all=all)
     return Response(status_code=202)
 
 
 @session_router.delete(
     "/{session_id}/invoke",
     operation_id="cancel_session_invoke",
-    responses={
-        202: {"description": "The invocation is canceled"}
-    },
+    responses={202: {"description": "The invocation is canceled"}},
 )
 async def cancel_session_invoke(
     session_id: str = Path(description="The id of the session to cancel"),
 ) -> Response:
     """Invokes a session"""
     ApiDependencies.invoker.cancel(session_id)
     return Response(status_code=202)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api/sockets.py` & `InvokeAI-3.0.1rc2/invokeai/app/api/sockets.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,17 +12,15 @@
     __sio: SocketManager
 
     def __init__(self, app: FastAPI):
         self.__sio = SocketManager(app=app)
         self.__sio.on("subscribe", handler=self._handle_sub)
         self.__sio.on("unsubscribe", handler=self._handle_unsub)
 
-        local_handler.register(
-            event_name=EventServiceBase.session_event, _func=self._handle_session_event
-        )
+        local_handler.register(event_name=EventServiceBase.session_event, _func=self._handle_session_event)
 
     async def _handle_session_event(self, event: Event):
         await self.__sio.emit(
             event=event[1]["event"],
             data=event[1]["data"],
             room=event[1]["data"]["graph_execution_state_id"],
         )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/api_app.py` & `InvokeAI-3.0.1rc2/invokeai/app/api_app.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,88 +1,88 @@
 # Copyright (c) 2022-2023 Kyle Schouviller (https://github.com/kyle0654) and the InvokeAI Team
 import asyncio
 import sys
 from inspect import signature
 
+import logging
 import uvicorn
 import socket
 
 from fastapi import FastAPI
 from fastapi.middleware.cors import CORSMiddleware
 from fastapi.openapi.docs import get_redoc_html, get_swagger_ui_html
 from fastapi.openapi.utils import get_openapi
 from fastapi.staticfiles import StaticFiles
 from fastapi_events.handlers.local import local_handler
 from fastapi_events.middleware import EventHandlerASGIMiddleware
 from pathlib import Path
 from pydantic.schema import schema
 
-#This should come early so that modules can log their initialization properly
+# This should come early so that modules can log their initialization properly
 from .services.config import InvokeAIAppConfig
 from ..backend.util.logging import InvokeAILogger
+
 app_config = InvokeAIAppConfig.get_config()
 app_config.parse_args()
 logger = InvokeAILogger.getLogger(config=app_config)
 from invokeai.version.invokeai_version import __version__
 
 # we call this early so that the message appears before
 # other invokeai initialization messages
 if app_config.version:
-    print(f'InvokeAI version {__version__}')
+    print(f"InvokeAI version {__version__}")
     sys.exit(0)
 
 import invokeai.frontend.web as web_dir
 import mimetypes
 
 from .api.dependencies import ApiDependencies
 from .api.routers import sessions, models, images, boards, board_images, app_info
 from .api.sockets import SocketIO
 from .invocations.baseinvocation import BaseInvocation
-    
+
 
 import torch
 import invokeai.backend.util.hotfixes
+
 if torch.backends.mps.is_available():
     import invokeai.backend.util.mps_fixes
 
 # fix for windows mimetypes registry entries being borked
 # see https://github.com/invoke-ai/InvokeAI/discussions/3684#discussioncomment-6391352
-mimetypes.add_type('application/javascript', '.js')
-mimetypes.add_type('text/css', '.css')
+mimetypes.add_type("application/javascript", ".js")
+mimetypes.add_type("text/css", ".css")
 
 # Create the app
 # TODO: create this all in a method so configuration/etc. can be passed in?
 app = FastAPI(title="Invoke AI", docs_url=None, redoc_url=None)
 
 # Add event handler
 event_handler_id: int = id(app)
 app.add_middleware(
     EventHandlerASGIMiddleware,
-    handlers=[
-        local_handler
-    ],  # TODO: consider doing this in services to support different configurations
+    handlers=[local_handler],  # TODO: consider doing this in services to support different configurations
     middleware_id=event_handler_id,
 )
 
 socket_io = SocketIO(app)
 
+
 # Add startup event to load dependencies
 @app.on_event("startup")
 async def startup_event():
     app.add_middleware(
         CORSMiddleware,
         allow_origins=app_config.allow_origins,
         allow_credentials=app_config.allow_credentials,
         allow_methods=app_config.allow_methods,
         allow_headers=app_config.allow_headers,
     )
 
-    ApiDependencies.initialize(
-        config=app_config, event_handler_id=event_handler_id, logger=logger
-    )
+    ApiDependencies.initialize(config=app_config, event_handler_id=event_handler_id, logger=logger)
 
 
 # Shut down threads
 @app.on_event("shutdown")
 async def shutdown_event():
     ApiDependencies.shutdown()
 
@@ -99,15 +99,16 @@
 
 app.include_router(images.images_router, prefix="/api")
 
 app.include_router(boards.boards_router, prefix="/api")
 
 app.include_router(board_images.board_images_router, prefix="/api")
 
-app.include_router(app_info.app_router, prefix='/api')
+app.include_router(app_info.app_router, prefix="/api")
+
 
 # Build a custom OpenAPI to include all outputs
 # TODO: can outputs be included on metadata of invocation schemas somehow?
 def custom_openapi():
     if app.openapi_schema:
         return app.openapi_schema
     openapi_schema = get_openapi(
@@ -140,14 +141,15 @@
         output_type_title = output_type_titles[output_type.__name__]
         invoker_schema = openapi_schema["components"]["schemas"][invoker_name]
         outputs_ref = {"$ref": f"#/components/schemas/{output_type_title}"}
 
         invoker_schema["output"] = outputs_ref
 
     from invokeai.backend.model_management.models import get_model_config_enums
+
     for model_config_format_enum in set(get_model_config_enums()):
         name = model_config_format_enum.__qualname__
 
         if name in openapi_schema["components"]["schemas"]:
             # print(f"Config with name {name} already defined")
             continue
 
@@ -162,15 +164,16 @@
     app.openapi_schema = openapi_schema
     return app.openapi_schema
 
 
 app.openapi = custom_openapi
 
 # Override API doc favicons
-app.mount("/static", StaticFiles(directory=Path(web_dir.__path__[0], 'static/dream_web')), name="static")
+app.mount("/static", StaticFiles(directory=Path(web_dir.__path__[0], "static/dream_web")), name="static")
+
 
 @app.get("/docs", include_in_schema=False)
 def overridden_swagger():
     return get_swagger_ui_html(
         openapi_url=app.openapi_url,
         title=app.title,
         swagger_favicon_url="/static/favicon.ico",
@@ -183,39 +186,52 @@
         openapi_url=app.openapi_url,
         title=app.title,
         redoc_favicon_url="/static/favicon.ico",
     )
 
 
 # Must mount *after* the other routes else it borks em
-app.mount("/", 
-          StaticFiles(directory=Path(web_dir.__path__[0],"dist"), 
-                      html=True
-                     ), name="ui"
-         )
+app.mount("/", StaticFiles(directory=Path(web_dir.__path__[0], "dist"), html=True), name="ui")
+
 
 def invoke_api():
     def find_port(port: int):
         """Find a port not in use starting at given port"""
         # Taken from https://waylonwalker.com/python-find-available-port/, thanks Waylon!
         # https://github.com/WaylonWalker
         with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
             if s.connect_ex(("localhost", port)) == 0:
                 return find_port(port=port + 1)
             else:
                 return port
-            
+
     from invokeai.backend.install.check_root import check_invokeai_root
+
     check_invokeai_root(app_config)  # note, may exit with an exception if root not set up
-    
+
     port = find_port(app_config.port)
     if port != app_config.port:
         logger.warn(f"Port {app_config.port} in use, using port {port}")
+        
     # Start our own event loop for eventing usage
     loop = asyncio.new_event_loop()
-    config = uvicorn.Config(app=app, host=app_config.host, port=port, loop=loop)
-    # Use access_log to turn off logging
+    config = uvicorn.Config(
+        app=app,
+        host=app_config.host,
+        port=port,
+        loop=loop,
+        log_level=app_config.log_level,
+    )
     server = uvicorn.Server(config)
+
+    # replace uvicorn's loggers with InvokeAI's for consistent appearance
+    for logname in ["uvicorn.access", "uvicorn"]:
+        l = logging.getLogger(logname)
+        l.handlers.clear()
+        for ch in logger.handlers:
+            l.addHandler(ch)
+    
     loop.run_until_complete(server.serve())
 
+
 if __name__ == "__main__":
     invoke_api()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/cli/commands.py` & `InvokeAI-3.0.1rc2/invokeai/app/cli/commands.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,16 +10,22 @@
 import invokeai.backend.util.logging as logger
 from ..invocations.baseinvocation import BaseInvocation
 from ..invocations.image import ImageField
 from ..services.graph import GraphExecutionState, LibraryGraph, Edge
 from ..services.invoker import Invoker
 
 
-def add_field_argument(command_parser, name: str, field, default_override = None):
-    default = default_override if default_override is not None else field.default if field.default_factory is None else field.default_factory()
+def add_field_argument(command_parser, name: str, field, default_override=None):
+    default = (
+        default_override
+        if default_override is not None
+        else field.default
+        if field.default_factory is None
+        else field.default_factory()
+    )
     if get_origin(field.type_) == Literal:
         allowed_values = get_args(field.type_)
         allowed_types = set()
         for val in allowed_values:
             allowed_types.add(type(val))
         allowed_types_list = list(allowed_types)
         field_type = allowed_types_list[0] if len(allowed_types) == 1 else Union[allowed_types_list]  # type: ignore
@@ -43,44 +49,42 @@
 
 
 def add_parsers(
     subparsers,
     commands: list[type],
     command_field: str = "type",
     exclude_fields: list[str] = ["id", "type"],
-    add_arguments: Union[Callable[[argparse.ArgumentParser], None],None] = None
-    ):
+    add_arguments: Union[Callable[[argparse.ArgumentParser], None], None] = None,
+):
     """Adds parsers for each command to the subparsers"""
 
     # Create subparsers for each command
     for command in commands:
         hints = get_type_hints(command)
         cmd_name = get_args(hints[command_field])[0]
         command_parser = subparsers.add_parser(cmd_name, help=command.__doc__)
 
         if add_arguments is not None:
             add_arguments(command_parser)
 
         # Convert all fields to arguments
-        fields = command.__fields__ # type: ignore
+        fields = command.__fields__  # type: ignore
         for name, field in fields.items():
             if name in exclude_fields:
                 continue
 
             add_field_argument(command_parser, name, field)
 
 
 def add_graph_parsers(
-    subparsers,
-    graphs: list[LibraryGraph],
-    add_arguments: Union[Callable[[argparse.ArgumentParser], None], None] = None
+    subparsers, graphs: list[LibraryGraph], add_arguments: Union[Callable[[argparse.ArgumentParser], None], None] = None
 ):
     for graph in graphs:
         command_parser = subparsers.add_parser(graph.name, help=graph.description)
-        
+
         if add_arguments is not None:
             add_arguments(command_parser)
 
         # Add arguments for inputs
         for exposed_input in graph.exposed_inputs:
             node = graph.graph.get_node(exposed_input.node_path)
             field = node.__fields__[exposed_input.field]
@@ -124,14 +128,15 @@
         self.get_session()
         self.session.add_edge(edge)
         self.invoker.services.graph_execution_manager.set(self.session)
 
 
 class ExitCli(Exception):
     """Exception to exit the CLI"""
+
     pass
 
 
 class BaseCommand(ABC, BaseModel):
     """A CLI command"""
 
     # All commands must include a type name like this:
@@ -151,47 +156,45 @@
     @classmethod
     def get_commands(cls):
         return tuple(BaseCommand.get_all_subclasses())
 
     @classmethod
     def get_commands_map(cls):
         # Get the type strings out of the literals and into a dictionary
-        return dict(map(lambda t: (get_args(get_type_hints(t)['type'])[0], t),BaseCommand.get_all_subclasses()))
+        return dict(map(lambda t: (get_args(get_type_hints(t)["type"])[0], t), BaseCommand.get_all_subclasses()))
 
     @abstractmethod
     def run(self, context: CliContext) -> None:
         """Run the command. Raise ExitCli to exit."""
         pass
 
 
 class ExitCommand(BaseCommand):
     """Exits the CLI"""
-    type: Literal['exit'] = 'exit'
+
+    type: Literal["exit"] = "exit"
 
     def run(self, context: CliContext) -> None:
         raise ExitCli()
 
 
 class HelpCommand(BaseCommand):
     """Shows help"""
-    type: Literal['help'] = 'help'
+
+    type: Literal["help"] = "help"
 
     def run(self, context: CliContext) -> None:
         context.parser.print_help()
 
 
 def get_graph_execution_history(
     graph_execution_state: GraphExecutionState,
 ) -> Iterable[str]:
     """Gets the history of fully-executed invocations for a graph execution"""
-    return (
-        n
-        for n in reversed(graph_execution_state.executed_history)
-        if n in graph_execution_state.graph.nodes
-    )
+    return (n for n in reversed(graph_execution_state.executed_history) if n in graph_execution_state.graph.nodes)
 
 
 def get_invocation_command(invocation) -> str:
     fields = invocation.__fields__.items()
     type_hints = get_type_hints(type(invocation))
     command = [invocation.type]
     for name, field in fields:
@@ -214,15 +217,16 @@
                 command.append(f"--{name} {field_value}")
 
     return " ".join(command)
 
 
 class HistoryCommand(BaseCommand):
     """Shows the invocation history"""
-    type: Literal['history'] = 'history'
+
+    type: Literal["history"] = "history"
 
     # Inputs
     # fmt: off
     count: int = Field(default=5, gt=0, description="The number of history entries to show")
     # fmt: on
 
     def run(self, context: CliContext) -> None:
@@ -231,15 +235,16 @@
             entry_id = history[-1 - i]
             entry = context.get_session().graph.get_node(entry_id)
             logger.info(f"{entry_id}: {get_invocation_command(entry)}")
 
 
 class SetDefaultCommand(BaseCommand):
     """Sets a default value for a field"""
-    type: Literal['default'] = 'default'
+
+    type: Literal["default"] = "default"
 
     # Inputs
     # fmt: off
     field: str = Field(description="The field to set the default for")
     value: str = Field(description="The value to set the default to, or None to clear the default")
     # fmt: on
 
@@ -249,15 +254,16 @@
                 del context.defaults[self.field]
         else:
             context.defaults[self.field] = self.value
 
 
 class DrawGraphCommand(BaseCommand):
     """Debugs a graph"""
-    type: Literal['draw_graph'] = 'draw_graph'
+
+    type: Literal["draw_graph"] = "draw_graph"
 
     def run(self, context: CliContext) -> None:
         session: GraphExecutionState = context.invoker.services.graph_execution_manager.get(context.session.id)
         nxgraph = session.graph.nx_graph_flat()
 
         # Draw the networkx graph
         plt.figure(figsize=(20, 20))
@@ -267,29 +273,31 @@
         nx.draw_networkx_labels(nxgraph, pos, font_size=20, font_family="sans-serif")
         plt.axis("off")
         plt.show()
 
 
 class DrawExecutionGraphCommand(BaseCommand):
     """Debugs an execution graph"""
-    type: Literal['draw_xgraph'] = 'draw_xgraph'
+
+    type: Literal["draw_xgraph"] = "draw_xgraph"
 
     def run(self, context: CliContext) -> None:
         session: GraphExecutionState = context.invoker.services.graph_execution_manager.get(context.session.id)
         nxgraph = session.execution_graph.nx_graph_flat()
 
         # Draw the networkx graph
         plt.figure(figsize=(20, 20))
         pos = nx.spectral_layout(nxgraph)
         nx.draw_networkx_nodes(nxgraph, pos, node_size=1000)
         nx.draw_networkx_edges(nxgraph, pos, width=2)
         nx.draw_networkx_labels(nxgraph, pos, font_size=20, font_family="sans-serif")
         plt.axis("off")
         plt.show()
 
+
 class SortedHelpFormatter(argparse.HelpFormatter):
     def _iter_indented_subactions(self, action):
         try:
             get_subactions = action._get_subactions
         except AttributeError:
             pass
         else:
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/cli/completer.py` & `InvokeAI-3.0.1rc2/invokeai/app/cli/completer.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,16 +15,16 @@
 from ..invocations.baseinvocation import BaseInvocation
 from .commands import BaseCommand
 from ..services.invocation_services import InvocationServices
 
 # singleton object, class variable
 completer = None
 
+
 class Completer(object):
-    
     def __init__(self, model_manager: ModelManager):
         self.commands = self.get_commands()
         self.matches = None
         self.linebuffer = None
         self.manager = model_manager
         return
 
@@ -39,109 +39,113 @@
             options = None
             try:
                 current_command, current_switch = self.get_current_command(buffer)
                 options = self.get_command_options(current_command, current_switch)
             except IndexError:
                 pass
             options = options or list(self.parse_commands().keys())
-            
+
             if not text:  # first time
                 self.matches = options
             else:
                 self.matches = [s for s in options if s and s.startswith(text)]
 
         try:
             match = self.matches[state]
         except IndexError:
             match = None
         return match
 
     @classmethod
-    def get_commands(self)->List[object]:
+    def get_commands(self) -> List[object]:
         """
         Return a list of all the client commands and invocations.
         """
         return BaseCommand.get_commands() + BaseInvocation.get_invocations()
 
-    def get_current_command(self, buffer: str)->tuple[str, str]:
+    def get_current_command(self, buffer: str) -> tuple[str, str]:
         """
         Parse the readline buffer to find the most recent command and its switch.
         """
-        if len(buffer)==0:
+        if len(buffer) == 0:
             return None, None
         tokens = shlex.split(buffer)
         command = None
         switch = None
         for t in tokens:
             if t[0].isalpha():
                 if switch is None:
                     command = t
             else:
                 switch = t
         # don't try to autocomplete switches that are already complete
-        if switch and buffer.endswith(' '):
-            switch=None
-        return command or '', switch or ''
+        if switch and buffer.endswith(" "):
+            switch = None
+        return command or "", switch or ""
 
-    def parse_commands(self)->Dict[str, List[str]]:
+    def parse_commands(self) -> Dict[str, List[str]]:
         """
         Return a dict in which the keys are the command name
         and the values are the parameters the command takes.
         """
         result = dict()
         for command in self.commands:
             hints = get_type_hints(command)
-            name = get_args(hints['type'])[0]
-            result.update({name:hints})
+            name = get_args(hints["type"])[0]
+            result.update({name: hints})
         return result
 
-    def get_command_options(self, command: str, switch: str)->List[str]:
+    def get_command_options(self, command: str, switch: str) -> List[str]:
         """
         Return all the parameters that can be passed to the command as
         command-line switches. Returns None if the command is unrecognized.
         """
         parsed_commands = self.parse_commands()
         if command not in parsed_commands:
             return None
-        
+
         # handle switches in the format "-foo=bar"
         argument = None
-        if switch and '=' in switch:
-            switch, argument = switch.split('=')
-            
-        parameter = switch.strip('-')
+        if switch and "=" in switch:
+            switch, argument = switch.split("=")
+
+        parameter = switch.strip("-")
         if parameter in parsed_commands[command]:
             if argument is None:
                 return self.get_parameter_options(parameter, parsed_commands[command][parameter])
             else:
-                return [f"--{parameter}={x}" for x in self.get_parameter_options(parameter, parsed_commands[command][parameter])]
+                return [
+                    f"--{parameter}={x}"
+                    for x in self.get_parameter_options(parameter, parsed_commands[command][parameter])
+                ]
         else:
             return [f"--{x}" for x in parsed_commands[command].keys()]
 
-    def get_parameter_options(self, parameter: str, typehint)->List[str]:
+    def get_parameter_options(self, parameter: str, typehint) -> List[str]:
         """
         Given a parameter type (such as Literal), offers autocompletions.
         """
         if get_origin(typehint) == Literal:
             return get_args(typehint)
-        if parameter == 'model':
+        if parameter == "model":
             return self.manager.model_names()
-        
+
     def _pre_input_hook(self):
         if self.linebuffer:
             readline.insert_text(self.linebuffer)
             readline.redisplay()
             self.linebuffer = None
-    
+
+
 def set_autocompleter(services: InvocationServices) -> Completer:
     global completer
-    
+
     if completer:
         return completer
-    
+
     completer = Completer(services.model_manager)
 
     readline.set_completer(completer.complete)
     # pyreadline3 does not have a set_auto_history() method
     try:
         readline.set_auto_history(True)
     except:
@@ -158,12 +162,10 @@
     try:
         readline.read_history_file(histfile)
         readline.set_history_length(1000)
     except FileNotFoundError:
         pass
     except OSError:  # file likely corrupted
         newname = f"{histfile}.old"
-        logger.error(
-            f"Your history file {histfile} couldn't be loaded and may be corrupted. Renaming it to {newname}"
-        )
+        logger.error(f"Your history file {histfile} couldn't be loaded and may be corrupted. Renaming it to {newname}")
         histfile.replace(Path(newname))
     atexit.register(readline.write_history_file, histfile)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/cli_app.py` & `InvokeAI-3.0.1rc2/invokeai/app/cli_app.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,22 +9,23 @@
 
 from pydantic import BaseModel, ValidationError
 from pydantic.fields import Field
 
 # This should come early so that the logger can pick up its configuration options
 from .services.config import InvokeAIAppConfig
 from invokeai.backend.util.logging import InvokeAILogger
+
 config = InvokeAIAppConfig.get_config()
 config.parse_args()
 logger = InvokeAILogger().getLogger(config=config)
 from invokeai.version.invokeai_version import __version__
 
 # we call this early so that the message appears before other invokeai initialization messages
 if config.version:
-    print(f'InvokeAI version {__version__}')
+    print(f"InvokeAI version {__version__}")
     sys.exit(0)
 
 from invokeai.app.services.board_image_record_storage import (
     SqliteBoardImageRecordStorage,
 )
 from invokeai.app.services.board_images import (
     BoardImagesService,
@@ -32,47 +33,52 @@
 )
 from invokeai.app.services.board_record_storage import SqliteBoardRecordStorage
 from invokeai.app.services.boards import BoardService, BoardServiceDependencies
 from invokeai.app.services.image_record_storage import SqliteImageRecordStorage
 from invokeai.app.services.images import ImageService, ImageServiceDependencies
 from invokeai.app.services.resource_name import SimpleNameService
 from invokeai.app.services.urls import LocalUrlService
-from .services.default_graphs import (default_text_to_image_graph_id,
-                                      create_system_graphs)
+from .services.default_graphs import default_text_to_image_graph_id, create_system_graphs
 from .services.latent_storage import DiskLatentsStorage, ForwardCacheLatentsStorage
 
-from .cli.commands import (BaseCommand, CliContext, ExitCli,
-                           SortedHelpFormatter, add_graph_parsers, add_parsers)
+from .cli.commands import BaseCommand, CliContext, ExitCli, SortedHelpFormatter, add_graph_parsers, add_parsers
 from .cli.completer import set_autocompleter
 from .invocations.baseinvocation import BaseInvocation
 from .services.events import EventServiceBase
-from .services.graph import (Edge, EdgeConnection, GraphExecutionState,
-                             GraphInvocation, LibraryGraph,
-                             are_connection_types_compatible)
+from .services.graph import (
+    Edge,
+    EdgeConnection,
+    GraphExecutionState,
+    GraphInvocation,
+    LibraryGraph,
+    are_connection_types_compatible,
+)
 from .services.image_file_storage import DiskImageFileStorage
 from .services.invocation_queue import MemoryInvocationQueue
 from .services.invocation_services import InvocationServices
 from .services.invoker import Invoker
 from .services.model_manager_service import ModelManagerService
 from .services.processor import DefaultInvocationProcessor
 from .services.sqlite import SqliteItemStorage
 
 import torch
 import invokeai.backend.util.hotfixes
+
 if torch.backends.mps.is_available():
     import invokeai.backend.util.mps_fixes
 
 
 class CliCommand(BaseModel):
     command: Union[BaseCommand.get_commands() + BaseInvocation.get_invocations()] = Field(discriminator="type")  # type: ignore
 
 
 class InvalidArgs(Exception):
     pass
 
+
 def add_invocation_args(command_parser):
     # Add linking capability
     command_parser.add_argument(
         "--link",
         "-l",
         action="append",
         nargs=3,
@@ -109,44 +115,54 @@
     # TODO: add a way to identify these graphs
     text_to_image = services.graph_library.get(default_text_to_image_graph_id)
     add_graph_parsers(subparsers, [text_to_image], add_arguments=add_invocation_args)
 
     return parser
 
 
-class NodeField():
+class NodeField:
     alias: str
     node_path: str
     field: str
     field_type: type
 
     def __init__(self, alias: str, node_path: str, field: str, field_type: type):
         self.alias = alias
         self.node_path = node_path
         self.field = field
         self.field_type = field_type
 
 
-def fields_from_type_hints(hints: dict[str, type], node_path: str) -> dict[str,NodeField]:
-    return {k:NodeField(alias=k, node_path=node_path, field=k, field_type=v) for k, v in hints.items()}
+def fields_from_type_hints(hints: dict[str, type], node_path: str) -> dict[str, NodeField]:
+    return {k: NodeField(alias=k, node_path=node_path, field=k, field_type=v) for k, v in hints.items()}
 
 
 def get_node_input_field(graph: LibraryGraph, field_alias: str, node_id: str) -> NodeField:
     """Gets the node field for the specified field alias"""
     exposed_input = next(e for e in graph.exposed_inputs if e.alias == field_alias)
     node_type = type(graph.graph.get_node(exposed_input.node_path))
-    return NodeField(alias=exposed_input.alias, node_path=f'{node_id}.{exposed_input.node_path}', field=exposed_input.field, field_type=get_type_hints(node_type)[exposed_input.field])
+    return NodeField(
+        alias=exposed_input.alias,
+        node_path=f"{node_id}.{exposed_input.node_path}",
+        field=exposed_input.field,
+        field_type=get_type_hints(node_type)[exposed_input.field],
+    )
 
 
 def get_node_output_field(graph: LibraryGraph, field_alias: str, node_id: str) -> NodeField:
     """Gets the node field for the specified field alias"""
     exposed_output = next(e for e in graph.exposed_outputs if e.alias == field_alias)
     node_type = type(graph.graph.get_node(exposed_output.node_path))
     node_output_type = node_type.get_output_type()
-    return NodeField(alias=exposed_output.alias, node_path=f'{node_id}.{exposed_output.node_path}', field=exposed_output.field, field_type=get_type_hints(node_output_type)[exposed_output.field])
+    return NodeField(
+        alias=exposed_output.alias,
+        node_path=f"{node_id}.{exposed_output.node_path}",
+        field=exposed_output.field,
+        field_type=get_type_hints(node_output_type)[exposed_output.field],
+    )
 
 
 def get_node_inputs(invocation: BaseInvocation, context: CliContext) -> dict[str, NodeField]:
     """Gets the inputs for the specified invocation from the context"""
     node_type = type(invocation)
     if node_type is not GraphInvocation:
         return fields_from_type_hints(get_type_hints(node_type), invocation.id)
@@ -161,42 +177,43 @@
     if node_type is not GraphInvocation:
         return fields_from_type_hints(get_type_hints(node_type.get_output_type()), invocation.id)
     else:
         graph: LibraryGraph = context.invoker.services.graph_library.get(context.graph_nodes[invocation.id])
         return {e.alias: get_node_output_field(graph, e.alias, invocation.id) for e in graph.exposed_outputs}
 
 
-def generate_matching_edges(
-    a: BaseInvocation, b: BaseInvocation, context: CliContext
-) -> list[Edge]:
+def generate_matching_edges(a: BaseInvocation, b: BaseInvocation, context: CliContext) -> list[Edge]:
     """Generates all possible edges between two invocations"""
     afields = get_node_outputs(a, context)
     bfields = get_node_inputs(b, context)
 
     matching_fields = set(afields.keys()).intersection(bfields.keys())
 
     # Remove invalid fields
     invalid_fields = set(["type", "id"])
     matching_fields = matching_fields.difference(invalid_fields)
 
     # Validate types
-    matching_fields = [f for f in matching_fields if are_connection_types_compatible(afields[f].field_type, bfields[f].field_type)]
+    matching_fields = [
+        f for f in matching_fields if are_connection_types_compatible(afields[f].field_type, bfields[f].field_type)
+    ]
 
     edges = [
         Edge(
             source=EdgeConnection(node_id=afields[alias].node_path, field=afields[alias].field),
-            destination=EdgeConnection(node_id=bfields[alias].node_path, field=bfields[alias].field)
+            destination=EdgeConnection(node_id=bfields[alias].node_path, field=bfields[alias].field),
         )
         for alias in matching_fields
     ]
     return edges
 
 
 class SessionError(Exception):
     """Raised when a session error has occurred"""
+
     pass
 
 
 def invoke_all(context: CliContext):
     """Runs all invocations in the specified session"""
     context.invoker.invoke(context.session, invoke_all=True)
     while not context.get_session().is_complete():
@@ -205,46 +222,47 @@
 
     # Print any errors
     if context.session.has_error():
         for n in context.session.errors:
             context.invoker.services.logger.error(
                 f"Error in node {n} (source node {context.session.prepared_source_mapping[n]}): {context.session.errors[n]}"
             )
-        
+
         raise SessionError()
 
+
 def invoke_cli():
-    logger.info(f'InvokeAI version {__version__}')
+    logger.info(f"InvokeAI version {__version__}")
     # get the optional list of invocations to execute on the command line
     parser = config.get_parser()
-    parser.add_argument('commands',nargs='*')
+    parser.add_argument("commands", nargs="*")
     invocation_commands = parser.parse_args().commands
 
     # get the optional file to read commands from.
     # Simplest is to use it for STDIN
     if infile := config.from_file:
-        sys.stdin = open(infile,"r")
-    
-    model_manager = ModelManagerService(config,logger)
+        sys.stdin = open(infile, "r")
+
+    model_manager = ModelManagerService(config, logger)
 
     events = EventServiceBase()
     output_folder = config.output_path
 
     # TODO: build a file/path manager?
     if config.use_memory_db:
         db_location = ":memory:"
     else:
         db_location = config.db_path
-        db_location.parent.mkdir(parents=True,exist_ok=True)
+        db_location.parent.mkdir(parents=True, exist_ok=True)
 
     logger.info(f'InvokeAI database location is "{db_location}"')
 
     graph_execution_manager = SqliteItemStorage[GraphExecutionState](
-            filename=db_location, table_name="graph_executions"
-        )
+        filename=db_location, table_name="graph_executions"
+    )
 
     urls = LocalUrlService()
     image_record_storage = SqliteImageRecordStorage(db_location)
     image_file_storage = DiskImageFileStorage(f"{output_folder}/images")
     names = SimpleNameService()
 
     board_record_storage = SqliteBoardRecordStorage(db_location)
@@ -277,66 +295,63 @@
             image_file_storage=image_file_storage,
             url=urls,
             logger=logger,
             names=names,
             graph_execution_manager=graph_execution_manager,
         )
     )
-        
+
     services = InvocationServices(
         model_manager=model_manager,
         events=events,
-        latents = ForwardCacheLatentsStorage(DiskLatentsStorage(f'{output_folder}/latents')),
+        latents=ForwardCacheLatentsStorage(DiskLatentsStorage(f"{output_folder}/latents")),
         images=images,
         boards=boards,
         board_images=board_images,
         queue=MemoryInvocationQueue(),
-        graph_library=SqliteItemStorage[LibraryGraph](
-            filename=db_location, table_name="graphs"
-        ),
+        graph_library=SqliteItemStorage[LibraryGraph](filename=db_location, table_name="graphs"),
         graph_execution_manager=graph_execution_manager,
         processor=DefaultInvocationProcessor(),
         logger=logger,
         configuration=config,
     )
-    
 
     system_graphs = create_system_graphs(services.graph_library)
     system_graph_names = set([g.name for g in system_graphs])
     set_autocompleter(services)
 
     invoker = Invoker(services)
     session: GraphExecutionState = invoker.create_execution_state()
     parser = get_command_parser(services)
 
-    re_negid = re.compile('^-[0-9]+$')
+    re_negid = re.compile("^-[0-9]+$")
 
     # Uncomment to print out previous sessions at startup
     # print(services.session_manager.list())
 
     context = CliContext(invoker, session, parser)
     set_autocompleter(services)
 
     command_line_args_exist = len(invocation_commands) > 0
     done = False
-    
+
     while not done:
         try:
             if command_line_args_exist:
                 cmd_input = invocation_commands.pop(0)
                 done = len(invocation_commands) == 0
             else:
                 cmd_input = input("invoke> ")
         except (KeyboardInterrupt, EOFError):
             # Ctrl-c exits
             break
 
         try:
             # Refresh the state of the session
-            #history = list(get_graph_execution_history(context.session))
+            # history = list(get_graph_execution_history(context.session))
             history = list(reversed(context.nodes_added))
 
             # Split the command for piping
             cmds = cmd_input.split("|")
             start_id = len(context.nodes_added)
             current_id = start_id
             new_invocations = list()
@@ -349,25 +364,25 @@
 
                 # Override defaults
                 for field_name, field_default in context.defaults.items():
                     if field_name in args:
                         args[field_name] = field_default
 
                 # Parse invocation
-                command: CliCommand = None # type:ignore
+                command: CliCommand = None  # type:ignore
                 system_graph: Optional[LibraryGraph] = None
-                if args['type'] in system_graph_names:
-                    system_graph = next(filter(lambda g: g.name == args['type'], system_graphs))
+                if args["type"] in system_graph_names:
+                    system_graph = next(filter(lambda g: g.name == args["type"], system_graphs))
                     invocation = GraphInvocation(graph=system_graph.graph, id=str(current_id))
                     for exposed_input in system_graph.exposed_inputs:
                         if exposed_input.alias in args:
                             node = invocation.graph.get_node(exposed_input.node_path)
                             field = exposed_input.field
                             setattr(node, field, args[exposed_input.alias])
-                    command = CliCommand(command = invocation)
+                    command = CliCommand(command=invocation)
                     context.graph_nodes[invocation.id] = system_graph.id
                 else:
                     args["id"] = current_id
                     command = CliCommand(command=args)
 
                 if command is None:
                     continue
@@ -381,58 +396,56 @@
                     command.command.run(context)
                     continue
 
                 # TODO: handle linking with library graphs
                 # Pipe previous command output (if there was a previous command)
                 edges: list[Edge] = list()
                 if len(history) > 0 or current_id != start_id:
-                    from_id = (
-                        history[0] if current_id == start_id else str(current_id - 1)
-                    )
+                    from_id = history[0] if current_id == start_id else str(current_id - 1)
                     from_node = (
                         next(filter(lambda n: n[0].id == from_id, new_invocations))[0]
                         if current_id != start_id
                         else context.session.graph.get_node(from_id)
                     )
-                    matching_edges = generate_matching_edges(
-                        from_node, command.command, context
-                    )
+                    matching_edges = generate_matching_edges(from_node, command.command, context)
                     edges.extend(matching_edges)
 
                 # Parse provided links
                 if "link_node" in args and args["link_node"]:
                     for link in args["link_node"]:
                         node_id = link
                         if re_negid.match(node_id):
                             node_id = str(current_id + int(node_id))
 
                         link_node = context.session.graph.get_node(node_id)
-                        matching_edges = generate_matching_edges(
-                            link_node, command.command, context
-                        )
+                        matching_edges = generate_matching_edges(link_node, command.command, context)
                         matching_destinations = [e.destination for e in matching_edges]
                         edges = [e for e in edges if e.destination not in matching_destinations]
                         edges.extend(matching_edges)
 
                 if "link" in args and args["link"]:
                     for link in args["link"]:
-                        edges = [e for e in edges if e.destination.node_id != command.command.id or e.destination.field != link[2]]
+                        edges = [
+                            e
+                            for e in edges
+                            if e.destination.node_id != command.command.id or e.destination.field != link[2]
+                        ]
 
                         node_id = link[0]
                         if re_negid.match(node_id):
                             node_id = str(current_id + int(node_id))
 
                         # TODO: handle missing input/output
                         node_output = get_node_outputs(context.session.graph.get_node(node_id), context)[link[1]]
                         node_input = get_node_inputs(command.command, context)[link[2]]
 
                         edges.append(
                             Edge(
                                 source=EdgeConnection(node_id=node_output.node_path, field=node_output.field),
-                                destination=EdgeConnection(node_id=node_input.node_path, field=node_input.field)
+                                destination=EdgeConnection(node_id=node_input.node_path, field=node_input.field),
                             )
                         )
 
                 new_invocations.append((command.command, edges))
 
                 current_id = current_id + 1
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/baseinvocation.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/baseinvocation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 # Copyright (c) 2022 Kyle Schouviller (https://github.com/kyle0654)
 
 from __future__ import annotations
 
 from abc import ABC, abstractmethod
 from inspect import signature
-from typing import (TYPE_CHECKING, Dict, List, Literal, TypedDict, get_args,
-                    get_type_hints)
+from typing import TYPE_CHECKING, Dict, List, Literal, TypedDict, get_args, get_type_hints
 
 from pydantic import BaseConfig, BaseModel, Field
 
 if TYPE_CHECKING:
     from ..services.invocation_services import InvocationServices
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/collections.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/collections.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,16 +4,15 @@
 
 import numpy as np
 from pydantic import Field, validator
 
 from invokeai.app.models.image import ImageField
 from invokeai.app.util.misc import SEED_MAX, get_random_seed
 
-from .baseinvocation import (BaseInvocation, BaseInvocationOutput,
-                             InvocationConfig, InvocationContext, UIConfig)
+from .baseinvocation import BaseInvocation, BaseInvocationOutput, InvocationConfig, InvocationContext, UIConfig
 
 
 class IntCollectionOutput(BaseInvocationOutput):
     """A collection of integers"""
 
     type: Literal["int_collection"] = "int_collection"
 
@@ -23,26 +22,24 @@
 
 class FloatCollectionOutput(BaseInvocationOutput):
     """A collection of floats"""
 
     type: Literal["float_collection"] = "float_collection"
 
     # Outputs
-    collection: list[float] = Field(
-        default=[], description="The float collection")
+    collection: list[float] = Field(default=[], description="The float collection")
 
 
 class ImageCollectionOutput(BaseInvocationOutput):
     """A collection of images"""
 
     type: Literal["image_collection"] = "image_collection"
 
     # Outputs
-    collection: list[ImageField] = Field(
-        default=[], description="The output images")
+    collection: list[ImageField] = Field(default=[], description="The output images")
 
     class Config:
         schema_extra = {"required": ["type", "collection"]}
 
 
 class RangeInvocation(BaseInvocation):
     """Creates a range of numbers from start to stop with step"""
@@ -52,91 +49,70 @@
     # Inputs
     start: int = Field(default=0, description="The start of the range")
     stop: int = Field(default=10, description="The stop of the range")
     step: int = Field(default=1, description="The step of the range")
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Range",
-                "tags": ["range", "integer", "collection"]
-            },
+            "ui": {"title": "Range", "tags": ["range", "integer", "collection"]},
         }
 
     @validator("stop")
     def stop_gt_start(cls, v, values):
         if "start" in values and v <= values["start"]:
             raise ValueError("stop must be greater than start")
         return v
 
     def invoke(self, context: InvocationContext) -> IntCollectionOutput:
-        return IntCollectionOutput(
-            collection=list(range(self.start, self.stop, self.step))
-        )
+        return IntCollectionOutput(collection=list(range(self.start, self.stop, self.step)))
 
 
 class RangeOfSizeInvocation(BaseInvocation):
     """Creates a range from start to start + size with step"""
 
     type: Literal["range_of_size"] = "range_of_size"
 
     # Inputs
     start: int = Field(default=0, description="The start of the range")
     size: int = Field(default=1, description="The number of values")
     step: int = Field(default=1, description="The step of the range")
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Sized Range",
-                "tags": ["range", "integer", "size", "collection"]
-            },
+            "ui": {"title": "Sized Range", "tags": ["range", "integer", "size", "collection"]},
         }
 
     def invoke(self, context: InvocationContext) -> IntCollectionOutput:
-        return IntCollectionOutput(
-            collection=list(
-                range(
-                    self.start, self.start + self.size,
-                    self.step)))
+        return IntCollectionOutput(collection=list(range(self.start, self.start + self.size, self.step)))
 
 
 class RandomRangeInvocation(BaseInvocation):
     """Creates a collection of random numbers"""
 
     type: Literal["random_range"] = "random_range"
 
     # Inputs
     low: int = Field(default=0, description="The inclusive low value")
-    high: int = Field(
-        default=np.iinfo(np.int32).max, description="The exclusive high value"
-    )
+    high: int = Field(default=np.iinfo(np.int32).max, description="The exclusive high value")
     size: int = Field(default=1, description="The number of values to generate")
     seed: int = Field(
         ge=0,
         le=SEED_MAX,
         description="The seed for the RNG (omit for random)",
         default_factory=get_random_seed,
     )
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Random Range",
-                "tags": ["range", "integer", "random", "collection"]
-            },
+            "ui": {"title": "Random Range", "tags": ["range", "integer", "random", "collection"]},
         }
 
     def invoke(self, context: InvocationContext) -> IntCollectionOutput:
         rng = np.random.default_rng(self.seed)
-        return IntCollectionOutput(
-            collection=list(
-                rng.integers(
-                    low=self.low, high=self.high,
-                    size=self.size)))
+        return IntCollectionOutput(collection=list(rng.integers(low=self.low, high=self.high, size=self.size)))
 
 
 class ImageCollectionInvocation(BaseInvocation):
     """Load a collection of images and provide it as output."""
 
     # fmt: off
     type: Literal["image_collection"] = "image_collection"
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/compel.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/compel.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,110 +1,104 @@
 from typing import Literal, Optional, Union, List, Annotated
 from pydantic import BaseModel, Field
 import re
 import torch
 from compel import Compel, ReturnedEmbeddingsType
-from compel.prompt_parser import (Blend, Conjunction,
-                                  CrossAttentionControlSubstitute,
-                                  FlattenedPrompt, Fragment)
+from compel.prompt_parser import Blend, Conjunction, CrossAttentionControlSubstitute, FlattenedPrompt, Fragment
 from ...backend.util.devices import torch_dtype
 from ...backend.model_management import ModelType
 from ...backend.model_management.models import ModelNotFoundException
 from ...backend.model_management.lora import ModelPatcher
 from ...backend.stable_diffusion.diffusion import InvokeAIDiffuserComponent
-from .baseinvocation import (BaseInvocation, BaseInvocationOutput,
-                             InvocationConfig, InvocationContext)
+from .baseinvocation import BaseInvocation, BaseInvocationOutput, InvocationConfig, InvocationContext
 from .model import ClipField
 from dataclasses import dataclass
 
 
 class ConditioningField(BaseModel):
-    conditioning_name: Optional[str] = Field(
-        default=None, description="The name of conditioning data")
+    conditioning_name: Optional[str] = Field(default=None, description="The name of conditioning data")
 
     class Config:
         schema_extra = {"required": ["conditioning_name"]}
 
+
 @dataclass
 class BasicConditioningInfo:
-    #type: Literal["basic_conditioning"] = "basic_conditioning"
+    # type: Literal["basic_conditioning"] = "basic_conditioning"
     embeds: torch.Tensor
     extra_conditioning: Optional[InvokeAIDiffuserComponent.ExtraConditioningInfo]
     # weight: float
     # mode: ConditioningAlgo
 
+
 @dataclass
 class SDXLConditioningInfo(BasicConditioningInfo):
-    #type: Literal["sdxl_conditioning"] = "sdxl_conditioning"
+    # type: Literal["sdxl_conditioning"] = "sdxl_conditioning"
     pooled_embeds: torch.Tensor
     add_time_ids: torch.Tensor
 
-ConditioningInfoType = Annotated[
-    Union[BasicConditioningInfo, SDXLConditioningInfo],
-    Field(discriminator="type")
-]
+
+ConditioningInfoType = Annotated[Union[BasicConditioningInfo, SDXLConditioningInfo], Field(discriminator="type")]
+
 
 @dataclass
 class ConditioningFieldData:
     conditionings: List[Union[BasicConditioningInfo, SDXLConditioningInfo]]
-    #unconditioned: Optional[torch.Tensor]
+    # unconditioned: Optional[torch.Tensor]
+
 
-#class ConditioningAlgo(str, Enum):
+# class ConditioningAlgo(str, Enum):
 #    Compose = "compose"
 #    ComposeEx = "compose_ex"
 #    PerpNeg = "perp_neg"
 
+
 class CompelOutput(BaseInvocationOutput):
     """Compel parser output"""
 
-    #fmt: off
+    # fmt: off
     type: Literal["compel_output"] = "compel_output"
 
     conditioning: ConditioningField = Field(default=None, description="Conditioning")
-    #fmt: on
+    # fmt: on
 
 
 class CompelInvocation(BaseInvocation):
     """Parse prompt using compel package to conditioning."""
 
     type: Literal["compel"] = "compel"
 
     prompt: str = Field(default="", description="Prompt")
     clip: ClipField = Field(None, description="Clip to use")
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Prompt (Compel)",
-                "tags": ["prompt", "compel"],
-                "type_hints": {
-                    "model": "model"
-                }
-            },
+            "ui": {"title": "Prompt (Compel)", "tags": ["prompt", "compel"], "type_hints": {"model": "model"}},
         }
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> CompelOutput:
         tokenizer_info = context.services.model_manager.get_model(
-            **self.clip.tokenizer.dict(), context=context,
+            **self.clip.tokenizer.dict(),
+            context=context,
         )
         text_encoder_info = context.services.model_manager.get_model(
-            **self.clip.text_encoder.dict(), context=context,
+            **self.clip.text_encoder.dict(),
+            context=context,
         )
 
         def _lora_loader():
             for lora in self.clip.loras:
-                lora_info = context.services.model_manager.get_model(
-                    **lora.dict(exclude={"weight"}), context=context)
+                lora_info = context.services.model_manager.get_model(**lora.dict(exclude={"weight"}), context=context)
                 yield (lora_info.context.model, lora.weight)
                 del lora_info
             return
 
-        #loras = [(context.services.model_manager.get_model(**lora.dict(exclude={"weight"})).context.model, lora.weight) for lora in self.clip.loras]
+        # loras = [(context.services.model_manager.get_model(**lora.dict(exclude={"weight"})).context.model, lora.weight) for lora in self.clip.loras]
 
         ti_list = []
         for trigger in re.findall(r"<[a-zA-Z0-9., _-]+>", self.prompt):
             name = trigger[1:-1]
             try:
                 ti_list.append(
                     context.services.model_manager.get_model(
@@ -112,45 +106,46 @@
                         base_model=self.clip.text_encoder.base_model,
                         model_type=ModelType.TextualInversion,
                         context=context,
                     ).context.model
                 )
             except ModelNotFoundException:
                 # print(e)
-                #import traceback
-                #print(traceback.format_exc())
-                print(f"Warn: trigger: \"{trigger}\" not found")
-
-        with ModelPatcher.apply_lora_text_encoder(text_encoder_info.context.model, _lora_loader()),\
-                ModelPatcher.apply_ti(tokenizer_info.context.model, text_encoder_info.context.model, ti_list) as (tokenizer, ti_manager),\
-                ModelPatcher.apply_clip_skip(text_encoder_info.context.model, self.clip.skipped_layers),\
-                text_encoder_info as text_encoder:
-
+                # import traceback
+                # print(traceback.format_exc())
+                print(f'Warn: trigger: "{trigger}" not found')
+
+        with ModelPatcher.apply_lora_text_encoder(
+            text_encoder_info.context.model, _lora_loader()
+        ), ModelPatcher.apply_ti(tokenizer_info.context.model, text_encoder_info.context.model, ti_list) as (
+            tokenizer,
+            ti_manager,
+        ), ModelPatcher.apply_clip_skip(
+            text_encoder_info.context.model, self.clip.skipped_layers
+        ), text_encoder_info as text_encoder:
             compel = Compel(
                 tokenizer=tokenizer,
                 text_encoder=text_encoder,
                 textual_inversion_manager=ti_manager,
                 dtype_for_device_getter=torch_dtype,
                 truncate_long_prompts=True,
             )
 
             conjunction = Compel.parse_prompt_string(self.prompt)
             prompt: Union[FlattenedPrompt, Blend] = conjunction.prompts[0]
 
             if context.services.configuration.log_tokenization:
                 log_tokenization_for_prompt_object(prompt, tokenizer)
 
-            c, options = compel.build_conditioning_tensor_for_prompt_object(
-                prompt)
+            c, options = compel.build_conditioning_tensor_for_prompt_object(prompt)
 
             ec = InvokeAIDiffuserComponent.ExtraConditioningInfo(
-                tokens_count_including_eos_bos=get_max_token_count(
-                    tokenizer, conjunction),
-                cross_attention_control_args=options.get(
-                    "cross_attention_control", None),)
+                tokens_count_including_eos_bos=get_max_token_count(tokenizer, conjunction),
+                cross_attention_control_args=options.get("cross_attention_control", None),
+            )
 
         c = c.detach().to("cpu")
 
         conditioning_data = ConditioningFieldData(
             conditionings=[
                 BasicConditioningInfo(
                     embeds=c,
@@ -164,32 +159,34 @@
 
         return CompelOutput(
             conditioning=ConditioningField(
                 conditioning_name=conditioning_name,
             ),
         )
 
+
 class SDXLPromptInvocationBase:
     def run_clip_raw(self, context, clip_field, prompt, get_pooled):
         tokenizer_info = context.services.model_manager.get_model(
-            **clip_field.tokenizer.dict(), context=context,
+            **clip_field.tokenizer.dict(),
+            context=context,
         )
         text_encoder_info = context.services.model_manager.get_model(
-            **clip_field.text_encoder.dict(), context=context,
+            **clip_field.text_encoder.dict(),
+            context=context,
         )
 
         def _lora_loader():
             for lora in clip_field.loras:
-                lora_info = context.services.model_manager.get_model(
-                    **lora.dict(exclude={"weight"}), context=context)
+                lora_info = context.services.model_manager.get_model(**lora.dict(exclude={"weight"}), context=context)
                 yield (lora_info.context.model, lora.weight)
                 del lora_info
             return
 
-        #loras = [(context.services.model_manager.get_model(**lora.dict(exclude={"weight"})).context.model, lora.weight) for lora in self.clip.loras]
+        # loras = [(context.services.model_manager.get_model(**lora.dict(exclude={"weight"})).context.model, lora.weight) for lora in self.clip.loras]
 
         ti_list = []
         for trigger in re.findall(r"<[a-zA-Z0-9., _-]+>", prompt):
             name = trigger[1:-1]
             try:
                 ti_list.append(
                     context.services.model_manager.get_model(
@@ -197,23 +194,26 @@
                         base_model=clip_field.text_encoder.base_model,
                         model_type=ModelType.TextualInversion,
                         context=context,
                     ).context.model
                 )
             except ModelNotFoundException:
                 # print(e)
-                #import traceback
-                #print(traceback.format_exc())
-                print(f"Warn: trigger: \"{trigger}\" not found")
-
-        with ModelPatcher.apply_lora_text_encoder(text_encoder_info.context.model, _lora_loader()),\
-                ModelPatcher.apply_ti(tokenizer_info.context.model, text_encoder_info.context.model, ti_list) as (tokenizer, ti_manager),\
-                ModelPatcher.apply_clip_skip(text_encoder_info.context.model, clip_field.skipped_layers),\
-                text_encoder_info as text_encoder:
-
+                # import traceback
+                # print(traceback.format_exc())
+                print(f'Warn: trigger: "{trigger}" not found')
+
+        with ModelPatcher.apply_lora_text_encoder(
+            text_encoder_info.context.model, _lora_loader()
+        ), ModelPatcher.apply_ti(tokenizer_info.context.model, text_encoder_info.context.model, ti_list) as (
+            tokenizer,
+            ti_manager,
+        ), ModelPatcher.apply_clip_skip(
+            text_encoder_info.context.model, clip_field.skipped_layers
+        ), text_encoder_info as text_encoder:
             text_inputs = tokenizer(
                 prompt,
                 padding="max_length",
                 max_length=tokenizer.model_max_length,
                 truncation=True,
                 return_tensors="pt",
             )
@@ -237,29 +237,30 @@
         if c_pooled is not None:
             c_pooled = c_pooled.detach().to("cpu")
 
         return c, c_pooled, None
 
     def run_clip_compel(self, context, clip_field, prompt, get_pooled):
         tokenizer_info = context.services.model_manager.get_model(
-            **clip_field.tokenizer.dict(), context=context,
+            **clip_field.tokenizer.dict(),
+            context=context,
         )
         text_encoder_info = context.services.model_manager.get_model(
-            **clip_field.text_encoder.dict(), context=context,
+            **clip_field.text_encoder.dict(),
+            context=context,
         )
 
         def _lora_loader():
             for lora in clip_field.loras:
-                lora_info = context.services.model_manager.get_model(
-                    **lora.dict(exclude={"weight"}), context=context)
+                lora_info = context.services.model_manager.get_model(**lora.dict(exclude={"weight"}), context=context)
                 yield (lora_info.context.model, lora.weight)
                 del lora_info
             return
 
-        #loras = [(context.services.model_manager.get_model(**lora.dict(exclude={"weight"})).context.model, lora.weight) for lora in self.clip.loras]
+        # loras = [(context.services.model_manager.get_model(**lora.dict(exclude={"weight"})).context.model, lora.weight) for lora in self.clip.loras]
 
         ti_list = []
         for trigger in re.findall(r"<[a-zA-Z0-9., _-]+>", prompt):
             name = trigger[1:-1]
             try:
                 ti_list.append(
                     context.services.model_manager.get_model(
@@ -267,30 +268,33 @@
                         base_model=clip_field.text_encoder.base_model,
                         model_type=ModelType.TextualInversion,
                         context=context,
                     ).context.model
                 )
             except ModelNotFoundException:
                 # print(e)
-                #import traceback
-                #print(traceback.format_exc())
-                print(f"Warn: trigger: \"{trigger}\" not found")
-
-        with ModelPatcher.apply_lora_text_encoder(text_encoder_info.context.model, _lora_loader()),\
-                ModelPatcher.apply_ti(tokenizer_info.context.model, text_encoder_info.context.model, ti_list) as (tokenizer, ti_manager),\
-                ModelPatcher.apply_clip_skip(text_encoder_info.context.model, clip_field.skipped_layers),\
-                text_encoder_info as text_encoder:
-
+                # import traceback
+                # print(traceback.format_exc())
+                print(f'Warn: trigger: "{trigger}" not found')
+
+        with ModelPatcher.apply_lora_text_encoder(
+            text_encoder_info.context.model, _lora_loader()
+        ), ModelPatcher.apply_ti(tokenizer_info.context.model, text_encoder_info.context.model, ti_list) as (
+            tokenizer,
+            ti_manager,
+        ), ModelPatcher.apply_clip_skip(
+            text_encoder_info.context.model, clip_field.skipped_layers
+        ), text_encoder_info as text_encoder:
             compel = Compel(
                 tokenizer=tokenizer,
                 text_encoder=text_encoder,
                 textual_inversion_manager=ti_manager,
                 dtype_for_device_getter=torch_dtype,
                 truncate_long_prompts=True,  # TODO:
-                returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, # TODO: clip skip
+                returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,  # TODO: clip skip
                 requires_pooled=True,
             )
 
             conjunction = Compel.parse_prompt_string(prompt)
 
             if context.services.configuration.log_tokenization:
                 # TODO: better logging for and syntax
@@ -316,14 +320,15 @@
 
         c = c.detach().to("cpu")
         if c_pooled is not None:
             c_pooled = c_pooled.detach().to("cpu")
 
         return c, c_pooled, ec
 
+
 class SDXLCompelPromptInvocation(BaseInvocation, SDXLPromptInvocationBase):
     """Parse prompt using compel package to conditioning."""
 
     type: Literal["sdxl_compel_prompt"] = "sdxl_compel_prompt"
 
     prompt: str = Field(default="", description="Prompt")
     style: str = Field(default="", description="Style prompt")
@@ -335,38 +340,30 @@
     target_height: int = Field(1024, description="")
     clip: ClipField = Field(None, description="Clip to use")
     clip2: ClipField = Field(None, description="Clip2 to use")
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "SDXL Prompt (Compel)",
-                "tags": ["prompt", "compel"],
-                "type_hints": {
-                    "model": "model"
-                }
-            },
+            "ui": {"title": "SDXL Prompt (Compel)", "tags": ["prompt", "compel"], "type_hints": {"model": "model"}},
         }
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> CompelOutput:
         c1, c1_pooled, ec1 = self.run_clip_compel(context, self.clip, self.prompt, False)
         if self.style.strip() == "":
             c2, c2_pooled, ec2 = self.run_clip_compel(context, self.clip2, self.prompt, True)
         else:
             c2, c2_pooled, ec2 = self.run_clip_compel(context, self.clip2, self.style, True)
 
         original_size = (self.original_height, self.original_width)
         crop_coords = (self.crop_top, self.crop_left)
         target_size = (self.target_height, self.target_width)
 
-        add_time_ids = torch.tensor([
-            original_size + crop_coords + target_size
-        ])
+        add_time_ids = torch.tensor([original_size + crop_coords + target_size])
 
         conditioning_data = ConditioningFieldData(
             conditionings=[
                 SDXLConditioningInfo(
                     embeds=torch.cat([c1, c2], dim=-1),
                     pooled_embeds=c2_pooled,
                     add_time_ids=add_time_ids,
@@ -380,70 +377,68 @@
 
         return CompelOutput(
             conditioning=ConditioningField(
                 conditioning_name=conditioning_name,
             ),
         )
 
+
 class SDXLRefinerCompelPromptInvocation(BaseInvocation, SDXLPromptInvocationBase):
     """Parse prompt using compel package to conditioning."""
 
     type: Literal["sdxl_refiner_compel_prompt"] = "sdxl_refiner_compel_prompt"
 
-    style: str = Field(default="", description="Style prompt") # TODO: ?
+    style: str = Field(default="", description="Style prompt")  # TODO: ?
     original_width: int = Field(1024, description="")
     original_height: int = Field(1024, description="")
     crop_top: int = Field(0, description="")
     crop_left: int = Field(0, description="")
     aesthetic_score: float = Field(6.0, description="")
     clip2: ClipField = Field(None, description="Clip to use")
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "SDXL Refiner Prompt (Compel)",
                 "tags": ["prompt", "compel"],
-                "type_hints": {
-                    "model": "model"
-                }
+                "type_hints": {"model": "model"},
             },
         }
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> CompelOutput:
         c2, c2_pooled, ec2 = self.run_clip_compel(context, self.clip2, self.style, True)
 
         original_size = (self.original_height, self.original_width)
         crop_coords = (self.crop_top, self.crop_left)
 
-        add_time_ids = torch.tensor([
-            original_size + crop_coords + (self.aesthetic_score,)
-        ])
+        add_time_ids = torch.tensor([original_size + crop_coords + (self.aesthetic_score,)])
 
         conditioning_data = ConditioningFieldData(
             conditionings=[
                 SDXLConditioningInfo(
                     embeds=c2,
                     pooled_embeds=c2_pooled,
                     add_time_ids=add_time_ids,
-                    extra_conditioning=ec2, # or None
+                    extra_conditioning=ec2,  # or None
                 )
             ]
         )
 
         conditioning_name = f"{context.graph_execution_state_id}_{self.id}_conditioning"
         context.services.latents.save(conditioning_name, conditioning_data)
 
         return CompelOutput(
             conditioning=ConditioningField(
                 conditioning_name=conditioning_name,
             ),
         )
 
+
 class SDXLRawPromptInvocation(BaseInvocation, SDXLPromptInvocationBase):
     """Pass unmodified prompt to conditioning without compel processing."""
 
     type: Literal["sdxl_raw_prompt"] = "sdxl_raw_prompt"
 
     prompt: str = Field(default="", description="Prompt")
     style: str = Field(default="", description="Style prompt")
@@ -455,38 +450,30 @@
     target_height: int = Field(1024, description="")
     clip: ClipField = Field(None, description="Clip to use")
     clip2: ClipField = Field(None, description="Clip2 to use")
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "SDXL Prompt (Raw)",
-                "tags": ["prompt", "compel"],
-                "type_hints": {
-                    "model": "model"
-                }
-            },
+            "ui": {"title": "SDXL Prompt (Raw)", "tags": ["prompt", "compel"], "type_hints": {"model": "model"}},
         }
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> CompelOutput:
         c1, c1_pooled, ec1 = self.run_clip_raw(context, self.clip, self.prompt, False)
         if self.style.strip() == "":
             c2, c2_pooled, ec2 = self.run_clip_raw(context, self.clip2, self.prompt, True)
         else:
             c2, c2_pooled, ec2 = self.run_clip_raw(context, self.clip2, self.style, True)
 
         original_size = (self.original_height, self.original_width)
         crop_coords = (self.crop_top, self.crop_left)
         target_size = (self.target_height, self.target_width)
 
-        add_time_ids = torch.tensor([
-            original_size + crop_coords + target_size
-        ])
+        add_time_ids = torch.tensor([original_size + crop_coords + target_size])
 
         conditioning_data = ConditioningFieldData(
             conditionings=[
                 SDXLConditioningInfo(
                     embeds=torch.cat([c1, c2], dim=-1),
                     pooled_embeds=c2_pooled,
                     add_time_ids=add_time_ids,
@@ -500,57 +487,54 @@
 
         return CompelOutput(
             conditioning=ConditioningField(
                 conditioning_name=conditioning_name,
             ),
         )
 
+
 class SDXLRefinerRawPromptInvocation(BaseInvocation, SDXLPromptInvocationBase):
     """Parse prompt using compel package to conditioning."""
 
     type: Literal["sdxl_refiner_raw_prompt"] = "sdxl_refiner_raw_prompt"
 
-    style: str = Field(default="", description="Style prompt") # TODO: ?
+    style: str = Field(default="", description="Style prompt")  # TODO: ?
     original_width: int = Field(1024, description="")
     original_height: int = Field(1024, description="")
     crop_top: int = Field(0, description="")
     crop_left: int = Field(0, description="")
     aesthetic_score: float = Field(6.0, description="")
     clip2: ClipField = Field(None, description="Clip to use")
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "SDXL Refiner Prompt (Raw)",
                 "tags": ["prompt", "compel"],
-                "type_hints": {
-                    "model": "model"
-                }
+                "type_hints": {"model": "model"},
             },
         }
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> CompelOutput:
         c2, c2_pooled, ec2 = self.run_clip_raw(context, self.clip2, self.style, True)
 
         original_size = (self.original_height, self.original_width)
         crop_coords = (self.crop_top, self.crop_left)
 
-        add_time_ids = torch.tensor([
-            original_size + crop_coords + (self.aesthetic_score,)
-        ])
+        add_time_ids = torch.tensor([original_size + crop_coords + (self.aesthetic_score,)])
 
         conditioning_data = ConditioningFieldData(
             conditionings=[
                 SDXLConditioningInfo(
                     embeds=c2,
                     pooled_embeds=c2_pooled,
                     add_time_ids=add_time_ids,
-                    extra_conditioning=ec2, # or None
+                    extra_conditioning=ec2,  # or None
                 )
             ]
         )
 
         conditioning_name = f"{context.graph_execution_state_id}_{self.id}_conditioning"
         context.services.latents.save(conditioning_name, conditioning_data)
 
@@ -559,109 +543,81 @@
                 conditioning_name=conditioning_name,
             ),
         )
 
 
 class ClipSkipInvocationOutput(BaseInvocationOutput):
     """Clip skip node output"""
+
     type: Literal["clip_skip_output"] = "clip_skip_output"
     clip: ClipField = Field(None, description="Clip with skipped layers")
 
+
 class ClipSkipInvocation(BaseInvocation):
     """Skip layers in clip text_encoder model."""
+
     type: Literal["clip_skip"] = "clip_skip"
 
     clip: ClipField = Field(None, description="Clip to use")
     skipped_layers: int = Field(0, description="Number of layers to skip in text_encoder")
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "CLIP Skip",
-                "tags": ["clip", "skip"]
-            },
+            "ui": {"title": "CLIP Skip", "tags": ["clip", "skip"]},
         }
 
     def invoke(self, context: InvocationContext) -> ClipSkipInvocationOutput:
         self.clip.skipped_layers += self.skipped_layers
         return ClipSkipInvocationOutput(
             clip=self.clip,
         )
 
 
 def get_max_token_count(
-        tokenizer, prompt: Union[FlattenedPrompt, Blend, Conjunction],
-        truncate_if_too_long=False) -> int:
+    tokenizer, prompt: Union[FlattenedPrompt, Blend, Conjunction], truncate_if_too_long=False
+) -> int:
     if type(prompt) is Blend:
         blend: Blend = prompt
-        return max(
-            [
-                get_max_token_count(tokenizer, p, truncate_if_too_long)
-                for p in blend.prompts
-            ]
-        )
+        return max([get_max_token_count(tokenizer, p, truncate_if_too_long) for p in blend.prompts])
     elif type(prompt) is Conjunction:
         conjunction: Conjunction = prompt
-        return sum(
-            [
-                get_max_token_count(tokenizer, p, truncate_if_too_long)
-                for p in conjunction.prompts
-            ]
-        )
+        return sum([get_max_token_count(tokenizer, p, truncate_if_too_long) for p in conjunction.prompts])
     else:
-        return len(
-            get_tokens_for_prompt_object(
-                tokenizer, prompt, truncate_if_too_long))
+        return len(get_tokens_for_prompt_object(tokenizer, prompt, truncate_if_too_long))
 
 
-def get_tokens_for_prompt_object(
-    tokenizer, parsed_prompt: FlattenedPrompt, truncate_if_too_long=True
-) -> List[str]:
+def get_tokens_for_prompt_object(tokenizer, parsed_prompt: FlattenedPrompt, truncate_if_too_long=True) -> List[str]:
     if type(parsed_prompt) is Blend:
-        raise ValueError(
-            "Blend is not supported here - you need to get tokens for each of its .children"
-        )
+        raise ValueError("Blend is not supported here - you need to get tokens for each of its .children")
 
     text_fragments = [
         x.text
         if type(x) is Fragment
-        else (
-            " ".join([f.text for f in x.original])
-            if type(x) is CrossAttentionControlSubstitute
-            else str(x)
-        )
+        else (" ".join([f.text for f in x.original]) if type(x) is CrossAttentionControlSubstitute else str(x))
         for x in parsed_prompt.children
     ]
     text = " ".join(text_fragments)
     tokens = tokenizer.tokenize(text)
     if truncate_if_too_long:
         max_tokens_length = tokenizer.model_max_length - 2  # typically 75
         tokens = tokens[0:max_tokens_length]
     return tokens
 
 
-def log_tokenization_for_conjunction(
-    c: Conjunction, tokenizer, display_label_prefix=None
-):
+def log_tokenization_for_conjunction(c: Conjunction, tokenizer, display_label_prefix=None):
     display_label_prefix = display_label_prefix or ""
     for i, p in enumerate(c.prompts):
         if len(c.prompts) > 1:
             this_display_label_prefix = f"{display_label_prefix}(conjunction part {i + 1}, weight={c.weights[i]})"
         else:
             this_display_label_prefix = display_label_prefix
-        log_tokenization_for_prompt_object(
-            p,
-            tokenizer,
-            display_label_prefix=this_display_label_prefix
-        )
+        log_tokenization_for_prompt_object(p, tokenizer, display_label_prefix=this_display_label_prefix)
 
 
-def log_tokenization_for_prompt_object(
-    p: Union[Blend, FlattenedPrompt], tokenizer, display_label_prefix=None
-):
+def log_tokenization_for_prompt_object(p: Union[Blend, FlattenedPrompt], tokenizer, display_label_prefix=None):
     display_label_prefix = display_label_prefix or ""
     if type(p) is Blend:
         blend: Blend = p
         for i, c in enumerate(blend.prompts):
             log_tokenization_for_prompt_object(
                 c,
                 tokenizer,
@@ -690,21 +646,18 @@
             log_tokenization_for_text(
                 edited_text,
                 tokenizer,
                 display_label=f"{display_label_prefix}(.swap replacements)",
             )
         else:
             text = " ".join([x.text for x in flattened_prompt.children])
-            log_tokenization_for_text(
-                text, tokenizer, display_label=display_label_prefix
-            )
+            log_tokenization_for_text(text, tokenizer, display_label=display_label_prefix)
 
 
-def log_tokenization_for_text(
-        text, tokenizer, display_label=None, truncate_if_too_long=False):
+def log_tokenization_for_text(text, tokenizer, display_label=None, truncate_if_too_long=False):
     """shows how the prompt is tokenized
     # usually tokens have '</w>' to indicate end-of-word,
     # but for readability it has been replaced with ' '
     """
     tokens = tokenizer.tokenize(text)
     tokenized = ""
     discarded = ""
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/controlnet_image_processors.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/controlnet_image_processors.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,43 +2,51 @@
 # initial implementation by Gregg Helt, 2023
 # heavily leverages controlnet_aux package: https://github.com/patrickvonplaten/controlnet_aux
 from builtins import bool, float
 from typing import Dict, List, Literal, Optional, Union
 
 import cv2
 import numpy as np
-from controlnet_aux import (CannyDetector, ContentShuffleDetector, HEDdetector,
-                            LeresDetector, LineartAnimeDetector,
-                            LineartDetector, MediapipeFaceDetector,
-                            MidasDetector, MLSDdetector, NormalBaeDetector,
-                            OpenposeDetector, PidiNetDetector, SamDetector,
-                            ZoeDetector)
+from controlnet_aux import (
+    CannyDetector,
+    ContentShuffleDetector,
+    HEDdetector,
+    LeresDetector,
+    LineartAnimeDetector,
+    LineartDetector,
+    MediapipeFaceDetector,
+    MidasDetector,
+    MLSDdetector,
+    NormalBaeDetector,
+    OpenposeDetector,
+    PidiNetDetector,
+    SamDetector,
+    ZoeDetector,
+)
 from controlnet_aux.util import HWC3, ade_palette
 from PIL import Image
 from pydantic import BaseModel, Field, validator
 
 from ...backend.model_management import BaseModelType, ModelType
 from ..models.image import ImageCategory, ImageField, ResourceOrigin
-from .baseinvocation import (BaseInvocation, BaseInvocationOutput,
-                             InvocationConfig, InvocationContext)
+from .baseinvocation import BaseInvocation, BaseInvocationOutput, InvocationConfig, InvocationContext
 from ..models.image import ImageOutput, PILInvocationConfig
 
 CONTROLNET_DEFAULT_MODELS = [
     ###########################################
     # lllyasviel sd v1.5, ControlNet v1.0 models
     ##############################################
     "lllyasviel/sd-controlnet-canny",
     "lllyasviel/sd-controlnet-depth",
     "lllyasviel/sd-controlnet-hed",
     "lllyasviel/sd-controlnet-seg",
     "lllyasviel/sd-controlnet-openpose",
     "lllyasviel/sd-controlnet-scribble",
     "lllyasviel/sd-controlnet-normal",
     "lllyasviel/sd-controlnet-mlsd",
-
     #############################################
     # lllyasviel sd v1.5, ControlNet v1.1 models
     #############################################
     "lllyasviel/control_v11p_sd15_canny",
     "lllyasviel/control_v11p_sd15_openpose",
     "lllyasviel/control_v11p_sd15_seg",
     # "lllyasviel/control_v11p_sd15_depth",  # broken
@@ -52,108 +60,110 @@
     "lllyasviel/control_v11p_sd15_inpaint",
     # "lllyasviel/control_v11u_sd15_tile",
     # problem (temporary?) with huffingface "lllyasviel/control_v11u_sd15_tile",
     # so for now replace  "lllyasviel/control_v11f1e_sd15_tile",
     "lllyasviel/control_v11e_sd15_shuffle",
     "lllyasviel/control_v11e_sd15_ip2p",
     "lllyasviel/control_v11f1e_sd15_tile",
-
     #################################################
     #  thibaud sd v2.1 models (ControlNet v1.0? or v1.1?
     ##################################################
     "thibaud/controlnet-sd21-openpose-diffusers",
     "thibaud/controlnet-sd21-canny-diffusers",
     "thibaud/controlnet-sd21-depth-diffusers",
     "thibaud/controlnet-sd21-scribble-diffusers",
     "thibaud/controlnet-sd21-hed-diffusers",
     "thibaud/controlnet-sd21-zoedepth-diffusers",
     "thibaud/controlnet-sd21-color-diffusers",
     "thibaud/controlnet-sd21-openposev2-diffusers",
     "thibaud/controlnet-sd21-lineart-diffusers",
     "thibaud/controlnet-sd21-normalbae-diffusers",
     "thibaud/controlnet-sd21-ade20k-diffusers",
-
     ##############################################
     #  ControlNetMediaPipeface, ControlNet v1.1
     ##############################################
     # ["CrucibleAI/ControlNetMediaPipeFace", "diffusion_sd15"],  # SD 1.5
     #    diffusion_sd15 needs to be passed to from_pretrained() as subfolder arg
     #    hacked t2l to split to model & subfolder if format is "model,subfolder"
     "CrucibleAI/ControlNetMediaPipeFace,diffusion_sd15",  # SD 1.5
     "CrucibleAI/ControlNetMediaPipeFace",  # SD 2.1?
 ]
 
 CONTROLNET_NAME_VALUES = Literal[tuple(CONTROLNET_DEFAULT_MODELS)]
-CONTROLNET_MODE_VALUES = Literal[tuple(
-    ["balanced", "more_prompt", "more_control", "unbalanced"])]
-CONTROLNET_RESIZE_VALUES = Literal[tuple(
-    ["just_resize", "crop_resize", "fill_resize", "just_resize_simple",])]
+CONTROLNET_MODE_VALUES = Literal[tuple(["balanced", "more_prompt", "more_control", "unbalanced"])]
+CONTROLNET_RESIZE_VALUES = Literal[
+    tuple(
+        [
+            "just_resize",
+            "crop_resize",
+            "fill_resize",
+            "just_resize_simple",
+        ]
+    )
+]
 
 
 class ControlNetModelField(BaseModel):
     """ControlNet model field"""
 
     model_name: str = Field(description="Name of the ControlNet model")
     base_model: BaseModelType = Field(description="Base model")
 
 
 class ControlField(BaseModel):
     image: ImageField = Field(default=None, description="The control image")
-    control_model: Optional[ControlNetModelField] = Field(
-        default=None, description="The ControlNet model to use")
+    control_model: Optional[ControlNetModelField] = Field(default=None, description="The ControlNet model to use")
     # control_weight: Optional[float] = Field(default=1, description="weight given to controlnet")
-    control_weight: Union[float, List[float]] = Field(
-        default=1, description="The weight given to the ControlNet")
+    control_weight: Union[float, List[float]] = Field(default=1, description="The weight given to the ControlNet")
     begin_step_percent: float = Field(
-        default=0, ge=0, le=1,
-        description="When the ControlNet is first applied (% of total steps)")
+        default=0, ge=0, le=1, description="When the ControlNet is first applied (% of total steps)"
+    )
     end_step_percent: float = Field(
-        default=1, ge=0, le=1,
-        description="When the ControlNet is last applied (% of total steps)")
-    control_mode: CONTROLNET_MODE_VALUES = Field(
-        default="balanced", description="The control mode to use")
-    resize_mode: CONTROLNET_RESIZE_VALUES = Field(
-        default="just_resize", description="The resize mode to use")
+        default=1, ge=0, le=1, description="When the ControlNet is last applied (% of total steps)"
+    )
+    control_mode: CONTROLNET_MODE_VALUES = Field(default="balanced", description="The control mode to use")
+    resize_mode: CONTROLNET_RESIZE_VALUES = Field(default="just_resize", description="The resize mode to use")
 
     @validator("control_weight")
     def validate_control_weight(cls, v):
         """Validate that all control weights in the valid range"""
         if isinstance(v, list):
             for i in v:
                 if i < -1 or i > 2:
-                    raise ValueError(
-                        'Control weights must be within -1 to 2 range')
+                    raise ValueError("Control weights must be within -1 to 2 range")
         else:
             if v < -1 or v > 2:
-                raise ValueError('Control weights must be within -1 to 2 range')
+                raise ValueError("Control weights must be within -1 to 2 range")
         return v
 
     class Config:
         schema_extra = {
             "required": ["image", "control_model", "control_weight", "begin_step_percent", "end_step_percent"],
             "ui": {
                 "type_hints": {
                     "control_weight": "float",
                     "control_model": "controlnet_model",
                     # "control_weight": "number",
                 }
-            }
+            },
         }
 
 
 class ControlOutput(BaseInvocationOutput):
     """node output for ControlNet info"""
+
     # fmt: off
     type: Literal["control_output"] = "control_output"
     control: ControlField = Field(default=None, description="The control info")
     # fmt: on
 
 
 class ControlNetInvocation(BaseInvocation):
     """Collects ControlNet info to pass to other nodes"""
+
     # fmt: off
     type: Literal["controlnet"] = "controlnet"
     # Inputs
     image: ImageField = Field(default=None, description="The control image")
     control_model: ControlNetModelField = Field(default="lllyasviel/sd-controlnet-canny",
                                                   description="control model used")
     control_weight: Union[float, List[float]] = Field(default=1.0, description="The weight given to the ControlNet")
@@ -172,15 +182,15 @@
                 "tags": ["controlnet", "latents"],
                 "type_hints": {
                     "model": "model",
                     "control": "control",
                     # "cfg_scale": "float",
                     "cfg_scale": "number",
                     "control_weight": "float",
-                }
+                },
             },
         }
 
     def invoke(self, context: InvocationContext) -> ControlOutput:
         return ControlOutput(
             control=ControlField(
                 image=self.image,
@@ -201,18 +211,15 @@
     type: Literal["image_processor"] = "image_processor"
     # Inputs
     image: ImageField = Field(default=None, description="The image to process")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Image Processor",
-                "tags": ["image", "processor"]
-            },
+            "ui": {"title": "Image Processor", "tags": ["image", "processor"]},
         }
 
     def run_processor(self, image):
         # superclass just passes through image without processing
         return image
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
@@ -229,485 +236,460 @@
         #    so for now setting image_type to RESULT instead of INTERMEDIATE so will get saved in gallery
         image_dto = context.services.images.create(
             image=processed_image,
             image_origin=ResourceOrigin.INTERNAL,
             image_category=ImageCategory.CONTROL,
             session_id=context.graph_execution_state_id,
             node_id=self.id,
-            is_intermediate=self.is_intermediate
+            is_intermediate=self.is_intermediate,
         )
 
         """Builds an ImageOutput and its ImageField"""
         processed_image_field = ImageField(image_name=image_dto.image_name)
         return ImageOutput(
             image=processed_image_field,
             # width=processed_image.width,
             width=image_dto.width,
             # height=processed_image.height,
             height=image_dto.height,
             # mode=processed_image.mode,
         )
 
 
-class CannyImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class CannyImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Canny edge detection for ControlNet"""
+
     # fmt: off
     type: Literal["canny_image_processor"] = "canny_image_processor"
     # Input
     low_threshold: int = Field(default=100, ge=0, le=255, description="The low threshold of the Canny pixel gradient (0-255)")
     high_threshold: int = Field(default=200, ge=0, le=255, description="The high threshold of the Canny pixel gradient (0-255)")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Canny Processor",
-                "tags": ["controlnet", "canny", "image", "processor"]
-            },
+            "ui": {"title": "Canny Processor", "tags": ["controlnet", "canny", "image", "processor"]},
         }
 
     def run_processor(self, image):
         canny_processor = CannyDetector()
-        processed_image = canny_processor(
-            image, self.low_threshold, self.high_threshold)
+        processed_image = canny_processor(image, self.low_threshold, self.high_threshold)
         return processed_image
 
 
-class HedImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class HedImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies HED edge detection to image"""
+
     # fmt: off
     type: Literal["hed_image_processor"] = "hed_image_processor"
     # Inputs
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     # safe not supported in controlnet_aux v0.0.3
     # safe: bool = Field(default=False, description="whether to use safe mode")
     scribble: bool = Field(default=False, description="Whether to use scribble mode")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Softedge(HED) Processor",
-                "tags": ["controlnet", "softedge", "hed", "image", "processor"]
-            },
+            "ui": {"title": "Softedge(HED) Processor", "tags": ["controlnet", "softedge", "hed", "image", "processor"]},
         }
 
     def run_processor(self, image):
         hed_processor = HEDdetector.from_pretrained("lllyasviel/Annotators")
-        processed_image = hed_processor(image,
-                                        detect_resolution=self.detect_resolution,
-                                        image_resolution=self.image_resolution,
-                                        # safe not supported in controlnet_aux v0.0.3
-                                        # safe=self.safe,
-                                        scribble=self.scribble,
-                                        )
+        processed_image = hed_processor(
+            image,
+            detect_resolution=self.detect_resolution,
+            image_resolution=self.image_resolution,
+            # safe not supported in controlnet_aux v0.0.3
+            # safe=self.safe,
+            scribble=self.scribble,
+        )
         return processed_image
 
 
-class LineartImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class LineartImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies line art processing to image"""
+
     # fmt: off
     type: Literal["lineart_image_processor"] = "lineart_image_processor"
     # Inputs
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     coarse: bool = Field(default=False, description="Whether to use coarse mode")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Lineart Processor",
-                "tags": ["controlnet", "lineart", "image", "processor"]
-            },
+            "ui": {"title": "Lineart Processor", "tags": ["controlnet", "lineart", "image", "processor"]},
         }
 
     def run_processor(self, image):
-        lineart_processor = LineartDetector.from_pretrained(
-            "lllyasviel/Annotators")
+        lineart_processor = LineartDetector.from_pretrained("lllyasviel/Annotators")
         processed_image = lineart_processor(
-            image, detect_resolution=self.detect_resolution,
-            image_resolution=self.image_resolution, coarse=self.coarse)
+            image, detect_resolution=self.detect_resolution, image_resolution=self.image_resolution, coarse=self.coarse
+        )
         return processed_image
 
 
-class LineartAnimeImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class LineartAnimeImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies line art anime processing to image"""
+
     # fmt: off
     type: Literal["lineart_anime_image_processor"] = "lineart_anime_image_processor"
     # Inputs
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "Lineart Anime Processor",
-                "tags": ["controlnet", "lineart", "anime", "image", "processor"]
+                "tags": ["controlnet", "lineart", "anime", "image", "processor"],
             },
         }
 
     def run_processor(self, image):
-        processor = LineartAnimeDetector.from_pretrained(
-            "lllyasviel/Annotators")
-        processed_image = processor(image,
-                                    detect_resolution=self.detect_resolution,
-                                    image_resolution=self.image_resolution,
-                                    )
+        processor = LineartAnimeDetector.from_pretrained("lllyasviel/Annotators")
+        processed_image = processor(
+            image,
+            detect_resolution=self.detect_resolution,
+            image_resolution=self.image_resolution,
+        )
         return processed_image
 
 
-class OpenposeImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class OpenposeImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies Openpose processing to image"""
+
     # fmt: off
     type: Literal["openpose_image_processor"] = "openpose_image_processor"
     # Inputs
     hand_and_face: bool = Field(default=False, description="Whether to use hands and face mode")
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Openpose Processor",
-                "tags": ["controlnet", "openpose", "image", "processor"]
-            },
+            "ui": {"title": "Openpose Processor", "tags": ["controlnet", "openpose", "image", "processor"]},
         }
 
     def run_processor(self, image):
-        openpose_processor = OpenposeDetector.from_pretrained(
-            "lllyasviel/Annotators")
+        openpose_processor = OpenposeDetector.from_pretrained("lllyasviel/Annotators")
         processed_image = openpose_processor(
-            image, detect_resolution=self.detect_resolution,
+            image,
+            detect_resolution=self.detect_resolution,
             image_resolution=self.image_resolution,
-            hand_and_face=self.hand_and_face,)
+            hand_and_face=self.hand_and_face,
+        )
         return processed_image
 
 
-class MidasDepthImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class MidasDepthImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies Midas depth processing to image"""
+
     # fmt: off
     type: Literal["midas_depth_image_processor"] = "midas_depth_image_processor"
     # Inputs
     a_mult: float = Field(default=2.0, ge=0, description="Midas parameter `a_mult` (a = a_mult * PI)")
     bg_th: float = Field(default=0.1, ge=0, description="Midas parameter `bg_th`")
     # depth_and_normal not supported in controlnet_aux v0.0.3
     # depth_and_normal: bool = Field(default=False, description="whether to use depth and normal mode")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Midas (Depth) Processor",
-                "tags": ["controlnet", "midas", "depth", "image", "processor"]
-            },
+            "ui": {"title": "Midas (Depth) Processor", "tags": ["controlnet", "midas", "depth", "image", "processor"]},
         }
 
     def run_processor(self, image):
         midas_processor = MidasDetector.from_pretrained("lllyasviel/Annotators")
-        processed_image = midas_processor(image,
-                                          a=np.pi * self.a_mult,
-                                          bg_th=self.bg_th,
-                                          # dept_and_normal not supported in controlnet_aux v0.0.3
-                                          # depth_and_normal=self.depth_and_normal,
-                                          )
+        processed_image = midas_processor(
+            image,
+            a=np.pi * self.a_mult,
+            bg_th=self.bg_th,
+            # dept_and_normal not supported in controlnet_aux v0.0.3
+            # depth_and_normal=self.depth_and_normal,
+        )
         return processed_image
 
 
-class NormalbaeImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class NormalbaeImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies NormalBae processing to image"""
+
     # fmt: off
     type: Literal["normalbae_image_processor"] = "normalbae_image_processor"
     # Inputs
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Normal BAE Processor",
-                "tags": ["controlnet", "normal", "bae", "image", "processor"]
-            },
+            "ui": {"title": "Normal BAE Processor", "tags": ["controlnet", "normal", "bae", "image", "processor"]},
         }
 
     def run_processor(self, image):
-        normalbae_processor = NormalBaeDetector.from_pretrained(
-            "lllyasviel/Annotators")
+        normalbae_processor = NormalBaeDetector.from_pretrained("lllyasviel/Annotators")
         processed_image = normalbae_processor(
-            image, detect_resolution=self.detect_resolution,
-            image_resolution=self.image_resolution)
+            image, detect_resolution=self.detect_resolution, image_resolution=self.image_resolution
+        )
         return processed_image
 
 
-class MlsdImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class MlsdImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies MLSD processing to image"""
+
     # fmt: off
     type: Literal["mlsd_image_processor"] = "mlsd_image_processor"
     # Inputs
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     thr_v: float = Field(default=0.1, ge=0, description="MLSD parameter `thr_v`")
     thr_d: float = Field(default=0.1, ge=0, description="MLSD parameter `thr_d`")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "MLSD Processor",
-                "tags": ["controlnet", "mlsd", "image", "processor"]
-            },
+            "ui": {"title": "MLSD Processor", "tags": ["controlnet", "mlsd", "image", "processor"]},
         }
 
     def run_processor(self, image):
         mlsd_processor = MLSDdetector.from_pretrained("lllyasviel/Annotators")
         processed_image = mlsd_processor(
-            image, detect_resolution=self.detect_resolution,
-            image_resolution=self.image_resolution, thr_v=self.thr_v,
-            thr_d=self.thr_d)
+            image,
+            detect_resolution=self.detect_resolution,
+            image_resolution=self.image_resolution,
+            thr_v=self.thr_v,
+            thr_d=self.thr_d,
+        )
         return processed_image
 
 
-class PidiImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class PidiImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies PIDI processing to image"""
+
     # fmt: off
     type: Literal["pidi_image_processor"] = "pidi_image_processor"
     # Inputs
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     safe: bool = Field(default=False, description="Whether to use safe mode")
     scribble: bool = Field(default=False, description="Whether to use scribble mode")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "PIDI Processor",
-                "tags": ["controlnet", "pidi", "image", "processor"]
-            },
+            "ui": {"title": "PIDI Processor", "tags": ["controlnet", "pidi", "image", "processor"]},
         }
 
     def run_processor(self, image):
-        pidi_processor = PidiNetDetector.from_pretrained(
-            "lllyasviel/Annotators")
+        pidi_processor = PidiNetDetector.from_pretrained("lllyasviel/Annotators")
         processed_image = pidi_processor(
-            image, detect_resolution=self.detect_resolution,
-            image_resolution=self.image_resolution, safe=self.safe,
-            scribble=self.scribble)
+            image,
+            detect_resolution=self.detect_resolution,
+            image_resolution=self.image_resolution,
+            safe=self.safe,
+            scribble=self.scribble,
+        )
         return processed_image
 
 
-class ContentShuffleImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class ContentShuffleImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies content shuffle processing to image"""
+
     # fmt: off
     type: Literal["content_shuffle_image_processor"] = "content_shuffle_image_processor"
     # Inputs
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     h: Optional[int] = Field(default=512, ge=0, description="Content shuffle `h` parameter")
     w: Optional[int] = Field(default=512, ge=0, description="Content shuffle `w` parameter")
     f: Optional[int] = Field(default=256, ge=0, description="Content shuffle `f` parameter")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "Content Shuffle Processor",
-                "tags": ["controlnet", "contentshuffle", "image", "processor"]
+                "tags": ["controlnet", "contentshuffle", "image", "processor"],
             },
         }
 
     def run_processor(self, image):
         content_shuffle_processor = ContentShuffleDetector()
-        processed_image = content_shuffle_processor(image,
-                                                    detect_resolution=self.detect_resolution,
-                                                    image_resolution=self.image_resolution,
-                                                    h=self.h,
-                                                    w=self.w,
-                                                    f=self.f
-                                                    )
+        processed_image = content_shuffle_processor(
+            image,
+            detect_resolution=self.detect_resolution,
+            image_resolution=self.image_resolution,
+            h=self.h,
+            w=self.w,
+            f=self.f,
+        )
         return processed_image
 
 
 # should work with controlnet_aux >= 0.0.4 and timm <= 0.6.13
-class ZoeDepthImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class ZoeDepthImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies Zoe depth processing to image"""
+
     # fmt: off
     type: Literal["zoe_depth_image_processor"] = "zoe_depth_image_processor"
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Zoe (Depth) Processor",
-                "tags": ["controlnet", "zoe", "depth", "image", "processor"]
-            },
+            "ui": {"title": "Zoe (Depth) Processor", "tags": ["controlnet", "zoe", "depth", "image", "processor"]},
         }
 
     def run_processor(self, image):
-        zoe_depth_processor = ZoeDetector.from_pretrained(
-            "lllyasviel/Annotators")
+        zoe_depth_processor = ZoeDetector.from_pretrained("lllyasviel/Annotators")
         processed_image = zoe_depth_processor(image)
         return processed_image
 
 
-class MediapipeFaceProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class MediapipeFaceProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies mediapipe face processing to image"""
+
     # fmt: off
     type: Literal["mediapipe_face_processor"] = "mediapipe_face_processor"
     # Inputs
     max_faces: int = Field(default=1, ge=1, description="Maximum number of faces to detect")
     min_confidence: float = Field(default=0.5, ge=0, le=1, description="Minimum confidence for face detection")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Mediapipe Processor",
-                "tags": ["controlnet", "mediapipe", "image", "processor"]
-            },
+            "ui": {"title": "Mediapipe Processor", "tags": ["controlnet", "mediapipe", "image", "processor"]},
         }
 
     def run_processor(self, image):
         # MediaPipeFaceDetector throws an error if image has alpha channel
         #     so convert to RGB if needed
-        if image.mode == 'RGBA':
-            image = image.convert('RGB')
+        if image.mode == "RGBA":
+            image = image.convert("RGB")
         mediapipe_face_processor = MediapipeFaceDetector()
-        processed_image = mediapipe_face_processor(
-            image, max_faces=self.max_faces, min_confidence=self.min_confidence)
+        processed_image = mediapipe_face_processor(image, max_faces=self.max_faces, min_confidence=self.min_confidence)
         return processed_image
 
 
-class LeresImageProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class LeresImageProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies leres processing to image"""
+
     # fmt: off
     type: Literal["leres_image_processor"] = "leres_image_processor"
     # Inputs
     thr_a: float = Field(default=0, description="Leres parameter `thr_a`")
     thr_b: float = Field(default=0, description="Leres parameter `thr_b`")
     boost: bool = Field(default=False, description="Whether to use boost mode")
     detect_resolution: int = Field(default=512, ge=0, description="The pixel resolution for detection")
     image_resolution: int = Field(default=512, ge=0, description="The pixel resolution for the output image")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Leres (Depth) Processor",
-                "tags": ["controlnet", "leres", "depth", "image", "processor"]
-            },
+            "ui": {"title": "Leres (Depth) Processor", "tags": ["controlnet", "leres", "depth", "image", "processor"]},
         }
 
     def run_processor(self, image):
         leres_processor = LeresDetector.from_pretrained("lllyasviel/Annotators")
         processed_image = leres_processor(
-            image, thr_a=self.thr_a, thr_b=self.thr_b, boost=self.boost,
+            image,
+            thr_a=self.thr_a,
+            thr_b=self.thr_b,
+            boost=self.boost,
             detect_resolution=self.detect_resolution,
-            image_resolution=self.image_resolution)
+            image_resolution=self.image_resolution,
+        )
         return processed_image
 
 
-class TileResamplerProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
-
+class TileResamplerProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     # fmt: off
     type: Literal["tile_image_processor"] = "tile_image_processor"
     # Inputs
     #res: int = Field(default=512, ge=0, le=1024, description="The pixel resolution for each tile")
     down_sampling_rate: float = Field(default=1.0, ge=1.0, le=8.0, description="Down sampling rate")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "Tile Resample Processor",
-                "tags": ["controlnet", "tile", "resample", "image", "processor"]
+                "tags": ["controlnet", "tile", "resample", "image", "processor"],
             },
         }
 
     # tile_resample copied from sd-webui-controlnet/scripts/processor.py
-    def tile_resample(self,
-                      np_img: np.ndarray,
-                      res=512,   # never used?
-                      down_sampling_rate=1.0,
-                      ):
+    def tile_resample(
+        self,
+        np_img: np.ndarray,
+        res=512,  # never used?
+        down_sampling_rate=1.0,
+    ):
         np_img = HWC3(np_img)
         if down_sampling_rate < 1.1:
             return np_img
         H, W, C = np_img.shape
         H = int(float(H) / float(down_sampling_rate))
         W = int(float(W) / float(down_sampling_rate))
         np_img = cv2.resize(np_img, (W, H), interpolation=cv2.INTER_AREA)
         return np_img
 
     def run_processor(self, img):
         np_img = np.array(img, dtype=np.uint8)
-        processed_np_image = self.tile_resample(np_img,
-                                                # res=self.tile_size,
-                                                down_sampling_rate=self.down_sampling_rate
-                                                )
+        processed_np_image = self.tile_resample(
+            np_img,
+            # res=self.tile_size,
+            down_sampling_rate=self.down_sampling_rate,
+        )
         processed_image = Image.fromarray(processed_np_image)
         return processed_image
 
 
-class SegmentAnythingProcessorInvocation(
-        ImageProcessorInvocation, PILInvocationConfig):
+class SegmentAnythingProcessorInvocation(ImageProcessorInvocation, PILInvocationConfig):
     """Applies segment anything processing to image"""
+
     # fmt: off
     type: Literal["segment_anything_processor"] = "segment_anything_processor"
     # fmt: on
 
     class Config(InvocationConfig):
-        schema_extra = {"ui": {"title": "Segment Anything Processor", "tags": [
-            "controlnet", "segment", "anything", "sam", "image", "processor"]}, }
+        schema_extra = {
+            "ui": {
+                "title": "Segment Anything Processor",
+                "tags": ["controlnet", "segment", "anything", "sam", "image", "processor"],
+            },
+        }
 
     def run_processor(self, image):
         # segment_anything_processor = SamDetector.from_pretrained("ybelkada/segment-anything", subfolder="checkpoints")
         segment_anything_processor = SamDetectorReproducibleColors.from_pretrained(
-            "ybelkada/segment-anything", subfolder="checkpoints")
+            "ybelkada/segment-anything", subfolder="checkpoints"
+        )
         np_img = np.array(image, dtype=np.uint8)
         processed_image = segment_anything_processor(np_img)
         return processed_image
 
 
 class SamDetectorReproducibleColors(SamDetector):
-
     # overriding SamDetector.show_anns() method to use reproducible colors for segmentation image
     #     base class show_anns() method randomizes colors,
     #     which seems to also lead to non-reproducible image generation
     # so using ADE20k color palette instead
     def show_anns(self, anns: List[Dict]):
         if len(anns) == 0:
             return
-        sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
-        h, w = anns[0]['segmentation'].shape
-        final_img = Image.fromarray(
-            np.zeros((h, w, 3), dtype=np.uint8), mode="RGB")
+        sorted_anns = sorted(anns, key=(lambda x: x["area"]), reverse=True)
+        h, w = anns[0]["segmentation"].shape
+        final_img = Image.fromarray(np.zeros((h, w, 3), dtype=np.uint8), mode="RGB")
         palette = ade_palette()
         for i, ann in enumerate(sorted_anns):
-            m = ann['segmentation']
+            m = ann["segmentation"]
             img = np.empty((m.shape[0], m.shape[1], 3), dtype=np.uint8)
             # doing modulo just in case number of annotated regions exceeds number of colors in palette
             ann_color = palette[i % len(palette)]
             img[:, :] = ann_color
-            final_img.paste(
-                Image.fromarray(img, mode="RGB"),
-                (0, 0),
-                Image.fromarray(np.uint8(m * 255)))
+            final_img.paste(Image.fromarray(img, mode="RGB"), (0, 0), Image.fromarray(np.uint8(m * 255)))
         return np.array(final_img, dtype=np.uint8)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/cv.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/cv.py`

 * *Files 3% similar despite different names*

```diff
@@ -33,18 +33,15 @@
     # Inputs
     image: ImageField = Field(default=None, description="The image to inpaint")
     mask: ImageField = Field(default=None, description="The mask to use when inpainting")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "OpenCV Inpaint",
-                "tags": ["opencv", "inpaint"]
-            },
+            "ui": {"title": "OpenCV Inpaint", "tags": ["opencv", "inpaint"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
         mask = context.services.images.get_pil_image(self.mask.image_name)
 
         # Convert to cv image/mask
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/generate.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/generate.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,16 +2,15 @@
 
 from functools import partial
 from typing import Literal, Optional, get_args
 
 import torch
 from pydantic import Field
 
-from invokeai.app.models.image import (ColorField, ImageCategory, ImageField,
-                                       ResourceOrigin)
+from invokeai.app.models.image import ColorField, ImageCategory, ImageField, ResourceOrigin
 from invokeai.app.util.misc import SEED_MAX, get_random_seed
 from invokeai.backend.generator.inpaint import infill_methods
 
 from ...backend.generator import Inpaint, InvokeAIGenerator
 from ...backend.stable_diffusion import PipelineIntermediateState
 from ..util.step_callback import stable_diffusion_step_callback
 from .baseinvocation import BaseInvocation, InvocationConfig, InvocationContext
@@ -21,33 +20,33 @@
 from ...backend.stable_diffusion.diffusers_pipeline import StableDiffusionGeneratorPipeline
 from .model import UNetField, VaeField
 from .compel import ConditioningField
 from contextlib import contextmanager, ExitStack, ContextDecorator
 
 SAMPLER_NAME_VALUES = Literal[tuple(InvokeAIGenerator.schedulers())]
 INFILL_METHODS = Literal[tuple(infill_methods())]
-DEFAULT_INFILL_METHOD = (
-    "patchmatch" if "patchmatch" in get_args(INFILL_METHODS) else "tile"
-)
+DEFAULT_INFILL_METHOD = "patchmatch" if "patchmatch" in get_args(INFILL_METHODS) else "tile"
 
 
 from .latent import get_scheduler
 
+
 class OldModelContext(ContextDecorator):
     model: StableDiffusionGeneratorPipeline
 
     def __init__(self, model):
         self.model = model
 
     def __enter__(self):
         return self.model
 
     def __exit__(self, *exc):
         return False
 
+
 class OldModelInfo:
     name: str
     hash: str
     context: OldModelContext
 
     def __init__(self, name: str, hash: str, model: StableDiffusionGeneratorPipeline):
         self.name = name
@@ -60,48 +59,54 @@
 class InpaintInvocation(BaseInvocation):
     """Generates an image using inpaint."""
 
     type: Literal["inpaint"] = "inpaint"
 
     positive_conditioning: Optional[ConditioningField] = Field(description="Positive conditioning for generation")
     negative_conditioning: Optional[ConditioningField] = Field(description="Negative conditioning for generation")
-    seed:        int = Field(ge=0, le=SEED_MAX, description="The seed to use (omit for random)", default_factory=get_random_seed)
-    steps:       int = Field(default=30, gt=0, description="The number of steps to use to generate the image")
-    width:       int = Field(default=512, multiple_of=8, gt=0, description="The width of the resulting image", )
-    height:      int = Field(default=512, multiple_of=8, gt=0, description="The height of the resulting image", )
-    cfg_scale: float = Field(default=7.5, ge=1, description="The Classifier-Free Guidance, higher values may result in a result closer to the prompt", )
-    scheduler: SAMPLER_NAME_VALUES = Field(default="euler", description="The scheduler to use" )
+    seed: int = Field(
+        ge=0, le=SEED_MAX, description="The seed to use (omit for random)", default_factory=get_random_seed
+    )
+    steps: int = Field(default=30, gt=0, description="The number of steps to use to generate the image")
+    width: int = Field(
+        default=512,
+        multiple_of=8,
+        gt=0,
+        description="The width of the resulting image",
+    )
+    height: int = Field(
+        default=512,
+        multiple_of=8,
+        gt=0,
+        description="The height of the resulting image",
+    )
+    cfg_scale: float = Field(
+        default=7.5,
+        ge=1,
+        description="The Classifier-Free Guidance, higher values may result in a result closer to the prompt",
+    )
+    scheduler: SAMPLER_NAME_VALUES = Field(default="euler", description="The scheduler to use")
     unet: UNetField = Field(default=None, description="UNet model")
     vae: VaeField = Field(default=None, description="Vae model")
 
     # Inputs
     image: Optional[ImageField] = Field(description="The input image")
-    strength: float = Field(
-        default=0.75, gt=0, le=1, description="The strength of the original image"
-    )
+    strength: float = Field(default=0.75, gt=0, le=1, description="The strength of the original image")
     fit: bool = Field(
         default=True,
         description="Whether or not the result should be fit to the aspect ratio of the input image",
     )
 
     # Inputs
     mask: Optional[ImageField] = Field(description="The mask")
     seam_size: int = Field(default=96, ge=1, description="The seam inpaint size (px)")
-    seam_blur: int = Field(
-        default=16, ge=0, description="The seam inpaint blur radius (px)"
-    )
-    seam_strength: float = Field(
-        default=0.75, gt=0, le=1, description="The seam inpaint strength"
-    )
-    seam_steps: int = Field(
-        default=30, ge=1, description="The number of steps to use for seam inpaint"
-    )
-    tile_size: int = Field(
-        default=32, ge=1, description="The tile infill method size (px)"
-    )
+    seam_blur: int = Field(default=16, ge=0, description="The seam inpaint blur radius (px)")
+    seam_strength: float = Field(default=0.75, gt=0, le=1, description="The seam inpaint strength")
+    seam_steps: int = Field(default=30, ge=1, description="The number of steps to use for seam inpaint")
+    tile_size: int = Field(default=32, ge=1, description="The tile infill method size (px)")
     infill_method: INFILL_METHODS = Field(
         default=DEFAULT_INFILL_METHOD,
         description="The method used to infill empty regions (px)",
     )
     inpaint_width: Optional[int] = Field(
         default=None,
         multiple_of=8,
@@ -124,18 +129,15 @@
         le=1.0,
         description="The amount by which to replace masked areas with latent noise",
     )
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "tags": ["stable-diffusion", "image"],
-                "title": "Inpaint"
-            },
+            "ui": {"tags": ["stable-diffusion", "image"], "title": "Inpaint"},
         }
 
     def dispatch_progress(
         self,
         context: InvocationContext,
         source_node_id: str,
         intermediate_state: PipelineIntermediateState,
@@ -158,26 +160,31 @@
         return (uc, c, extra_conditioning_info)
 
     @contextmanager
     def load_model_old_way(self, context, scheduler):
         def _lora_loader():
             for lora in self.unet.loras:
                 lora_info = context.services.model_manager.get_model(
-                    **lora.dict(exclude={"weight"}), context=context,)
+                    **lora.dict(exclude={"weight"}),
+                    context=context,
+                )
                 yield (lora_info.context.model, lora.weight)
                 del lora_info
             return
-        
-        unet_info = context.services.model_manager.get_model(**self.unet.unet.dict(), context=context,)
-        vae_info = context.services.model_manager.get_model(**self.vae.vae.dict(), context=context,)
-
-        with vae_info as vae,\
-                ModelPatcher.apply_lora_unet(unet_info.context.model, _lora_loader()),\
-                unet_info as unet:
 
+        unet_info = context.services.model_manager.get_model(
+            **self.unet.unet.dict(),
+            context=context,
+        )
+        vae_info = context.services.model_manager.get_model(
+            **self.vae.vae.dict(),
+            context=context,
+        )
+
+        with vae_info as vae, ModelPatcher.apply_lora_unet(unet_info.context.model, _lora_loader()), unet_info as unet:
             device = context.services.model_manager.mgr.cache.execution_device
             dtype = context.services.model_manager.mgr.cache.precision
 
             pipeline = StableDiffusionGeneratorPipeline(
                 vae=vae,
                 text_encoder=None,
                 tokenizer=None,
@@ -193,29 +200,19 @@
             yield OldModelInfo(
                 name=self.unet.unet.model_name,
                 hash="<NO-HASH>",
                 model=pipeline,
             )
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
-        image = (
-            None
-            if self.image is None
-            else context.services.images.get_pil_image(self.image.image_name)
-        )
-        mask = (
-            None
-            if self.mask is None
-            else context.services.images.get_pil_image(self.mask.image_name)
-        )
+        image = None if self.image is None else context.services.images.get_pil_image(self.image.image_name)
+        mask = None if self.mask is None else context.services.images.get_pil_image(self.mask.image_name)
 
         # Get the source node id (we are invoking the prepared node)
-        graph_execution_state = context.services.graph_execution_manager.get(
-            context.graph_execution_state_id
-        )
+        graph_execution_state = context.services.graph_execution_manager.get(context.graph_execution_state_id)
         source_node_id = graph_execution_state.prepared_source_mapping[self.id]
 
         scheduler = get_scheduler(
             context=context,
             scheduler_info=self.unet.scheduler,
             scheduler_name=self.scheduler,
         )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/image.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/image.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,43 +5,45 @@
 import numpy
 from PIL import Image, ImageFilter, ImageOps, ImageChops
 from pydantic import Field
 from pathlib import Path
 from typing import Union
 from invokeai.app.invocations.metadata import CoreMetadata
 from ..models.image import (
-    ImageCategory, ImageField, ResourceOrigin,
-    PILInvocationConfig, ImageOutput, MaskOutput,
-)    
+    ImageCategory,
+    ImageField,
+    ResourceOrigin,
+    PILInvocationConfig,
+    ImageOutput,
+    MaskOutput,
+)
 from .baseinvocation import (
     BaseInvocation,
     InvocationContext,
     InvocationConfig,
 )
 from invokeai.backend.image_util.safety_checker import SafetyChecker
 from invokeai.backend.image_util.invisible_watermark import InvisibleWatermark
 
+
 class LoadImageInvocation(BaseInvocation):
     """Load an image and provide it as output."""
 
     # fmt: off
     type: Literal["load_image"] = "load_image"
 
     # Inputs
     image: Optional[ImageField] = Field(
         default=None, description="The image to load"
     )
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Load Image",
-                "tags": ["image", "load"]
-            },
+            "ui": {"title": "Load Image", "tags": ["image", "load"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         return ImageOutput(
             image=ImageField(image_name=self.image.image_name),
@@ -52,24 +54,19 @@
 
 class ShowImageInvocation(BaseInvocation):
     """Displays a provided image, and passes it forward in the pipeline."""
 
     type: Literal["show_image"] = "show_image"
 
     # Inputs
-    image: Optional[ImageField] = Field(
-        default=None, description="The image to show"
-    )
+    image: Optional[ImageField] = Field(default=None, description="The image to show")
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Show Image",
-                "tags": ["image", "show"]
-            },
+            "ui": {"title": "Show Image", "tags": ["image", "show"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
         if image:
             image.show()
 
@@ -94,26 +91,21 @@
     y:      int = Field(default=0, description="The top y coordinate of the crop rectangle")
     width:  int = Field(default=512, gt=0, description="The width of the crop rectangle")
     height: int = Field(default=512, gt=0, description="The height of the crop rectangle")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Crop Image",
-                "tags": ["image", "crop"]
-            },
+            "ui": {"title": "Crop Image", "tags": ["image", "crop"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
-        image_crop = Image.new(
-            mode="RGBA", size=(self.width, self.height), color=(0, 0, 0, 0)
-        )
+        image_crop = Image.new(mode="RGBA", size=(self.width, self.height), color=(0, 0, 0, 0))
         image_crop.paste(image, (-self.x, -self.y))
 
         image_dto = context.services.images.create(
             image=image_crop,
             image_origin=ResourceOrigin.INTERNAL,
             image_category=ImageCategory.GENERAL,
             node_id=self.id,
@@ -140,40 +132,31 @@
     mask: Optional[ImageField] = Field(default=None, description="The mask to use when pasting")
     x:                     int = Field(default=0, description="The left x coordinate at which to paste the image")
     y:                     int = Field(default=0, description="The top y coordinate at which to paste the image")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Paste Image",
-                "tags": ["image", "paste"]
-            },
+            "ui": {"title": "Paste Image", "tags": ["image", "paste"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         base_image = context.services.images.get_pil_image(self.base_image.image_name)
         image = context.services.images.get_pil_image(self.image.image_name)
         mask = (
-            None
-            if self.mask is None
-            else ImageOps.invert(
-                context.services.images.get_pil_image(self.mask.image_name)
-            )
+            None if self.mask is None else ImageOps.invert(context.services.images.get_pil_image(self.mask.image_name))
         )
         # TODO: probably shouldn't invert mask here... should user be required to do it?
 
         min_x = min(0, self.x)
         min_y = min(0, self.y)
         max_x = max(base_image.width, image.width + self.x)
         max_y = max(base_image.height, image.height + self.y)
 
-        new_image = Image.new(
-            mode="RGBA", size=(max_x - min_x, max_y - min_y), color=(0, 0, 0, 0)
-        )
+        new_image = Image.new(mode="RGBA", size=(max_x - min_x, max_y - min_y), color=(0, 0, 0, 0))
         new_image.paste(base_image, (abs(min_x), abs(min_y)))
         new_image.paste(image, (max(0, self.x), max(0, self.y)), mask=mask)
 
         image_dto = context.services.images.create(
             image=new_image,
             image_origin=ResourceOrigin.INTERNAL,
             image_category=ImageCategory.GENERAL,
@@ -198,18 +181,15 @@
     # Inputs
     image: Optional[ImageField]  = Field(default=None, description="The image to create the mask from")
     invert:      bool = Field(default=False, description="Whether or not to invert the mask")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Mask From Alpha",
-                "tags": ["image", "mask", "alpha"]
-            },
+            "ui": {"title": "Mask From Alpha", "tags": ["image", "mask", "alpha"]},
         }
 
     def invoke(self, context: InvocationContext) -> MaskOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         image_mask = image.split()[-1]
         if self.invert:
@@ -240,18 +220,15 @@
     # Inputs
     image1: Optional[ImageField]  = Field(default=None, description="The first image to multiply")
     image2: Optional[ImageField]  = Field(default=None, description="The second image to multiply")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Multiply Images",
-                "tags": ["image", "multiply"]
-            },
+            "ui": {"title": "Multiply Images", "tags": ["image", "multiply"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image1 = context.services.images.get_pil_image(self.image1.image_name)
         image2 = context.services.images.get_pil_image(self.image2.image_name)
 
         multiply_image = ImageChops.multiply(image1, image2)
@@ -284,18 +261,15 @@
     # Inputs
     image: Optional[ImageField]  = Field(default=None, description="The image to get the channel from")
     channel: IMAGE_CHANNELS  = Field(default="A", description="The channel to get")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Image Channel",
-                "tags": ["image", "channel"]
-            },
+            "ui": {"title": "Image Channel", "tags": ["image", "channel"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         channel_image = image.getchannel(self.channel)
 
@@ -327,18 +301,15 @@
     # Inputs
     image: Optional[ImageField]  = Field(default=None, description="The image to convert")
     mode: IMAGE_MODES  = Field(default="L", description="The mode to convert to")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Convert Image",
-                "tags": ["image", "convert"]
-            },
+            "ui": {"title": "Convert Image", "tags": ["image", "convert"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         converted_image = image.convert(self.mode)
 
@@ -353,41 +324,37 @@
 
         return ImageOutput(
             image=ImageField(image_name=image_dto.image_name),
             width=image_dto.width,
             height=image_dto.height,
         )
 
+
 class ImageBlurInvocation(BaseInvocation, PILInvocationConfig):
     """Blurs an image"""
 
     # fmt: off
     type: Literal["img_blur"] = "img_blur"
 
     # Inputs
     image: Optional[ImageField]  = Field(default=None, description="The image to blur")
     radius:     float = Field(default=8.0, ge=0, description="The blur radius")
     blur_type: Literal["gaussian", "box"] = Field(default="gaussian", description="The type of blur")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Blur Image",
-                "tags": ["image", "blur"]
-            },
+            "ui": {"title": "Blur Image", "tags": ["image", "blur"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         blur = (
-            ImageFilter.GaussianBlur(self.radius)
-            if self.blur_type == "gaussian"
-            else ImageFilter.BoxBlur(self.radius)
+            ImageFilter.GaussianBlur(self.radius) if self.blur_type == "gaussian" else ImageFilter.BoxBlur(self.radius)
         )
         blur_image = image.filter(blur)
 
         image_dto = context.services.images.create(
             image=blur_image,
             image_origin=ResourceOrigin.INTERNAL,
             image_category=ImageCategory.GENERAL,
@@ -434,18 +401,15 @@
     width:                         Union[int, None] = Field(ge=64, multiple_of=8, description="The width to resize to (px)")
     height:                        Union[int, None] = Field(ge=64, multiple_of=8, description="The height to resize to (px)")
     resample_mode:  PIL_RESAMPLING_MODES = Field(default="bicubic", description="The resampling mode")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Resize Image",
-                "tags": ["image", "resize"]
-            },
+            "ui": {"title": "Resize Image", "tags": ["image", "resize"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         resample_mode = PIL_RESAMPLING_MAP[self.resample_mode]
 
@@ -480,18 +444,15 @@
     image:          Optional[ImageField] = Field(default=None, description="The image to scale")
     scale_factor:        Optional[float] = Field(default=2.0, gt=0, description="The factor by which to scale the image")
     resample_mode:  PIL_RESAMPLING_MODES = Field(default="bicubic", description="The resampling mode")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Scale Image",
-                "tags": ["image", "scale"]
-            },
+            "ui": {"title": "Scale Image", "tags": ["image", "scale"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         resample_mode = PIL_RESAMPLING_MAP[self.resample_mode]
         width = int(image.width * self.scale_factor)
@@ -528,18 +489,15 @@
     image: Optional[ImageField]  = Field(default=None, description="The image to lerp")
     min: int = Field(default=0, ge=0, le=255, description="The minimum output value")
     max: int = Field(default=255, ge=0, le=255, description="The maximum output value")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Image Linear Interpolation",
-                "tags": ["image", "linear", "interpolation", "lerp"]
-            },
+            "ui": {"title": "Image Linear Interpolation", "tags": ["image", "linear", "interpolation", "lerp"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         image_arr = numpy.asarray(image, dtype=numpy.float32) / 255
         image_arr = image_arr * (self.max - self.min) + self.max
@@ -557,14 +515,15 @@
 
         return ImageOutput(
             image=ImageField(image_name=image_dto.image_name),
             width=image_dto.width,
             height=image_dto.height,
         )
 
+
 class ImageInverseLerpInvocation(BaseInvocation, PILInvocationConfig):
     """Inverse linear interpolation of all pixels of an image"""
 
     # fmt: off
     type: Literal["img_ilerp"] = "img_ilerp"
 
     # Inputs
@@ -573,28 +532,23 @@
     max: int = Field(default=255, ge=0, le=255, description="The maximum input value")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "Image Inverse Linear Interpolation",
-                "tags": ["image", "linear", "interpolation", "inverse"]
+                "tags": ["image", "linear", "interpolation", "inverse"],
             },
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         image_arr = numpy.asarray(image, dtype=numpy.float32)
-        image_arr = (
-            numpy.minimum(
-                numpy.maximum(image_arr - self.min, 0) / float(self.max - self.min), 1
-            )
-            * 255
-        )
+        image_arr = numpy.minimum(numpy.maximum(image_arr - self.min, 0) / float(self.max - self.min), 1) * 255
 
         ilerp_image = Image.fromarray(numpy.uint8(image_arr))
 
         image_dto = context.services.images.create(
             image=ilerp_image,
             image_origin=ResourceOrigin.INTERNAL,
             image_category=ImageCategory.GENERAL,
@@ -605,84 +559,81 @@
 
         return ImageOutput(
             image=ImageField(image_name=image_dto.image_name),
             width=image_dto.width,
             height=image_dto.height,
         )
 
+
 class ImageNSFWBlurInvocation(BaseInvocation, PILInvocationConfig):
     """Add blur to NSFW-flagged images"""
 
     # fmt: off
     type: Literal["img_nsfw"] = "img_nsfw"
 
     # Inputs
     image: Optional[ImageField]  = Field(default=None, description="The image to check")
     metadata: Optional[CoreMetadata] = Field(default=None, description="Optional core metadata to be written to the image")    
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Blur NSFW Images",
-                "tags": ["image", "nsfw", "checker"]
-            },
+            "ui": {"title": "Blur NSFW Images", "tags": ["image", "nsfw", "checker"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
-        
+
         logger = context.services.logger
         logger.debug("Running NSFW checker")
         if SafetyChecker.has_nsfw_concept(image):
             logger.info("A potentially NSFW image has been detected. Image will be blurred.")
             blurry_image = image.filter(filter=ImageFilter.GaussianBlur(radius=32))
             caution = self._get_caution_img()
-            blurry_image.paste(caution,(0,0),caution)
+            blurry_image.paste(caution, (0, 0), caution)
             image = blurry_image
 
         image_dto = context.services.images.create(
             image=image,
             image_origin=ResourceOrigin.INTERNAL,
             image_category=ImageCategory.GENERAL,
             node_id=self.id,
             session_id=context.graph_execution_state_id,
             is_intermediate=self.is_intermediate,
             metadata=self.metadata.dict() if self.metadata else None,
         )
-                
+
         return ImageOutput(
             image=ImageField(image_name=image_dto.image_name),
             width=image_dto.width,
             height=image_dto.height,
         )
-    
-    def _get_caution_img(self)->Image:
+
+    def _get_caution_img(self) -> Image:
         import invokeai.app.assets.images as image_assets
-        caution = Image.open(Path(image_assets.__path__[0]) / 'caution.png')
-        return caution.resize((caution.width // 2, caution.height //2))
+
+        caution = Image.open(Path(image_assets.__path__[0]) / "caution.png")
+        return caution.resize((caution.width // 2, caution.height // 2))
+
 
 class ImageWatermarkInvocation(BaseInvocation, PILInvocationConfig):
-    """ Add an invisible watermark to an image """
+    """Add an invisible watermark to an image"""
 
     # fmt: off
     type: Literal["img_watermark"] = "img_watermark"
 
     # Inputs
     image: Optional[ImageField]  = Field(default=None, description="The image to check")
     text: str = Field(default='InvokeAI', description="Watermark text")
     metadata: Optional[CoreMetadata] = Field(default=None, description="Optional core metadata to be written to the image")    
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Add Invisible Watermark",
-                "tags": ["image", "watermark", "invisible"]
-            },
+            "ui": {"title": "Add Invisible Watermark", "tags": ["image", "watermark", "invisible"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
         new_image = InvisibleWatermark.add_watermark(image, self.text)
         image_dto = context.services.images.create(
             image=new_image,
@@ -695,10 +646,7 @@
         )
 
         return ImageOutput(
             image=ImageField(image_name=image_dto.image_name),
             width=image_dto.width,
             height=image_dto.height,
         )
-
-
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/infill.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/infill.py`

 * *Files 3% similar despite different names*

```diff
@@ -26,31 +26,27 @@
     ]
     if PatchMatch.patchmatch_available():
         methods.insert(0, "patchmatch")
     return methods
 
 
 INFILL_METHODS = Literal[tuple(infill_methods())]
-DEFAULT_INFILL_METHOD = (
-    "patchmatch" if "patchmatch" in get_args(INFILL_METHODS) else "tile"
-)
+DEFAULT_INFILL_METHOD = "patchmatch" if "patchmatch" in get_args(INFILL_METHODS) else "tile"
 
 
 def infill_patchmatch(im: Image.Image) -> Image.Image:
     if im.mode != "RGBA":
         return im
 
     # Skip patchmatch if patchmatch isn't available
     if not PatchMatch.patchmatch_available():
         return im
 
     # Patchmatch (note, we may want to expose patch_size? Increasing it significantly impacts performance though)
-    im_patched_np = PatchMatch.inpaint(
-        im.convert("RGB"), ImageOps.invert(im.split()[-1]), patch_size=3
-    )
+    im_patched_np = PatchMatch.inpaint(im.convert("RGB"), ImageOps.invert(im.split()[-1]), patch_size=3)
     im_patched = Image.fromarray(im_patched_np, mode="RGB")
     return im_patched
 
 
 def get_tile_images(image: np.ndarray, width=8, height=8):
     _nrows, _ncols, depth = image.shape
     _strides = image.strides
@@ -64,17 +60,15 @@
         np.ravel(image),
         shape=(nrows, ncols, height, width, depth),
         strides=(height * _strides[0], width * _strides[1], *_strides),
         writeable=False,
     )
 
 
-def tile_fill_missing(
-    im: Image.Image, tile_size: int = 16, seed: Optional[int] = None
-) -> Image.Image:
+def tile_fill_missing(im: Image.Image, tile_size: int = 16, seed: Optional[int] = None) -> Image.Image:
     # Only fill if there's an alpha layer
     if im.mode != "RGBA":
         return im
 
     a = np.asarray(im, dtype=np.uint8)
 
     tile_size_tuple = (tile_size, tile_size)
@@ -99,17 +93,15 @@
 
     if len(filtered_tiles) == 0:
         return im
 
     # Find all invalid tiles and replace with a random valid tile
     replace_count = (tiles_mask == False).sum()
     rng = np.random.default_rng(seed=seed)
-    tiles_all[np.logical_not(tiles_mask)] = filtered_tiles[
-        rng.choice(filtered_tiles.shape[0], replace_count), :, :, :
-    ]
+    tiles_all[np.logical_not(tiles_mask)] = filtered_tiles[rng.choice(filtered_tiles.shape[0], replace_count), :, :, :]
 
     # Convert back to an image
     tiles_all = tiles_all.reshape(tshape)
     tiles_all = tiles_all.swapaxes(1, 2)
     st = tiles_all.reshape(
         (
             math.prod(tiles_all.shape[0:2]),
@@ -122,28 +114,23 @@
     return si
 
 
 class InfillColorInvocation(BaseInvocation):
     """Infills transparent areas of an image with a solid color"""
 
     type: Literal["infill_rgba"] = "infill_rgba"
-    image: Optional[ImageField] = Field(
-        default=None, description="The image to infill"
-    )
+    image: Optional[ImageField] = Field(default=None, description="The image to infill")
     color: ColorField = Field(
         default=ColorField(r=127, g=127, b=127, a=255),
         description="The color to use to infill",
     )
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Color Infill",
-                "tags": ["image", "inpaint", "color", "infill"]
-            },
+            "ui": {"title": "Color Infill", "tags": ["image", "inpaint", "color", "infill"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         solid_bg = Image.new("RGBA", image.size, self.color.tuple())
         infilled = Image.alpha_composite(solid_bg, image.convert("RGBA"))
@@ -167,39 +154,32 @@
 
 
 class InfillTileInvocation(BaseInvocation):
     """Infills transparent areas of an image with tiles of the image"""
 
     type: Literal["infill_tile"] = "infill_tile"
 
-    image: Optional[ImageField] = Field(
-        default=None, description="The image to infill"
-    )
+    image: Optional[ImageField] = Field(default=None, description="The image to infill")
     tile_size: int = Field(default=32, ge=1, description="The tile size (px)")
     seed: int = Field(
         ge=0,
         le=SEED_MAX,
         description="The seed to use for tile generation (omit for random)",
         default_factory=get_random_seed,
     )
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Tile Infill",
-                "tags": ["image", "inpaint", "tile", "infill"]
-            },
+            "ui": {"title": "Tile Infill", "tags": ["image", "inpaint", "tile", "infill"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
-        infilled = tile_fill_missing(
-            image.copy(), seed=self.seed, tile_size=self.tile_size
-        )
+        infilled = tile_fill_missing(image.copy(), seed=self.seed, tile_size=self.tile_size)
         infilled.paste(image, (0, 0), image.split()[-1])
 
         image_dto = context.services.images.create(
             image=infilled,
             image_origin=ResourceOrigin.INTERNAL,
             image_category=ImageCategory.GENERAL,
             node_id=self.id,
@@ -215,24 +195,19 @@
 
 
 class InfillPatchMatchInvocation(BaseInvocation):
     """Infills transparent areas of an image using the PatchMatch algorithm"""
 
     type: Literal["infill_patchmatch"] = "infill_patchmatch"
 
-    image: Optional[ImageField] = Field(
-        default=None, description="The image to infill"
-    )
+    image: Optional[ImageField] = Field(default=None, description="The image to infill")
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Patch Match Infill",
-                "tags": ["image", "inpaint", "patchmatch", "infill"]
-            },
+            "ui": {"title": "Patch Match Infill", "tags": ["image", "inpaint", "patchmatch", "infill"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
 
         if PatchMatch.patchmatch_available():
             infilled = infill_patchmatch(image.copy())
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/latent.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/latent.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,28 +8,29 @@
 from diffusers import ControlNetModel
 from diffusers.image_processor import VaeImageProcessor
 from diffusers.schedulers import SchedulerMixin as Scheduler
 from pydantic import BaseModel, Field, validator
 
 from invokeai.app.invocations.metadata import CoreMetadata
 from invokeai.app.util.step_callback import stable_diffusion_step_callback
-from invokeai.backend.model_management.models.base import ModelType
+from invokeai.backend.model_management.models import ModelType, SilenceWarnings
 
 from ...backend.model_management.lora import ModelPatcher
 from ...backend.stable_diffusion import PipelineIntermediateState
 from ...backend.stable_diffusion.diffusers_pipeline import (
-    ConditioningData, ControlNetData, StableDiffusionGeneratorPipeline,
-    image_resized_to_grid_as_tensor)
-from ...backend.stable_diffusion.diffusion.shared_invokeai_diffusion import \
-    PostprocessingSettings
+    ConditioningData,
+    ControlNetData,
+    StableDiffusionGeneratorPipeline,
+    image_resized_to_grid_as_tensor,
+)
+from ...backend.stable_diffusion.diffusion.shared_invokeai_diffusion import PostprocessingSettings
 from ...backend.stable_diffusion.schedulers import SCHEDULER_MAP
 from ...backend.util.devices import choose_torch_device, torch_dtype, choose_precision
 from ..models.image import ImageCategory, ImageField, ResourceOrigin
-from .baseinvocation import (BaseInvocation, BaseInvocationOutput,
-                             InvocationConfig, InvocationContext)
+from .baseinvocation import BaseInvocation, BaseInvocationOutput, InvocationConfig, InvocationContext
 from .compel import ConditioningField
 from .controlnet_image_processors import ControlField
 from .image import ImageOutput
 from .model import ModelInfo, UNetField, VaeField
 from invokeai.app.util.controlnet_utils import prepare_control_image
 
 from diffusers.models.attention_processor import (
@@ -42,71 +43,68 @@
 
 DEFAULT_PRECISION = choose_precision(choose_torch_device())
 
 
 class LatentsField(BaseModel):
     """A latents field used for passing latents between invocations"""
 
-    latents_name: Optional[str] = Field(
-        default=None, description="The name of the latents")
+    latents_name: Optional[str] = Field(default=None, description="The name of the latents")
 
     class Config:
         schema_extra = {"required": ["latents_name"]}
 
 
 class LatentsOutput(BaseInvocationOutput):
     """Base class for invocations that output latents"""
-    #fmt: off
+
+    # fmt: off
     type: Literal["latents_output"] = "latents_output"
 
     # Inputs
     latents: LatentsField          = Field(default=None, description="The output latents")
     width:                     int = Field(description="The width of the latents in pixels")
     height:                    int = Field(description="The height of the latents in pixels")
-    #fmt: on
+    # fmt: on
 
 
 def build_latents_output(latents_name: str, latents: torch.Tensor):
     return LatentsOutput(
         latents=LatentsField(latents_name=latents_name),
         width=latents.size()[3] * 8,
         height=latents.size()[2] * 8,
     )
 
 
-SAMPLER_NAME_VALUES = Literal[
-    tuple(list(SCHEDULER_MAP.keys()))
-]
+SAMPLER_NAME_VALUES = Literal[tuple(list(SCHEDULER_MAP.keys()))]
 
 
 def get_scheduler(
     context: InvocationContext,
     scheduler_info: ModelInfo,
     scheduler_name: str,
 ) -> Scheduler:
-    scheduler_class, scheduler_extra_config = SCHEDULER_MAP.get(
-        scheduler_name, SCHEDULER_MAP['ddim']
-    )
+    scheduler_class, scheduler_extra_config = SCHEDULER_MAP.get(scheduler_name, SCHEDULER_MAP["ddim"])
     orig_scheduler_info = context.services.model_manager.get_model(
-        **scheduler_info.dict(), context=context,
+        **scheduler_info.dict(),
+        context=context,
     )
     with orig_scheduler_info as orig_scheduler:
         scheduler_config = orig_scheduler.config
 
     if "_backup" in scheduler_config:
         scheduler_config = scheduler_config["_backup"]
     scheduler_config = {
         **scheduler_config,
         **scheduler_extra_config,
         "_backup": scheduler_config,
     }
     scheduler = scheduler_class.from_config(scheduler_config)
 
     # hack copied over from generate.py
-    if not hasattr(scheduler, 'uses_inpainting_model'):
+    if not hasattr(scheduler, "uses_inpainting_model"):
         scheduler.uses_inpainting_model = lambda: False
     return scheduler
 
 
 # Text to image
 class TextToLatentsInvocation(BaseInvocation):
     """Generates latents from conditionings."""
@@ -119,42 +117,42 @@
     negative_conditioning: Optional[ConditioningField] = Field(description="Negative conditioning for generation")
     noise: Optional[LatentsField] = Field(description="The noise to use")
     steps:       int = Field(default=10, gt=0, description="The number of steps to use to generate the image")
     cfg_scale: Union[float, List[float]] = Field(default=7.5, ge=1, description="The Classifier-Free Guidance, higher values may result in a result closer to the prompt", )
     scheduler: SAMPLER_NAME_VALUES = Field(default="euler", description="The scheduler to use" )
     unet: UNetField = Field(default=None, description="UNet submodel")
     control: Union[ControlField, list[ControlField]] = Field(default=None, description="The control to use")
-    #seamless:   bool = Field(default=False, description="Whether or not to generate an image that can tile without seams", )
-    #seamless_axes: str = Field(default="", description="The axes to tile the image on, 'x' and/or 'y'")
+    # seamless:   bool = Field(default=False, description="Whether or not to generate an image that can tile without seams", )
+    # seamless_axes: str = Field(default="", description="The axes to tile the image on, 'x' and/or 'y'")
     # fmt: on
 
     @validator("cfg_scale")
     def ge_one(cls, v):
         """validate that all cfg_scale values are >= 1"""
         if isinstance(v, list):
             for i in v:
                 if i < 1:
-                    raise ValueError('cfg_scale must be greater than 1')
+                    raise ValueError("cfg_scale must be greater than 1")
         else:
             if v < 1:
-                raise ValueError('cfg_scale must be greater than 1')
+                raise ValueError("cfg_scale must be greater than 1")
         return v
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "Text To Latents",
                 "tags": ["latents"],
                 "type_hints": {
                     "model": "model",
                     "control": "control",
                     # "cfg_scale": "float",
-                    "cfg_scale": "number"
-                }
+                    "cfg_scale": "number",
+                },
             },
         }
 
     # TODO: pass this an emitter method or something? or a session for dispatching?
     def dispatch_progress(
         self,
         context: InvocationContext,
@@ -186,24 +184,22 @@
             text_embeddings=c,
             guidance_scale=self.cfg_scale,
             extra=extra_conditioning_info,
             postprocessing_settings=PostprocessingSettings(
                 threshold=0.0,  # threshold,
                 warmup=0.2,  # warmup,
                 h_symmetry_time_pct=None,  # h_symmetry_time_pct,
-                v_symmetry_time_pct=None  # v_symmetry_time_pct,
+                v_symmetry_time_pct=None,  # v_symmetry_time_pct,
             ),
         )
 
         conditioning_data = conditioning_data.add_scheduler_args_if_applicable(
             scheduler,
-
             # for ddim scheduler
             eta=0.0,  # ddim_eta
-
             # for ancestral and sde schedulers
             generator=torch.Generator(device=unet.device).manual_seed(0),
         )
         return conditioning_data
 
     def create_pipeline(
         self,
@@ -243,29 +239,28 @@
         # really only need model for dtype and device
         model: StableDiffusionGeneratorPipeline,
         control_input: List[ControlField],
         latents_shape: List[int],
         exit_stack: ExitStack,
         do_classifier_free_guidance: bool = True,
     ) -> List[ControlNetData]:
-
         # assuming fixed dimensional scaling of 8:1 for image:latents
         control_height_resize = latents_shape[2] * 8
         control_width_resize = latents_shape[3] * 8
         if control_input is None:
             control_list = None
         elif isinstance(control_input, list) and len(control_input) == 0:
             control_list = None
         elif isinstance(control_input, ControlField):
             control_list = [control_input]
         elif isinstance(control_input, list) and len(control_input) > 0 and isinstance(control_input[0], ControlField):
             control_list = control_input
         else:
             control_list = None
-        if (control_list is None):
+        if control_list is None:
             control_data = None
             # from above handling, any control that is not None should now be of type list[ControlField]
         else:
             # FIXME: add checks to skip entry if model or image is None
             #        and if weight is None, populate with default 1.0?
             control_data = []
             control_models = []
@@ -277,17 +272,15 @@
                         base_model=control_info.control_model.base_model,
                         context=context,
                     )
                 )
 
                 control_models.append(control_model)
                 control_image_field = control_info.image
-                input_image = context.services.images.get_pil_image(
-                    control_image_field.image_name
-                )
+                input_image = context.services.images.get_pil_image(control_image_field.image_name)
                 # self.image.image_type, self.image.image_name
                 # FIXME: still need to test with different widths, heights, devices, dtypes
                 #        and add in batch_size, num_images_per_prompt?
                 #        and do real check for classifier_free_guidance?
                 # prepare_control_image should return torch.Tensor of shape(batch_size, 3, height, width)
                 control_image = prepare_control_image(
                     image=input_image,
@@ -314,200 +307,200 @@
                 )
                 control_data.append(control_item)
                 # MultiControlNetModel has been refactored out, just need list[ControlNetData]
         return control_data
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> LatentsOutput:
-        noise = context.services.latents.get(self.noise.latents_name)
-
-        # Get the source node id (we are invoking the prepared node)
-        graph_execution_state = context.services.graph_execution_manager.get(
-            context.graph_execution_state_id
-        )
-        source_node_id = graph_execution_state.prepared_source_mapping[self.id]
-
-        def step_callback(state: PipelineIntermediateState):
-            self.dispatch_progress(context, source_node_id, state)
-
-        def _lora_loader():
-            for lora in self.unet.loras:
-                lora_info = context.services.model_manager.get_model(
-                    **lora.dict(exclude={"weight"}), context=context,
-                )
-                yield (lora_info.context.model, lora.weight)
-                del lora_info
-            return
-
-        unet_info = context.services.model_manager.get_model(
-            **self.unet.unet.dict(), context=context,
-        )
-        with ExitStack() as exit_stack,\
-                ModelPatcher.apply_lora_unet(unet_info.context.model, _lora_loader()),\
-                unet_info as unet:
+        with SilenceWarnings():
+            noise = context.services.latents.get(self.noise.latents_name)
 
-            noise = noise.to(device=unet.device, dtype=unet.dtype)
+            # Get the source node id (we are invoking the prepared node)
+            graph_execution_state = context.services.graph_execution_manager.get(context.graph_execution_state_id)
+            source_node_id = graph_execution_state.prepared_source_mapping[self.id]
+
+            def step_callback(state: PipelineIntermediateState):
+                self.dispatch_progress(context, source_node_id, state)
+
+            def _lora_loader():
+                for lora in self.unet.loras:
+                    lora_info = context.services.model_manager.get_model(
+                        **lora.dict(exclude={"weight"}),
+                        context=context,
+                    )
+                    yield (lora_info.context.model, lora.weight)
+                    del lora_info
+                return
 
-            scheduler = get_scheduler(
+            unet_info = context.services.model_manager.get_model(
+                **self.unet.unet.dict(),
                 context=context,
-                scheduler_info=self.unet.scheduler,
-                scheduler_name=self.scheduler,
             )
+            with ExitStack() as exit_stack, ModelPatcher.apply_lora_unet(
+                unet_info.context.model, _lora_loader()
+            ), unet_info as unet:
+                noise = noise.to(device=unet.device, dtype=unet.dtype)
+
+                scheduler = get_scheduler(
+                    context=context,
+                    scheduler_info=self.unet.scheduler,
+                    scheduler_name=self.scheduler,
+                )
 
-            pipeline = self.create_pipeline(unet, scheduler)
-            conditioning_data = self.get_conditioning_data(context, scheduler, unet)
+                pipeline = self.create_pipeline(unet, scheduler)
+                conditioning_data = self.get_conditioning_data(context, scheduler, unet)
 
-            control_data = self.prep_control_data(
-                model=pipeline, context=context, control_input=self.control,
-                latents_shape=noise.shape,
-                # do_classifier_free_guidance=(self.cfg_scale >= 1.0))
-                do_classifier_free_guidance=True,
-                exit_stack=exit_stack,
-            )
+                control_data = self.prep_control_data(
+                    model=pipeline,
+                    context=context,
+                    control_input=self.control,
+                    latents_shape=noise.shape,
+                    # do_classifier_free_guidance=(self.cfg_scale >= 1.0))
+                    do_classifier_free_guidance=True,
+                    exit_stack=exit_stack,
+                )
 
-            # TODO: Verify the noise is the right size
-            result_latents, result_attention_map_saver = pipeline.latents_from_embeddings(
-                latents=torch.zeros_like(noise, dtype=torch_dtype(unet.device)),
-                noise=noise,
-                num_inference_steps=self.steps,
-                conditioning_data=conditioning_data,
-                control_data=control_data,  # list[ControlNetData]
-                callback=step_callback,
-            )
+                # TODO: Verify the noise is the right size
+                result_latents, result_attention_map_saver = pipeline.latents_from_embeddings(
+                    latents=torch.zeros_like(noise, dtype=torch_dtype(unet.device)),
+                    noise=noise,
+                    num_inference_steps=self.steps,
+                    conditioning_data=conditioning_data,
+                    control_data=control_data,  # list[ControlNetData]
+                    callback=step_callback,
+                )
 
-        # https://discuss.huggingface.co/t/memory-usage-by-later-pipeline-stages/23699
-        result_latents = result_latents.to("cpu")
-        torch.cuda.empty_cache()
+            # https://discuss.huggingface.co/t/memory-usage-by-later-pipeline-stages/23699
+            result_latents = result_latents.to("cpu")
+            torch.cuda.empty_cache()
 
-        name = f'{context.graph_execution_state_id}__{self.id}'
-        context.services.latents.save(name, result_latents)
-        return build_latents_output(latents_name=name, latents=result_latents)
+            name = f"{context.graph_execution_state_id}__{self.id}"
+            context.services.latents.save(name, result_latents)
+            return build_latents_output(latents_name=name, latents=result_latents)
 
 
 class LatentsToLatentsInvocation(TextToLatentsInvocation):
     """Generates latents using latents as base image."""
 
     type: Literal["l2l"] = "l2l"
 
     # Inputs
-    latents: Optional[LatentsField] = Field(
-        description="The latents to use as a base image")
-    strength: float = Field(
-        default=0.7, ge=0, le=1,
-        description="The strength of the latents to use")
+    latents: Optional[LatentsField] = Field(description="The latents to use as a base image")
+    strength: float = Field(default=0.7, ge=0, le=1, description="The strength of the latents to use")
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "Latent To Latents",
                 "tags": ["latents"],
                 "type_hints": {
                     "model": "model",
                     "control": "control",
                     "cfg_scale": "number",
-                }
+                },
             },
         }
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> LatentsOutput:
-        noise = context.services.latents.get(self.noise.latents_name)
-        latent = context.services.latents.get(self.latents.latents_name)
-
-        # Get the source node id (we are invoking the prepared node)
-        graph_execution_state = context.services.graph_execution_manager.get(
-            context.graph_execution_state_id
-        )
-        source_node_id = graph_execution_state.prepared_source_mapping[self.id]
-
-        def step_callback(state: PipelineIntermediateState):
-            self.dispatch_progress(context, source_node_id, state)
-
-        def _lora_loader():
-            for lora in self.unet.loras:
-                lora_info = context.services.model_manager.get_model(
-                    **lora.dict(exclude={"weight"}), context=context,
-                )
-                yield (lora_info.context.model, lora.weight)
-                del lora_info
-            return
-
-        unet_info = context.services.model_manager.get_model(
-            **self.unet.unet.dict(), context=context,
-        )
-        with ExitStack() as exit_stack,\
-                ModelPatcher.apply_lora_unet(unet_info.context.model, _lora_loader()),\
-                unet_info as unet:
-
-            noise = noise.to(device=unet.device, dtype=unet.dtype)
-            latent = latent.to(device=unet.device, dtype=unet.dtype)
+        with SilenceWarnings():  # this quenches NSFW nag from diffusers
+            noise = context.services.latents.get(self.noise.latents_name)
+            latent = context.services.latents.get(self.latents.latents_name)
+
+            # Get the source node id (we are invoking the prepared node)
+            graph_execution_state = context.services.graph_execution_manager.get(context.graph_execution_state_id)
+            source_node_id = graph_execution_state.prepared_source_mapping[self.id]
+
+            def step_callback(state: PipelineIntermediateState):
+                self.dispatch_progress(context, source_node_id, state)
+
+            def _lora_loader():
+                for lora in self.unet.loras:
+                    lora_info = context.services.model_manager.get_model(
+                        **lora.dict(exclude={"weight"}),
+                        context=context,
+                    )
+                    yield (lora_info.context.model, lora.weight)
+                    del lora_info
+                return
 
-            scheduler = get_scheduler(
+            unet_info = context.services.model_manager.get_model(
+                **self.unet.unet.dict(),
                 context=context,
-                scheduler_info=self.unet.scheduler,
-                scheduler_name=self.scheduler,
             )
+            with ExitStack() as exit_stack, ModelPatcher.apply_lora_unet(
+                unet_info.context.model, _lora_loader()
+            ), unet_info as unet:
+                noise = noise.to(device=unet.device, dtype=unet.dtype)
+                latent = latent.to(device=unet.device, dtype=unet.dtype)
+
+                scheduler = get_scheduler(
+                    context=context,
+                    scheduler_info=self.unet.scheduler,
+                    scheduler_name=self.scheduler,
+                )
 
-            pipeline = self.create_pipeline(unet, scheduler)
-            conditioning_data = self.get_conditioning_data(context, scheduler, unet)
+                pipeline = self.create_pipeline(unet, scheduler)
+                conditioning_data = self.get_conditioning_data(context, scheduler, unet)
 
-            control_data = self.prep_control_data(
-                model=pipeline, context=context, control_input=self.control,
-                latents_shape=noise.shape,
-                # do_classifier_free_guidance=(self.cfg_scale >= 1.0))
-                do_classifier_free_guidance=True,
-                exit_stack=exit_stack,
-            )
+                control_data = self.prep_control_data(
+                    model=pipeline,
+                    context=context,
+                    control_input=self.control,
+                    latents_shape=noise.shape,
+                    # do_classifier_free_guidance=(self.cfg_scale >= 1.0))
+                    do_classifier_free_guidance=True,
+                    exit_stack=exit_stack,
+                )
 
-            # TODO: Verify the noise is the right size
-            initial_latents = latent if self.strength < 1.0 else torch.zeros_like(
-                latent, device=unet.device, dtype=latent.dtype
-            )
+                # TODO: Verify the noise is the right size
+                initial_latents = (
+                    latent if self.strength < 1.0 else torch.zeros_like(latent, device=unet.device, dtype=latent.dtype)
+                )
 
-            timesteps, _ = pipeline.get_img2img_timesteps(
-                self.steps,
-                self.strength,
-                device=unet.device,
-            )
+                timesteps, _ = pipeline.get_img2img_timesteps(
+                    self.steps,
+                    self.strength,
+                    device=unet.device,
+                )
 
-            result_latents, result_attention_map_saver = pipeline.latents_from_embeddings(
-                latents=initial_latents,
-                timesteps=timesteps,
-                noise=noise,
-                num_inference_steps=self.steps,
-                conditioning_data=conditioning_data,
-                control_data=control_data,  # list[ControlNetData]
-                callback=step_callback
-            )
+                result_latents, result_attention_map_saver = pipeline.latents_from_embeddings(
+                    latents=initial_latents,
+                    timesteps=timesteps,
+                    noise=noise,
+                    num_inference_steps=self.steps,
+                    conditioning_data=conditioning_data,
+                    control_data=control_data,  # list[ControlNetData]
+                    callback=step_callback,
+                )
 
-        # https://discuss.huggingface.co/t/memory-usage-by-later-pipeline-stages/23699
-        result_latents = result_latents.to("cpu")
-        torch.cuda.empty_cache()
+            # https://discuss.huggingface.co/t/memory-usage-by-later-pipeline-stages/23699
+            result_latents = result_latents.to("cpu")
+            torch.cuda.empty_cache()
 
-        name = f'{context.graph_execution_state_id}__{self.id}'
-        context.services.latents.save(name, result_latents)
+            name = f"{context.graph_execution_state_id}__{self.id}"
+            context.services.latents.save(name, result_latents)
         return build_latents_output(latents_name=name, latents=result_latents)
 
 
 # Latent to image
 class LatentsToImageInvocation(BaseInvocation):
     """Generates an image from latents."""
 
     type: Literal["l2i"] = "l2i"
 
     # Inputs
-    latents: Optional[LatentsField] = Field(
-        description="The latents to generate an image from")
+    latents: Optional[LatentsField] = Field(description="The latents to generate an image from")
     vae: VaeField = Field(default=None, description="Vae submodel")
-    tiled: bool = Field(
-        default=False,
-        description="Decode latents by overlapping tiles(less memory consumption)")
-    fp32: bool = Field(DEFAULT_PRECISION=='float32', description="Decode in full precision")
-    metadata: Optional[CoreMetadata] = Field(default=None, description="Optional core metadata to be written to the image")
+    tiled: bool = Field(default=False, description="Decode latents by overlaping tiles (less memory consumption)")
+    fp32: bool = Field(DEFAULT_PRECISION == "float32", description="Decode in full precision")
+    metadata: Optional[CoreMetadata] = Field(
+        default=None, description="Optional core metadata to be written to the image"
+    )
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "Latents To Image",
                 "tags": ["latents", "image"],
@@ -515,15 +508,16 @@
         }
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> ImageOutput:
         latents = context.services.latents.get(self.latents.latents_name)
 
         vae_info = context.services.model_manager.get_model(
-            **self.vae.vae.dict(), context=context,
+            **self.vae.vae.dict(),
+            context=context,
         )
 
         with vae_info as vae:
             latents = latents.to(vae.device)
             if self.fp32:
                 vae.to(dtype=torch.float32)
 
@@ -582,54 +576,47 @@
         return ImageOutput(
             image=ImageField(image_name=image_dto.image_name),
             width=image_dto.width,
             height=image_dto.height,
         )
 
 
-LATENTS_INTERPOLATION_MODE = Literal["nearest", "linear",
-                                     "bilinear", "bicubic", "trilinear", "area", "nearest-exact"]
+LATENTS_INTERPOLATION_MODE = Literal["nearest", "linear", "bilinear", "bicubic", "trilinear", "area", "nearest-exact"]
 
 
 class ResizeLatentsInvocation(BaseInvocation):
     """Resizes latents to explicit width/height (in pixels). Provided dimensions are floor-divided by 8."""
 
     type: Literal["lresize"] = "lresize"
 
     # Inputs
-    latents: Optional[LatentsField] = Field(
-        description="The latents to resize")
-    width:                         Union[int, None] = Field(default=512,
-        ge=64, multiple_of=8, description="The width to resize to (px)")
-    height:                        Union[int, None] = Field(default=512,
-        ge=64, multiple_of=8, description="The height to resize to (px)")
-    mode:   LATENTS_INTERPOLATION_MODE = Field(
-        default="bilinear", description="The interpolation mode")
+    latents: Optional[LatentsField] = Field(description="The latents to resize")
+    width: Union[int, None] = Field(default=512, ge=64, multiple_of=8, description="The width to resize to (px)")
+    height: Union[int, None] = Field(default=512, ge=64, multiple_of=8, description="The height to resize to (px)")
+    mode: LATENTS_INTERPOLATION_MODE = Field(default="bilinear", description="The interpolation mode")
     antialias: bool = Field(
-        default=False,
-        description="Whether or not to antialias (applied in bilinear and bicubic modes only)")
+        default=False, description="Whether or not to antialias (applied in bilinear and bicubic modes only)"
+    )
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Resize Latents",
-                "tags": ["latents", "resize"]
-            },
+            "ui": {"title": "Resize Latents", "tags": ["latents", "resize"]},
         }
 
     def invoke(self, context: InvocationContext) -> LatentsOutput:
         latents = context.services.latents.get(self.latents.latents_name)
 
         # TODO:
-        device=choose_torch_device()
+        device = choose_torch_device()
 
         resized_latents = torch.nn.functional.interpolate(
-            latents.to(device), size=(self.height // 8, self.width // 8),
-            mode=self.mode, antialias=self.antialias
-            if self.mode in ["bilinear", "bicubic"] else False,
+            latents.to(device),
+            size=(self.height // 8, self.width // 8),
+            mode=self.mode,
+            antialias=self.antialias if self.mode in ["bilinear", "bicubic"] else False,
         )
 
         # https://discuss.huggingface.co/t/memory-usage-by-later-pipeline-stages/23699
         resized_latents = resized_latents.to("cpu")
         torch.cuda.empty_cache()
 
         name = f"{context.graph_execution_state_id}__{self.id}"
@@ -640,43 +627,38 @@
 
 class ScaleLatentsInvocation(BaseInvocation):
     """Scales latents by a given factor."""
 
     type: Literal["lscale"] = "lscale"
 
     # Inputs
-    latents: Optional[LatentsField] = Field(
-        description="The latents to scale")
-    scale_factor:               float = Field(
-        gt=0, description="The factor by which to scale the latents")
-    mode:  LATENTS_INTERPOLATION_MODE = Field(
-        default="bilinear", description="The interpolation mode")
+    latents: Optional[LatentsField] = Field(description="The latents to scale")
+    scale_factor: float = Field(gt=0, description="The factor by which to scale the latents")
+    mode: LATENTS_INTERPOLATION_MODE = Field(default="bilinear", description="The interpolation mode")
     antialias: bool = Field(
-        default=False,
-        description="Whether or not to antialias (applied in bilinear and bicubic modes only)")
+        default=False, description="Whether or not to antialias (applied in bilinear and bicubic modes only)"
+    )
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Scale Latents",
-                "tags": ["latents", "scale"]
-            },
+            "ui": {"title": "Scale Latents", "tags": ["latents", "scale"]},
         }
 
     def invoke(self, context: InvocationContext) -> LatentsOutput:
         latents = context.services.latents.get(self.latents.latents_name)
 
         # TODO:
-        device=choose_torch_device()
+        device = choose_torch_device()
 
         # resizing
         resized_latents = torch.nn.functional.interpolate(
-            latents.to(device), scale_factor=self.scale_factor, mode=self.mode,
-            antialias=self.antialias
-            if self.mode in ["bilinear", "bicubic"] else False,
+            latents.to(device),
+            scale_factor=self.scale_factor,
+            mode=self.mode,
+            antialias=self.antialias if self.mode in ["bilinear", "bicubic"] else False,
         )
 
         # https://discuss.huggingface.co/t/memory-usage-by-later-pipeline-stages/23699
         resized_latents = resized_latents.to("cpu")
         torch.cuda.empty_cache()
 
         name = f"{context.graph_execution_state_id}__{self.id}"
@@ -689,39 +671,34 @@
     """Encodes an image into latents."""
 
     type: Literal["i2l"] = "i2l"
 
     # Inputs
     image: Optional[ImageField] = Field(description="The image to encode")
     vae: VaeField = Field(default=None, description="Vae submodel")
-    tiled: bool = Field(
-        default=False,
-        description="Encode latents by overlaping tiles(less memory consumption)")
-    fp32: bool = Field(DEFAULT_PRECISION=='float32', description="Decode in full precision")
-
+    tiled: bool = Field(default=False, description="Encode latents by overlaping tiles(less memory consumption)")
+    fp32: bool = Field(DEFAULT_PRECISION == "float32", description="Decode in full precision")
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Image To Latents",
-                "tags": ["latents", "image"]
-            },
+            "ui": {"title": "Image To Latents", "tags": ["latents", "image"]},
         }
 
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> LatentsOutput:
         # image = context.services.images.get(
         #     self.image.image_type, self.image.image_name
         # )
         image = context.services.images.get_pil_image(self.image.image_name)
 
-        #vae_info = context.services.model_manager.get_model(**self.vae.vae.dict())
+        # vae_info = context.services.model_manager.get_model(**self.vae.vae.dict())
         vae_info = context.services.model_manager.get_model(
-            **self.vae.vae.dict(), context=context,
+            **self.vae.vae.dict(),
+            context=context,
         )
 
         image_tensor = image_resized_to_grid_as_tensor(image.convert("RGB"))
         if image_tensor.dim() == 3:
             image_tensor = einops.rearrange(image_tensor, "c h w -> 1 c h w")
 
         with vae_info as vae:
@@ -740,33 +717,31 @@
                 )
                 # if xformers or torch_2_0 is used attention block does not need
                 # to be in float32 which can save lots of memory
                 if use_torch_2_0_or_xformers:
                     vae.post_quant_conv.to(orig_dtype)
                     vae.decoder.conv_in.to(orig_dtype)
                     vae.decoder.mid_block.to(orig_dtype)
-                #else:
+                # else:
                 #    latents = latents.float()
 
             else:
                 vae.to(dtype=torch.float16)
-                #latents = latents.half()
+                # latents = latents.half()
 
             if self.tiled:
                 vae.enable_tiling()
             else:
                 vae.disable_tiling()
 
             # non_noised_latents_from_image
             image_tensor = image_tensor.to(device=vae.device, dtype=vae.dtype)
             with torch.inference_mode():
                 image_tensor_dist = vae.encode(image_tensor).latent_dist
-                latents = image_tensor_dist.sample().to(
-                    dtype=vae.dtype
-                )  # FIXME: uses torch.randn. make reproducible!
+                latents = image_tensor_dist.sample().to(dtype=vae.dtype)  # FIXME: uses torch.randn. make reproducible!
 
             latents = vae.config.scaling_factor * latents
             latents = latents.to(dtype=orig_dtype)
 
         name = f"{context.graph_execution_state_id}__{self.id}"
         latents = latents.to("cpu")
         context.services.latents.save(name, latents)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/math.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/math.py`

 * *Files 12% similar despite different names*

```diff
@@ -50,18 +50,15 @@
     type: Literal["add"] = "add"
     a: int = Field(default=0, description="The first number")
     b: int = Field(default=0, description="The second number")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Add",
-                "tags": ["math", "add"]
-            },
+            "ui": {"title": "Add", "tags": ["math", "add"]},
         }
 
     def invoke(self, context: InvocationContext) -> IntOutput:
         return IntOutput(a=self.a + self.b)
 
 
 class SubtractInvocation(BaseInvocation, MathInvocationConfig):
@@ -71,18 +68,15 @@
     type: Literal["sub"] = "sub"
     a: int = Field(default=0, description="The first number")
     b: int = Field(default=0, description="The second number")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Subtract",
-                "tags": ["math", "subtract"]
-            },
+            "ui": {"title": "Subtract", "tags": ["math", "subtract"]},
         }
 
     def invoke(self, context: InvocationContext) -> IntOutput:
         return IntOutput(a=self.a - self.b)
 
 
 class MultiplyInvocation(BaseInvocation, MathInvocationConfig):
@@ -92,18 +86,15 @@
     type: Literal["mul"] = "mul"
     a: int = Field(default=0, description="The first number")
     b: int = Field(default=0, description="The second number")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Multiply",
-                "tags": ["math", "multiply"]
-            },
+            "ui": {"title": "Multiply", "tags": ["math", "multiply"]},
         }
 
     def invoke(self, context: InvocationContext) -> IntOutput:
         return IntOutput(a=self.a * self.b)
 
 
 class DivideInvocation(BaseInvocation, MathInvocationConfig):
@@ -113,18 +104,15 @@
     type: Literal["div"] = "div"
     a: int = Field(default=0, description="The first number")
     b: int = Field(default=0, description="The second number")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Divide",
-                "tags": ["math", "divide"]
-            },
+            "ui": {"title": "Divide", "tags": ["math", "divide"]},
         }
 
     def invoke(self, context: InvocationContext) -> IntOutput:
         return IntOutput(a=int(self.a / self.b))
 
 
 class RandomIntInvocation(BaseInvocation):
@@ -136,15 +124,12 @@
     high: int = Field(
         default=np.iinfo(np.int32).max, description="The exclusive high value"
     )
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Random Integer",
-                "tags": ["math", "random", "integer"]
-            },
+            "ui": {"title": "Random Integer", "tags": ["math", "random", "integer"]},
         }
 
     def invoke(self, context: InvocationContext) -> IntOutput:
         return IntOutput(a=np.random.randint(self.low, self.high))
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/metadata.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/metadata.py`

 * *Files 11% similar despite different names*

```diff
@@ -7,14 +7,15 @@
     BaseInvocationOutput,
     InvocationConfig,
     InvocationContext,
 )
 from invokeai.app.invocations.controlnet_image_processors import ControlField
 from invokeai.app.invocations.model import LoRAModelField, MainModelField, VAEModelField
 
+
 class LoRAMetadataField(BaseModel):
     """LoRA metadata for an image generated in InvokeAI."""
 
     lora: LoRAModelField = Field(description="The LoRA model")
     weight: float = Field(description="The weight of the LoRA model")
 
 
@@ -33,72 +34,54 @@
     cfg_scale: float = Field(description="The classifier-free guidance scale parameter")
     steps: int = Field(description="The number of steps used for inference")
     scheduler: str = Field(description="The scheduler used for inference")
     clip_skip: int = Field(
         description="The number of skipped CLIP layers",
     )
     model: MainModelField = Field(description="The main model used for inference")
-    controlnets: list[ControlField] = Field(
-        description="The ControlNets used for inference"
-    )
+    controlnets: list[ControlField] = Field(description="The ControlNets used for inference")
     loras: list[LoRAMetadataField] = Field(description="The LoRAs used for inference")
     vae: Union[VAEModelField, None] = Field(
         default=None,
         description="The VAE used for decoding, if the main model's default was not used",
     )
 
     # Latents-to-Latents
     strength: Union[float, None] = Field(
         default=None,
         description="The strength used for latents-to-latents",
     )
-    init_image: Union[str, None] = Field(
-        default=None, description="The name of the initial image"
-    )
+    init_image: Union[str, None] = Field(default=None, description="The name of the initial image")
 
     # SDXL
-    positive_style_prompt: Union[str, None] = Field(
-        default=None, description="The positive style prompt parameter"
-    )
-    negative_style_prompt: Union[str, None] = Field(
-        default=None, description="The negative style prompt parameter"
-    )
+    positive_style_prompt: Union[str, None] = Field(default=None, description="The positive style prompt parameter")
+    negative_style_prompt: Union[str, None] = Field(default=None, description="The negative style prompt parameter")
 
     # SDXL Refiner
-    refiner_model: Union[MainModelField, None] = Field(
-        default=None, description="The SDXL Refiner model used"
-    )
+    refiner_model: Union[MainModelField, None] = Field(default=None, description="The SDXL Refiner model used")
     refiner_cfg_scale: Union[float, None] = Field(
         default=None,
         description="The classifier-free guidance scale parameter used for the refiner",
     )
-    refiner_steps: Union[int, None] = Field(
-        default=None, description="The number of steps used for the refiner"
-    )
-    refiner_scheduler: Union[str, None] = Field(
-        default=None, description="The scheduler used for the refiner"
-    )
+    refiner_steps: Union[int, None] = Field(default=None, description="The number of steps used for the refiner")
+    refiner_scheduler: Union[str, None] = Field(default=None, description="The scheduler used for the refiner")
     refiner_aesthetic_store: Union[float, None] = Field(
         default=None, description="The aesthetic score used for the refiner"
     )
-    refiner_start: Union[float, None] = Field(
-        default=None, description="The start value used for refiner denoising"
-    )
+    refiner_start: Union[float, None] = Field(default=None, description="The start value used for refiner denoising")
 
 
 class ImageMetadata(BaseModel):
     """An image's generation metadata"""
 
     metadata: Optional[dict] = Field(
         default=None,
         description="The image's core metadata, if it was created in the Linear or Canvas UI",
     )
-    graph: Optional[dict] = Field(
-        default=None, description="The graph that created the image"
-    )
+    graph: Optional[dict] = Field(default=None, description="The graph that created the image")
 
 
 class MetadataAccumulatorOutput(BaseInvocationOutput):
     """The output of the MetadataAccumulator node"""
 
     type: Literal["metadata_accumulator_output"] = "metadata_accumulator_output"
 
@@ -122,58 +105,42 @@
     cfg_scale: float = Field(description="The classifier-free guidance scale parameter")
     steps: int = Field(description="The number of steps used for inference")
     scheduler: str = Field(description="The scheduler used for inference")
     clip_skip: int = Field(
         description="The number of skipped CLIP layers",
     )
     model: MainModelField = Field(description="The main model used for inference")
-    controlnets: list[ControlField] = Field(
-        description="The ControlNets used for inference"
-    )
+    controlnets: list[ControlField] = Field(description="The ControlNets used for inference")
     loras: list[LoRAMetadataField] = Field(description="The LoRAs used for inference")
     strength: Union[float, None] = Field(
         default=None,
         description="The strength used for latents-to-latents",
     )
-    init_image: Union[str, None] = Field(
-        default=None, description="The name of the initial image"
-    )
+    init_image: Union[str, None] = Field(default=None, description="The name of the initial image")
     vae: Union[VAEModelField, None] = Field(
         default=None,
         description="The VAE used for decoding, if the main model's default was not used",
     )
 
     # SDXL
-    positive_style_prompt: Union[str, None] = Field(
-        default=None, description="The positive style prompt parameter"
-    )
-    negative_style_prompt: Union[str, None] = Field(
-        default=None, description="The negative style prompt parameter"
-    )
+    positive_style_prompt: Union[str, None] = Field(default=None, description="The positive style prompt parameter")
+    negative_style_prompt: Union[str, None] = Field(default=None, description="The negative style prompt parameter")
 
     # SDXL Refiner
-    refiner_model: Union[MainModelField, None] = Field(
-        default=None, description="The SDXL Refiner model used"
-    )
+    refiner_model: Union[MainModelField, None] = Field(default=None, description="The SDXL Refiner model used")
     refiner_cfg_scale: Union[float, None] = Field(
         default=None,
         description="The classifier-free guidance scale parameter used for the refiner",
     )
-    refiner_steps: Union[int, None] = Field(
-        default=None, description="The number of steps used for the refiner"
-    )
-    refiner_scheduler: Union[str, None] = Field(
-        default=None, description="The scheduler used for the refiner"
-    )
+    refiner_steps: Union[int, None] = Field(default=None, description="The number of steps used for the refiner")
+    refiner_scheduler: Union[str, None] = Field(default=None, description="The scheduler used for the refiner")
     refiner_aesthetic_store: Union[float, None] = Field(
         default=None, description="The aesthetic score used for the refiner"
     )
-    refiner_start: Union[float, None] = Field(
-        default=None, description="The start value used for refiner denoising"
-    )
+    refiner_start: Union[float, None] = Field(default=None, description="The start value used for refiner denoising")
 
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "Metadata Accumulator",
                 "tags": ["image", "metadata", "generation"],
             },
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/model.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/model.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,24 +1,21 @@
 import copy
 from typing import List, Literal, Optional, Union
 
 from pydantic import BaseModel, Field
 
 from ...backend.model_management import BaseModelType, ModelType, SubModelType
-from .baseinvocation import (BaseInvocation, BaseInvocationOutput,
-                             InvocationConfig, InvocationContext)
+from .baseinvocation import BaseInvocation, BaseInvocationOutput, InvocationConfig, InvocationContext
 
 
 class ModelInfo(BaseModel):
     model_name: str = Field(description="Info to load submodel")
     base_model: BaseModelType = Field(description="Base model")
     model_type: ModelType = Field(description="Info to load submodel")
-    submodel: Optional[SubModelType] = Field(
-        default=None, description="Info to load submodel"
-    )
+    submodel: Optional[SubModelType] = Field(default=None, description="Info to load submodel")
 
 
 class LoraInfo(ModelInfo):
     weight: float = Field(description="Lora's weight which to use when apply to model")
 
 
 class UNetField(BaseModel):
@@ -29,14 +26,15 @@
 
 class ClipField(BaseModel):
     tokenizer: ModelInfo = Field(description="Info to load tokenizer submodel")
     text_encoder: ModelInfo = Field(description="Info to load text_encoder submodel")
     skipped_layers: int = Field(description="Number of skipped layers in text_encoder")
     loras: List[LoraInfo] = Field(description="Loras to apply on model loading")
 
+
 class VaeField(BaseModel):
     # TODO: better naming?
     vae: ModelInfo = Field(description="Info to load vae submodel")
 
 
 class ModelLoaderOutput(BaseInvocationOutput):
     """Model loader output"""
@@ -45,27 +43,29 @@
     type: Literal["model_loader_output"] = "model_loader_output"
 
     unet: UNetField = Field(default=None, description="UNet submodel")
     clip: ClipField = Field(default=None, description="Tokenizer and text_encoder submodels")
     vae: VaeField = Field(default=None, description="Vae submodel")
     # fmt: on
 
+
 class MainModelField(BaseModel):
     """Main model field"""
 
     model_name: str = Field(description="Name of the model")
     base_model: BaseModelType = Field(description="Base model")
 
 
 class LoRAModelField(BaseModel):
     """LoRA model field"""
 
     model_name: str = Field(description="Name of the LoRA model")
     base_model: BaseModelType = Field(description="Base model")
 
+
 class MainModelLoaderInvocation(BaseInvocation):
     """Loads a main model, outputting its submodels."""
 
     type: Literal["main_model_loader"] = "main_model_loader"
 
     model: MainModelField = Field(description="The model to load")
     # TODO: precision?
@@ -176,15 +176,15 @@
                     base_model=base_model,
                     model_type=model_type,
                     submodel=SubModelType.Vae,
                 ),
             ),
         )
 
-    
+
 class LoraLoaderOutput(BaseInvocationOutput):
     """Model loader output"""
 
     # fmt: off
     type: Literal["lora_loader_output"] = "lora_loader_output"
 
     unet: Optional[UNetField] = Field(default=None, description="UNet submodel")
@@ -193,17 +193,15 @@
 
 
 class LoraLoaderInvocation(BaseInvocation):
     """Apply selected lora to unet and text_encoder."""
 
     type: Literal["lora_loader"] = "lora_loader"
 
-    lora: Union[LoRAModelField, None] = Field(
-        default=None, description="Lora model name"
-    )
+    lora: Union[LoRAModelField, None] = Field(default=None, description="Lora model name")
     weight: float = Field(default=0.75, description="With what weight to apply lora")
 
     unet: Optional[UNetField] = Field(description="UNet model for applying lora")
     clip: Optional[ClipField] = Field(description="Clip model for applying lora")
 
     class Config(InvocationConfig):
         schema_extra = {
@@ -224,22 +222,18 @@
         if not context.services.model_manager.model_exists(
             base_model=base_model,
             model_name=lora_name,
             model_type=ModelType.Lora,
         ):
             raise Exception(f"Unkown lora name: {lora_name}!")
 
-        if self.unet is not None and any(
-            lora.model_name == lora_name for lora in self.unet.loras
-        ):
+        if self.unet is not None and any(lora.model_name == lora_name for lora in self.unet.loras):
             raise Exception(f'Lora "{lora_name}" already applied to unet')
 
-        if self.clip is not None and any(
-            lora.model_name == lora_name for lora in self.clip.loras
-        ):
+        if self.clip is not None and any(lora.model_name == lora_name for lora in self.clip.loras):
             raise Exception(f'Lora "{lora_name}" already applied to clip')
 
         output = LoraLoaderOutput()
 
         if self.unet is not None:
             output.unet = copy.deepcopy(self.unet)
             output.unet.loras.append(
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/noise.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/noise.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/param_easing.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/param_easing.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,24 +8,45 @@
 
 from pydantic import BaseModel, Field
 import numpy as np
 import matplotlib.pyplot as plt
 
 from easing_functions import (
     LinearInOut,
-    QuadEaseInOut, QuadEaseIn, QuadEaseOut,
-    CubicEaseInOut, CubicEaseIn, CubicEaseOut,
-    QuarticEaseInOut, QuarticEaseIn, QuarticEaseOut,
-    QuinticEaseInOut, QuinticEaseIn, QuinticEaseOut,
-    SineEaseInOut, SineEaseIn, SineEaseOut,
-    CircularEaseIn, CircularEaseInOut, CircularEaseOut,
-    ExponentialEaseInOut, ExponentialEaseIn, ExponentialEaseOut,
-    ElasticEaseIn, ElasticEaseInOut, ElasticEaseOut,
-    BackEaseIn, BackEaseInOut, BackEaseOut,
-    BounceEaseIn, BounceEaseInOut, BounceEaseOut)
+    QuadEaseInOut,
+    QuadEaseIn,
+    QuadEaseOut,
+    CubicEaseInOut,
+    CubicEaseIn,
+    CubicEaseOut,
+    QuarticEaseInOut,
+    QuarticEaseIn,
+    QuarticEaseOut,
+    QuinticEaseInOut,
+    QuinticEaseIn,
+    QuinticEaseOut,
+    SineEaseInOut,
+    SineEaseIn,
+    SineEaseOut,
+    CircularEaseIn,
+    CircularEaseInOut,
+    CircularEaseOut,
+    ExponentialEaseInOut,
+    ExponentialEaseIn,
+    ExponentialEaseOut,
+    ElasticEaseIn,
+    ElasticEaseInOut,
+    ElasticEaseOut,
+    BackEaseIn,
+    BackEaseInOut,
+    BackEaseOut,
+    BounceEaseIn,
+    BounceEaseInOut,
+    BounceEaseOut,
+)
 
 from .baseinvocation import (
     BaseInvocation,
     BaseInvocationOutput,
     InvocationContext,
     InvocationConfig,
 )
@@ -41,25 +62,20 @@
     # Inputs
     start: float = Field(default=5, description="The first value of the range")
     stop: float = Field(default=10, description="The last value of the range")
     steps: int = Field(default=30, description="number of values to interpolate over (including start and stop)")
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Linear Range (Float)",
-                "tags": ["math", "float", "linear", "range"]
-            },
+            "ui": {"title": "Linear Range (Float)", "tags": ["math", "float", "linear", "range"]},
         }
 
     def invoke(self, context: InvocationContext) -> FloatCollectionOutput:
         param_list = list(np.linspace(self.start, self.stop, self.steps))
-        return FloatCollectionOutput(
-            collection=param_list
-        )
+        return FloatCollectionOutput(collection=param_list)
 
 
 EASING_FUNCTIONS_MAP = {
     "Linear": LinearInOut,
     "QuadIn": QuadEaseIn,
     "QuadOut": QuadEaseOut,
     "QuadInOut": QuadEaseInOut,
@@ -88,17 +104,15 @@
     "BackOut": BackEaseOut,
     "BackInOut": BackEaseInOut,
     "BounceIn": BounceEaseIn,
     "BounceOut": BounceEaseOut,
     "BounceInOut": BounceEaseInOut,
 }
 
-EASING_FUNCTION_KEYS: Any = Literal[
-    tuple(list(EASING_FUNCTIONS_MAP.keys()))
-]
+EASING_FUNCTION_KEYS: Any = Literal[tuple(list(EASING_FUNCTIONS_MAP.keys()))]
 
 
 # actually I think for now could just use CollectionOutput (which is list[Any]
 class StepParamEasingInvocation(BaseInvocation):
     """Experimental per-step parameter easing for denoising steps"""
 
     type: Literal["step_param_easing"] = "step_param_easing"
@@ -119,21 +133,17 @@
     # FIXME: add alt_mirror option (alternative to default or mirror), or remove entirely
     # alt_mirror: bool = Field(default=False, description="alternative mirroring by dual easing")
     show_easing_plot: bool = Field(default=False, description="show easing plot")
     # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Param Easing By Step",
-                "tags": ["param", "step", "easing"]
-            },
+            "ui": {"title": "Param Easing By Step", "tags": ["param", "step", "easing"]},
         }
 
-
     def invoke(self, context: InvocationContext) -> FloatCollectionOutput:
         log_diagnostics = False
         # convert from start_step_percent to nearest step <= (steps * start_step_percent)
         # start_step = int(np.floor(self.num_steps * self.start_step_percent))
         start_step = int(np.round(self.num_steps * self.start_step_percent))
         # convert from end_step_percent to nearest step >= (steps * end_step_percent)
         # end_step = int(np.ceil((self.num_steps - 1) * self.end_step_percent))
@@ -166,20 +176,21 @@
         if self.mirror:  # "expected" mirroring
             # if number of steps is even, squeeze duration down to (number_of_steps)/2
             # and create reverse copy of list to append
             # if number of steps is odd, squeeze duration down to ceil(number_of_steps/2)
             # and create reverse copy of list[1:end-1]
             # but if even then number_of_steps/2 === ceil(number_of_steps/2), so can just use ceil always
 
-            base_easing_duration = int(np.ceil(num_easing_steps/2.0))
-            if log_diagnostics: context.services.logger.debug("base easing duration: " + str(base_easing_duration))
-            even_num_steps = (num_easing_steps % 2 == 0)  # even number of steps
-            easing_function = easing_class(start=self.start_value,
-                                           end=self.end_value,
-                                           duration=base_easing_duration - 1)
+            base_easing_duration = int(np.ceil(num_easing_steps / 2.0))
+            if log_diagnostics:
+                context.services.logger.debug("base easing duration: " + str(base_easing_duration))
+            even_num_steps = num_easing_steps % 2 == 0  # even number of steps
+            easing_function = easing_class(
+                start=self.start_value, end=self.end_value, duration=base_easing_duration - 1
+            )
             base_easing_vals = list()
             for step_index in range(base_easing_duration):
                 easing_val = easing_function.ease(step_index)
                 base_easing_vals.append(easing_val)
                 if log_diagnostics:
                     context.services.logger.debug("step_index: " + str(step_index) + ", easing_val: " + str(easing_val))
             if even_num_steps:
@@ -210,17 +221,15 @@
         #         else:
         #             step_val = mirror_function.ease(step_index - half_ease_duration)
         #         easing_list.append(step_val)
         #         if log_diagnostics: logger.debug(step_index, step_val)
         #
 
         else:  # no mirroring (default)
-            easing_function = easing_class(start=self.start_value,
-                                           end=self.end_value,
-                                           duration=num_easing_steps - 1)
+            easing_function = easing_class(start=self.start_value, end=self.end_value, duration=num_easing_steps - 1)
             for step_index in range(num_easing_steps):
                 step_val = easing_function.ease(step_index)
                 easing_list.append(step_val)
                 if log_diagnostics:
                     context.services.logger.debug("step_index: " + str(step_index) + ", easing_val: " + str(step_val))
 
         if log_diagnostics:
@@ -236,17 +245,15 @@
             plt.ylabel("Param Value")
             plt.title("Per-Step Values Based On Easing: " + self.easing)
             plt.bar(range(len(param_list)), param_list)
             # plt.plot(param_list)
             ax = plt.gca()
             ax.xaxis.set_major_locator(MaxNLocator(integer=True))
             buf = io.BytesIO()
-            plt.savefig(buf, format='png')
+            plt.savefig(buf, format="png")
             buf.seek(0)
             im = PIL.Image.open(buf)
             im.show()
             buf.close()
 
         # output array of size steps, each entry list[i] is param value for step i
-        return FloatCollectionOutput(
-            collection=param_list
-        )
+        return FloatCollectionOutput(collection=param_list)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/params.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/params.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,70 +1,66 @@
 # Copyright (c) 2023 Kyle Schouviller (https://github.com/kyle0654)
 
 from typing import Literal
 
 from pydantic import Field
 
-from .baseinvocation import (BaseInvocation, BaseInvocationOutput,
-                             InvocationConfig, InvocationContext)
+from .baseinvocation import BaseInvocation, BaseInvocationOutput, InvocationConfig, InvocationContext
 from .math import FloatOutput, IntOutput
 
 # Pass-through parameter nodes - used by subgraphs
 
+
 class ParamIntInvocation(BaseInvocation):
     """An integer parameter"""
-    #fmt: off
+
+    # fmt: off
     type: Literal["param_int"] = "param_int"
     a: int = Field(default=0, description="The integer value")
-    #fmt: on
+    # fmt: on
 
     class Config(InvocationConfig):
-      schema_extra = {
-          "ui": {
-              "tags": ["param", "integer"],
-              "title": "Integer Parameter"
-          },
-      }
+        schema_extra = {
+            "ui": {"tags": ["param", "integer"], "title": "Integer Parameter"},
+        }
 
     def invoke(self, context: InvocationContext) -> IntOutput:
         return IntOutput(a=self.a)
 
+
 class ParamFloatInvocation(BaseInvocation):
     """A float parameter"""
-    #fmt: off
+
+    # fmt: off
     type: Literal["param_float"] = "param_float"
     param: float = Field(default=0.0, description="The float value")
-    #fmt: on
+    # fmt: on
 
     class Config(InvocationConfig):
-      schema_extra = {
-          "ui": {
-              "tags": ["param", "float"],
-              "title": "Float Parameter"
-          },
-      }
+        schema_extra = {
+            "ui": {"tags": ["param", "float"], "title": "Float Parameter"},
+        }
 
     def invoke(self, context: InvocationContext) -> FloatOutput:
         return FloatOutput(param=self.param)
 
+
 class StringOutput(BaseInvocationOutput):
     """A string output"""
+
     type: Literal["string_output"] = "string_output"
     text: str = Field(default=None, description="The output string")
 
 
 class ParamStringInvocation(BaseInvocation):
     """A string parameter"""
-    type: Literal['param_string'] = 'param_string'
-    text: str = Field(default='', description='The string value')
+
+    type: Literal["param_string"] = "param_string"
+    text: str = Field(default="", description="The string value")
 
     class Config(InvocationConfig):
-      schema_extra = {
-          "ui": {
-              "tags": ["param", "string"],
-              "title": "String Parameter"
-          },
-      }
+        schema_extra = {
+            "ui": {"tags": ["param", "string"], "title": "String Parameter"},
+        }
 
     def invoke(self, context: InvocationContext) -> StringOutput:
         return StringOutput(text=self.text)
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/prompt.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/prompt.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,27 +3,29 @@
 
 import numpy as np
 from pydantic import Field, validator
 
 from .baseinvocation import BaseInvocation, BaseInvocationOutput, InvocationConfig, InvocationContext
 from dynamicprompts.generators import RandomPromptGenerator, CombinatorialPromptGenerator
 
+
 class PromptOutput(BaseInvocationOutput):
     """Base class for invocations that output a prompt"""
-    #fmt: off
+
+    # fmt: off
     type: Literal["prompt"] = "prompt"
 
     prompt: str = Field(default=None, description="The output prompt")
-    #fmt: on
+    # fmt: on
 
     class Config:
         schema_extra = {
-            'required': [
-                'type',
-                'prompt',
+            "required": [
+                "type",
+                "prompt",
             ]
         }
 
 
 class PromptCollectionOutput(BaseInvocationOutput):
     """Base class for invocations that output a collection of prompts"""
 
@@ -40,56 +42,49 @@
 
 class DynamicPromptInvocation(BaseInvocation):
     """Parses a prompt using adieyal/dynamicprompts' random or combinatorial generator"""
 
     type: Literal["dynamic_prompt"] = "dynamic_prompt"
     prompt: str = Field(description="The prompt to parse with dynamicprompts")
     max_prompts: int = Field(default=1, description="The number of prompts to generate")
-    combinatorial: bool = Field(
-        default=False, description="Whether to use the combinatorial generator"
-    )
+    combinatorial: bool = Field(default=False, description="Whether to use the combinatorial generator")
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Dynamic Prompt",
-                "tags": ["prompt", "dynamic"]
-            },
+            "ui": {"title": "Dynamic Prompt", "tags": ["prompt", "dynamic"]},
         }
 
     def invoke(self, context: InvocationContext) -> PromptCollectionOutput:
         if self.combinatorial:
             generator = CombinatorialPromptGenerator()
             prompts = generator.generate(self.prompt, max_prompts=self.max_prompts)
         else:
             generator = RandomPromptGenerator()
             prompts = generator.generate(self.prompt, num_images=self.max_prompts)
 
         return PromptCollectionOutput(prompt_collection=prompts, count=len(prompts))
-    
+
 
 class PromptsFromFileInvocation(BaseInvocation):
-    '''Loads prompts from a text file'''
+    """Loads prompts from a text file"""
+
     # fmt: off
     type: Literal['prompt_from_file'] = 'prompt_from_file'
 
     # Inputs
     file_path: str = Field(description="Path to prompt text file")
     pre_prompt: Optional[str] = Field(description="String to prepend to each prompt")
     post_prompt: Optional[str] = Field(description="String to append to each prompt")
     start_line: int = Field(default=1, ge=1, description="Line in the file to start start from")
     max_prompts: int = Field(default=1, ge=0, description="Max lines to read from file (0=all)")
-    #fmt: on
+    # fmt: on
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Prompts From File",
-                "tags": ["prompt", "file"]
-            },
+            "ui": {"title": "Prompts From File", "tags": ["prompt", "file"]},
         }
 
     @validator("file_path")
     def file_path_exists(cls, v):
         if not exists(v):
             raise ValueError(FileNotFoundError)
         return v
@@ -99,15 +94,17 @@
         start_line -= 1
         end_line = start_line + max_prompts
         if max_prompts <= 0:
             end_line = np.iinfo(np.int32).max
         with open(file_path) as f:
             for i, line in enumerate(f):
                 if i >= start_line and i < end_line:
-                    prompts.append((pre_prompt or '') + line.strip() + (post_prompt or ''))
+                    prompts.append((pre_prompt or "") + line.strip() + (post_prompt or ""))
                 if i >= end_line:
                     break
         return prompts
 
     def invoke(self, context: InvocationContext) -> PromptCollectionOutput:
-        prompts = self.promptsFromFile(self.file_path, self.pre_prompt, self.post_prompt, self.start_line, self.max_prompts)
+        prompts = self.promptsFromFile(
+            self.file_path, self.pre_prompt, self.post_prompt, self.start_line, self.max_prompts
+        )
         return PromptCollectionOutput(prompt_collection=prompts, count=len(prompts))
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/sdxl.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/sdxl.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,43 +3,46 @@
 from tqdm import tqdm
 from typing import List, Literal, Optional, Union
 
 from pydantic import Field, validator
 
 from ...backend.model_management import ModelType, SubModelType
 from invokeai.app.util.step_callback import stable_diffusion_xl_step_callback
-from .baseinvocation import (BaseInvocation, BaseInvocationOutput,
-                             InvocationConfig, InvocationContext)
+from .baseinvocation import BaseInvocation, BaseInvocationOutput, InvocationConfig, InvocationContext
 
 from .model import UNetField, ClipField, VaeField, MainModelField, ModelInfo
 from .compel import ConditioningField
 from .latent import LatentsField, SAMPLER_NAME_VALUES, LatentsOutput, get_scheduler, build_latents_output
 
+
 class SDXLModelLoaderOutput(BaseInvocationOutput):
     """SDXL base model loader output"""
 
     # fmt: off
     type: Literal["sdxl_model_loader_output"] = "sdxl_model_loader_output"
 
     unet: UNetField = Field(default=None, description="UNet submodel")
     clip: ClipField = Field(default=None, description="Tokenizer and text_encoder submodels")
     clip2: ClipField = Field(default=None, description="Tokenizer and text_encoder submodels")
     vae: VaeField = Field(default=None, description="Vae submodel")
     # fmt: on
 
+
 class SDXLRefinerModelLoaderOutput(BaseInvocationOutput):
     """SDXL refiner model loader output"""
+
     # fmt: off
     type: Literal["sdxl_refiner_model_loader_output"] = "sdxl_refiner_model_loader_output"
     unet: UNetField = Field(default=None, description="UNet submodel")
     clip2: ClipField = Field(default=None, description="Tokenizer and text_encoder submodels")
     vae: VaeField = Field(default=None, description="Vae submodel")
     # fmt: on
-    #fmt: on
-    
+    # fmt: on
+
+
 class SDXLModelLoaderInvocation(BaseInvocation):
     """Loads an sdxl base model, outputting its submodels."""
 
     type: Literal["sdxl_model_loader"] = "sdxl_model_loader"
 
     model: MainModelField = Field(description="The model to load")
     # TODO: precision?
@@ -121,16 +124,18 @@
                     base_model=base_model,
                     model_type=model_type,
                     submodel=SubModelType.Vae,
                 ),
             ),
         )
 
+
 class SDXLRefinerModelLoaderInvocation(BaseInvocation):
     """Loads an sdxl refiner model, outputting its submodels."""
+
     type: Literal["sdxl_refiner_model_loader"] = "sdxl_refiner_model_loader"
 
     model: MainModelField = Field(description="The model to load")
     # TODO: precision?
 
     # Schema customisation
     class Config(InvocationConfig):
@@ -192,15 +197,16 @@
                     model_name=model_name,
                     base_model=base_model,
                     model_type=model_type,
                     submodel=SubModelType.Vae,
                 ),
             ),
         )
-    
+
+
 # Text to image
 class SDXLTextToLatentsInvocation(BaseInvocation):
     """Generates latents from conditionings."""
 
     type: Literal["t2l_sdxl"] = "t2l_sdxl"
 
     # Inputs
@@ -209,42 +215,42 @@
     negative_conditioning: Optional[ConditioningField] = Field(description="Negative conditioning for generation")
     noise: Optional[LatentsField] = Field(description="The noise to use")
     steps:       int = Field(default=10, gt=0, description="The number of steps to use to generate the image")
     cfg_scale: Union[float, List[float]] = Field(default=7.5, ge=1, description="The Classifier-Free Guidance, higher values may result in a result closer to the prompt", )
     scheduler: SAMPLER_NAME_VALUES = Field(default="euler", description="The scheduler to use" )
     unet: UNetField = Field(default=None, description="UNet submodel")
     denoising_end: float = Field(default=1.0, gt=0, le=1, description="")
-    #control: Union[ControlField, list[ControlField]] = Field(default=None, description="The control to use")
-    #seamless:   bool = Field(default=False, description="Whether or not to generate an image that can tile without seams", )
-    #seamless_axes: str = Field(default="", description="The axes to tile the image on, 'x' and/or 'y'")
+    # control: Union[ControlField, list[ControlField]] = Field(default=None, description="The control to use")
+    # seamless:   bool = Field(default=False, description="Whether or not to generate an image that can tile without seams", )
+    # seamless_axes: str = Field(default="", description="The axes to tile the image on, 'x' and/or 'y'")
     # fmt: on
 
     @validator("cfg_scale")
     def ge_one(cls, v):
         """validate that all cfg_scale values are >= 1"""
         if isinstance(v, list):
             for i in v:
                 if i < 1:
-                    raise ValueError('cfg_scale must be greater than 1')
+                    raise ValueError("cfg_scale must be greater than 1")
         else:
             if v < 1:
-                raise ValueError('cfg_scale must be greater than 1')
+                raise ValueError("cfg_scale must be greater than 1")
         return v
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "SDXL Text To Latents",
                 "tags": ["latents"],
                 "type_hints": {
-                  "model": "model",
-                  # "cfg_scale": "float",
-                  "cfg_scale": "number"
-                }
+                    "model": "model",
+                    # "cfg_scale": "float",
+                    "cfg_scale": "number",
+                },
             },
         }
 
     def dispatch_progress(
         self,
         context: InvocationContext,
         source_node_id: str,
@@ -261,17 +267,15 @@
             total_steps=total_steps,
         )
 
     # based on
     # https://github.com/huggingface/diffusers/blob/3ebbaf7c96801271f9e6c21400033b6aa5ffcf29/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py#L375
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> LatentsOutput:
-        graph_execution_state = context.services.graph_execution_manager.get(
-            context.graph_execution_state_id
-        )
+        graph_execution_state = context.services.graph_execution_manager.get(context.graph_execution_state_id)
         source_node_id = graph_execution_state.prepared_source_mapping[self.id]
         latents = context.services.latents.get(self.noise.latents_name)
 
         positive_cond_data = context.services.latents.get(self.positive_conditioning.conditioning_name)
         prompt_embeds = positive_cond_data.conditionings[0].embeds
         pooled_prompt_embeds = positive_cond_data.conditionings[0].pooled_embeds
         add_time_ids = positive_cond_data.conditionings[0].add_time_ids
@@ -289,22 +293,18 @@
 
         num_inference_steps = self.steps
         scheduler.set_timesteps(num_inference_steps)
         timesteps = scheduler.timesteps
 
         latents = latents * scheduler.init_noise_sigma
 
-
-        unet_info = context.services.model_manager.get_model(
-            **self.unet.unet.dict(), context=context
-        )
+        unet_info = context.services.model_manager.get_model(**self.unet.unet.dict(), context=context)
         do_classifier_free_guidance = True
         cross_attention_kwargs = None
         with unet_info as unet:
-
             extra_step_kwargs = dict()
             if "eta" in set(inspect.signature(scheduler.step).parameters.keys()):
                 extra_step_kwargs.update(
                     eta=0.0,
                 )
             if "generator" in set(inspect.signature(scheduler.step).parameters.keys()):
                 extra_step_kwargs.update(
@@ -346,49 +346,49 @@
                             return_dict=False,
                         )[0]
 
                         # perform guidance
                         if do_classifier_free_guidance:
                             noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                             noise_pred = noise_pred_uncond + self.cfg_scale * (noise_pred_text - noise_pred_uncond)
-                            #del noise_pred_uncond
-                            #del noise_pred_text
+                            # del noise_pred_uncond
+                            # del noise_pred_text
 
-                        #if do_classifier_free_guidance and guidance_rescale > 0.0:
+                        # if do_classifier_free_guidance and guidance_rescale > 0.0:
                         #    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                         #    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)
 
                         # compute the previous noisy sample x_t -> x_t-1
                         latents = scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]
 
                         # call the callback, if provided
                         if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % scheduler.order == 0):
                             progress_bar.update()
                             self.dispatch_progress(context, source_node_id, latents, i, num_inference_steps)
-                            #if callback is not None and i % callback_steps == 0:
+                            # if callback is not None and i % callback_steps == 0:
                             #    callback(i, t, latents)
             else:
                 negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.to(device=unet.device, dtype=unet.dtype)
                 negative_prompt_embeds = negative_prompt_embeds.to(device=unet.device, dtype=unet.dtype)
                 add_neg_time_ids = add_neg_time_ids.to(device=unet.device, dtype=unet.dtype)
                 pooled_prompt_embeds = pooled_prompt_embeds.to(device=unet.device, dtype=unet.dtype)
                 prompt_embeds = prompt_embeds.to(device=unet.device, dtype=unet.dtype)
                 add_time_ids = add_time_ids.to(device=unet.device, dtype=unet.dtype)
                 latents = latents.to(device=unet.device, dtype=unet.dtype)
 
                 with tqdm(total=num_inference_steps) as progress_bar:
                     for i, t in enumerate(timesteps):
                         # expand the latents if we are doing classifier free guidance
-                        #latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
+                        # latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
 
                         latent_model_input = scheduler.scale_model_input(latents, t)
 
-                        #import gc
-                        #gc.collect()
-                        #torch.cuda.empty_cache()
+                        # import gc
+                        # gc.collect()
+                        # torch.cuda.empty_cache()
 
                         # predict the noise residual
 
                         added_cond_kwargs = {"text_embeds": negative_pooled_prompt_embeds, "time_ids": add_neg_time_ids}
                         noise_pred_uncond = unet(
                             latent_model_input,
                             t,
@@ -407,50 +407,49 @@
                             added_cond_kwargs=added_cond_kwargs,
                             return_dict=False,
                         )[0]
 
                         # perform guidance
                         noise_pred = noise_pred_uncond + self.cfg_scale * (noise_pred_text - noise_pred_uncond)
 
-                        #del noise_pred_text
-                        #del noise_pred_uncond
-                        #import gc
-                        #gc.collect()
-                        #torch.cuda.empty_cache()
+                        # del noise_pred_text
+                        # del noise_pred_uncond
+                        # import gc
+                        # gc.collect()
+                        # torch.cuda.empty_cache()
 
-                        #if do_classifier_free_guidance and guidance_rescale > 0.0:
+                        # if do_classifier_free_guidance and guidance_rescale > 0.0:
                         #    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                         #    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)
 
                         # compute the previous noisy sample x_t -> x_t-1
                         latents = scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]
 
-                        #del noise_pred
-                        #import gc
-                        #gc.collect()
-                        #torch.cuda.empty_cache()
+                        # del noise_pred
+                        # import gc
+                        # gc.collect()
+                        # torch.cuda.empty_cache()
 
                         # call the callback, if provided
                         if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % scheduler.order == 0):
                             progress_bar.update()
                             self.dispatch_progress(context, source_node_id, latents, i, num_inference_steps)
-                            #if callback is not None and i % callback_steps == 0:
+                            # if callback is not None and i % callback_steps == 0:
                             #    callback(i, t, latents)
 
-
-
         #################
 
         latents = latents.to("cpu")
         torch.cuda.empty_cache()
 
-        name = f'{context.graph_execution_state_id}__{self.id}'
+        name = f"{context.graph_execution_state_id}__{self.id}"
         context.services.latents.save(name, latents)
         return build_latents_output(latents_name=name, latents=latents)
 
+
 class SDXLLatentsToLatentsInvocation(BaseInvocation):
     """Generates latents from conditionings."""
 
     type: Literal["l2l_sdxl"] = "l2l_sdxl"
 
     # Inputs
     # fmt: off
@@ -462,42 +461,42 @@
     scheduler: SAMPLER_NAME_VALUES = Field(default="euler", description="The scheduler to use" )
     unet: UNetField = Field(default=None, description="UNet submodel")
     latents: Optional[LatentsField] = Field(description="Initial latents")
 
     denoising_start: float = Field(default=0.0, ge=0, le=1, description="")
     denoising_end: float = Field(default=1.0, ge=0, le=1, description="")
 
-    #control: Union[ControlField, list[ControlField]] = Field(default=None, description="The control to use")
-    #seamless:   bool = Field(default=False, description="Whether or not to generate an image that can tile without seams", )
-    #seamless_axes: str = Field(default="", description="The axes to tile the image on, 'x' and/or 'y'")
+    # control: Union[ControlField, list[ControlField]] = Field(default=None, description="The control to use")
+    # seamless:   bool = Field(default=False, description="Whether or not to generate an image that can tile without seams", )
+    # seamless_axes: str = Field(default="", description="The axes to tile the image on, 'x' and/or 'y'")
     # fmt: on
 
     @validator("cfg_scale")
     def ge_one(cls, v):
         """validate that all cfg_scale values are >= 1"""
         if isinstance(v, list):
             for i in v:
                 if i < 1:
-                    raise ValueError('cfg_scale must be greater than 1')
+                    raise ValueError("cfg_scale must be greater than 1")
         else:
             if v < 1:
-                raise ValueError('cfg_scale must be greater than 1')
+                raise ValueError("cfg_scale must be greater than 1")
         return v
 
     # Schema customisation
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "title": "SDXL Latents to Latents",
                 "tags": ["latents"],
                 "type_hints": {
-                  "model": "model",
-                  # "cfg_scale": "float",
-                  "cfg_scale": "number"
-                }
+                    "model": "model",
+                    # "cfg_scale": "float",
+                    "cfg_scale": "number",
+                },
             },
         }
 
     def dispatch_progress(
         self,
         context: InvocationContext,
         source_node_id: str,
@@ -514,17 +513,15 @@
             total_steps=total_steps,
         )
 
     # based on
     # https://github.com/huggingface/diffusers/blob/3ebbaf7c96801271f9e6c21400033b6aa5ffcf29/src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py#L375
     @torch.no_grad()
     def invoke(self, context: InvocationContext) -> LatentsOutput:
-        graph_execution_state = context.services.graph_execution_manager.get(
-            context.graph_execution_state_id
-        )
+        graph_execution_state = context.services.graph_execution_manager.get(context.graph_execution_state_id)
         source_node_id = graph_execution_state.prepared_source_mapping[self.id]
         latents = context.services.latents.get(self.latents.latents_name)
 
         positive_cond_data = context.services.latents.get(self.positive_conditioning.conditioning_name)
         prompt_embeds = positive_cond_data.conditionings[0].embeds
         pooled_prompt_embeds = positive_cond_data.conditionings[0].pooled_embeds
         add_time_ids = positive_cond_data.conditionings[0].add_time_ids
@@ -541,30 +538,30 @@
         )
 
         # apply denoising_start
         num_inference_steps = self.steps
         scheduler.set_timesteps(num_inference_steps)
 
         t_start = int(round(self.denoising_start * num_inference_steps))
-        timesteps = scheduler.timesteps[t_start * scheduler.order:]
+        timesteps = scheduler.timesteps[t_start * scheduler.order :]
         num_inference_steps = num_inference_steps - t_start
 
         # apply noise(if provided)
         if self.noise is not None and timesteps.shape[0] > 0:
             noise = context.services.latents.get(self.noise.latents_name)
             latents = scheduler.add_noise(latents, noise, timesteps[:1])
             del noise
 
         unet_info = context.services.model_manager.get_model(
-            **self.unet.unet.dict(), context=context,
+            **self.unet.unet.dict(),
+            context=context,
         )
         do_classifier_free_guidance = True
         cross_attention_kwargs = None
         with unet_info as unet:
-
             # apply scheduler extra args
             extra_step_kwargs = dict()
             if "eta" in set(inspect.signature(scheduler.step).parameters.keys()):
                 extra_step_kwargs.update(
                     eta=0.0,
                 )
             if "generator" in set(inspect.signature(scheduler.step).parameters.keys()):
@@ -607,49 +604,49 @@
                             return_dict=False,
                         )[0]
 
                         # perform guidance
                         if do_classifier_free_guidance:
                             noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                             noise_pred = noise_pred_uncond + self.cfg_scale * (noise_pred_text - noise_pred_uncond)
-                            #del noise_pred_uncond
-                            #del noise_pred_text
+                            # del noise_pred_uncond
+                            # del noise_pred_text
 
-                        #if do_classifier_free_guidance and guidance_rescale > 0.0:
+                        # if do_classifier_free_guidance and guidance_rescale > 0.0:
                         #    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                         #    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)
 
                         # compute the previous noisy sample x_t -> x_t-1
                         latents = scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]
 
                         # call the callback, if provided
                         if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % scheduler.order == 0):
                             progress_bar.update()
                             self.dispatch_progress(context, source_node_id, latents, i, num_inference_steps)
-                            #if callback is not None and i % callback_steps == 0:
+                            # if callback is not None and i % callback_steps == 0:
                             #    callback(i, t, latents)
             else:
                 negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.to(device=unet.device, dtype=unet.dtype)
                 negative_prompt_embeds = negative_prompt_embeds.to(device=unet.device, dtype=unet.dtype)
                 add_neg_time_ids = add_neg_time_ids.to(device=unet.device, dtype=unet.dtype)
                 pooled_prompt_embeds = pooled_prompt_embeds.to(device=unet.device, dtype=unet.dtype)
                 prompt_embeds = prompt_embeds.to(device=unet.device, dtype=unet.dtype)
                 add_time_ids = add_time_ids.to(device=unet.device, dtype=unet.dtype)
                 latents = latents.to(device=unet.device, dtype=unet.dtype)
 
                 with tqdm(total=num_inference_steps) as progress_bar:
                     for i, t in enumerate(timesteps):
                         # expand the latents if we are doing classifier free guidance
-                        #latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
+                        # latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
 
                         latent_model_input = scheduler.scale_model_input(latents, t)
 
-                        #import gc
-                        #gc.collect()
-                        #torch.cuda.empty_cache()
+                        # import gc
+                        # gc.collect()
+                        # torch.cuda.empty_cache()
 
                         # predict the noise residual
 
                         added_cond_kwargs = {"text_embeds": negative_pooled_prompt_embeds, "time_ids": add_time_ids}
                         noise_pred_uncond = unet(
                             latent_model_input,
                             t,
@@ -668,42 +665,40 @@
                             added_cond_kwargs=added_cond_kwargs,
                             return_dict=False,
                         )[0]
 
                         # perform guidance
                         noise_pred = noise_pred_uncond + self.cfg_scale * (noise_pred_text - noise_pred_uncond)
 
-                        #del noise_pred_text
-                        #del noise_pred_uncond
-                        #import gc
-                        #gc.collect()
-                        #torch.cuda.empty_cache()
+                        # del noise_pred_text
+                        # del noise_pred_uncond
+                        # import gc
+                        # gc.collect()
+                        # torch.cuda.empty_cache()
 
-                        #if do_classifier_free_guidance and guidance_rescale > 0.0:
+                        # if do_classifier_free_guidance and guidance_rescale > 0.0:
                         #    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                         #    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)
 
                         # compute the previous noisy sample x_t -> x_t-1
                         latents = scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]
 
-                        #del noise_pred
-                        #import gc
-                        #gc.collect()
-                        #torch.cuda.empty_cache()
+                        # del noise_pred
+                        # import gc
+                        # gc.collect()
+                        # torch.cuda.empty_cache()
 
                         # call the callback, if provided
                         if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % scheduler.order == 0):
                             progress_bar.update()
                             self.dispatch_progress(context, source_node_id, latents, i, num_inference_steps)
-                            #if callback is not None and i % callback_steps == 0:
+                            # if callback is not None and i % callback_steps == 0:
                             #    callback(i, t, latents)
 
-
-
         #################
 
         latents = latents.to("cpu")
         torch.cuda.empty_cache()
 
-        name = f'{context.graph_execution_state_id}__{self.id}'
+        name = f"{context.graph_execution_state_id}__{self.id}"
         context.services.latents.save(name, latents)
         return build_latents_output(latents_name=name, latents=latents)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/invocations/upscale.py` & `InvokeAI-3.0.1rc2/invokeai/app/invocations/upscale.py`

 * *Files 8% similar despite different names*

```diff
@@ -25,24 +25,19 @@
 
 
 class ESRGANInvocation(BaseInvocation):
     """Upscales an image using RealESRGAN."""
 
     type: Literal["esrgan"] = "esrgan"
     image: Union[ImageField, None] = Field(default=None, description="The input image")
-    model_name: ESRGAN_MODELS = Field(
-        default="RealESRGAN_x4plus.pth", description="The Real-ESRGAN model to use"
-    )
+    model_name: ESRGAN_MODELS = Field(default="RealESRGAN_x4plus.pth", description="The Real-ESRGAN model to use")
 
     class Config(InvocationConfig):
         schema_extra = {
-            "ui": {
-                "title": "Upscale (RealESRGAN)",
-                "tags": ["image", "upscale", "realesrgan"]
-            },
+            "ui": {"title": "Upscale (RealESRGAN)", "tags": ["image", "upscale", "realesrgan"]},
         }
 
     def invoke(self, context: InvocationContext) -> ImageOutput:
         image = context.services.images.get_pil_image(self.image.image_name)
         models_path = context.services.configuration.models_path
 
         rrdbnet_model = None
@@ -104,17 +99,15 @@
 
         # We can pass an `outscale` value here, but it just resizes the image by that factor after
         # upscaling, so it's kinda pointless for our purposes. If you want something other than 4x
         # upscaling, you'll need to add a resize node after this one.
         upscaled_image, img_mode = upsampler.enhance(cv_image)
 
         # back to PIL
-        pil_image = Image.fromarray(
-            cv.cvtColor(upscaled_image, cv.COLOR_BGR2RGB)
-        ).convert("RGBA")
+        pil_image = Image.fromarray(cv.cvtColor(upscaled_image, cv.COLOR_BGR2RGB)).convert("RGBA")
 
         image_dto = context.services.images.create(
             image=pil_image,
             image_origin=ResourceOrigin.INTERNAL,
             image_category=ImageCategory.GENERAL,
             node_id=self.id,
             session_id=context.graph_execution_state_id,
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/models/image.py` & `InvokeAI-3.0.1rc2/invokeai/app/models/image.py`

 * *Files 0% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 
 from invokeai.app.util.metaenum import MetaEnum
 from ..invocations.baseinvocation import (
     BaseInvocationOutput,
     InvocationConfig,
 )
 
+
 class ImageField(BaseModel):
     """An image field used for passing image objects between invocations"""
 
     image_name: Optional[str] = Field(default=None, description="The name of the image")
 
     class Config:
         schema_extra = {"required": ["image_name"]}
@@ -30,24 +31,26 @@
 class ProgressImage(BaseModel):
     """The progress image sent intermittently during processing"""
 
     width: int = Field(description="The effective width of the image in pixels")
     height: int = Field(description="The effective height of the image in pixels")
     dataURL: str = Field(description="The image data as a b64 data URL")
 
+
 class PILInvocationConfig(BaseModel):
     """Helper class to provide all PIL invocations with additional config"""
 
     class Config(InvocationConfig):
         schema_extra = {
             "ui": {
                 "tags": ["PIL", "image"],
             },
         }
 
+
 class ImageOutput(BaseInvocationOutput):
     """Base class for invocations that output an image"""
 
     # fmt: off
     type: Literal["image_output"] = "image_output"
     image:      ImageField = Field(default=None, description="The output image")
     width:             int = Field(description="The width of the image in pixels")
@@ -72,14 +75,15 @@
         schema_extra = {
             "required": [
                 "type",
                 "mask",
             ]
         }
 
+
 class ResourceOrigin(str, Enum, metaclass=MetaEnum):
     """The origin of a resource (eg image).
 
     - INTERNAL: The resource was created by the application.
     - EXTERNAL: The resource was not created by the application.
     This may be a user-initiated upload, or an internal application upload (eg Canvas init image).
     """
@@ -128,9 +132,7 @@
     """Raised when a provided value is not a valid ImageCategory.
 
     Subclasses `ValueError`.
     """
 
     def __init__(self, message="Invalid image category."):
         super().__init__(message)
-
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/board_image_record_storage.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/board_image_record_storage.py`

 * *Files 1% similar despite different names*

```diff
@@ -203,17 +203,15 @@
             count = cast(int, self._cursor.fetchone()[0])
 
         except sqlite3.Error as e:
             self._conn.rollback()
             raise e
         finally:
             self._lock.release()
-        return OffsetPaginatedResults(
-            items=images, offset=offset, limit=limit, total=count
-        )
+        return OffsetPaginatedResults(items=images, offset=offset, limit=limit, total=count)
 
     def get_all_board_image_names_for_board(self, board_id: str) -> list[str]:
         try:
             self._lock.acquire()
             self._cursor.execute(
                 """--sql
                 SELECT image_name
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/board_images.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/board_images.py`

 * *Files 4% similar despite different names*

```diff
@@ -98,28 +98,24 @@
     ) -> None:
         self._services.board_image_records.remove_image_from_board(board_id, image_name)
 
     def get_all_board_image_names_for_board(
         self,
         board_id: str,
     ) -> list[str]:
-        return self._services.board_image_records.get_all_board_image_names_for_board(
-            board_id
-        )
+        return self._services.board_image_records.get_all_board_image_names_for_board(board_id)
 
     def get_board_for_image(
         self,
         image_name: str,
     ) -> Optional[str]:
         board_id = self._services.board_image_records.get_board_for_image(image_name)
         return board_id
 
 
-def board_record_to_dto(
-    board_record: BoardRecord, cover_image_name: Optional[str], image_count: int
-) -> BoardDTO:
+def board_record_to_dto(board_record: BoardRecord, cover_image_name: Optional[str], image_count: int) -> BoardDTO:
     """Converts a board record to a board DTO."""
     return BoardDTO(
         **board_record.dict(exclude={"cover_image_name"}),
         cover_image_name=cover_image_name,
         image_count=image_count,
     )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/board_record_storage.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/board_record_storage.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,17 +11,15 @@
 )
 
 from pydantic import BaseModel, Field, Extra
 
 
 class BoardChanges(BaseModel, extra=Extra.forbid):
     board_name: Optional[str] = Field(description="The board's new name.")
-    cover_image_name: Optional[str] = Field(
-        description="The name of the board's new cover image."
-    )
+    cover_image_name: Optional[str] = Field(description="The name of the board's new cover image.")
 
 
 class BoardRecordNotFoundException(Exception):
     """Raised when an board record is not found."""
 
     def __init__(self, message="Board record not found"):
         super().__init__(message)
@@ -288,17 +286,15 @@
                 FROM boards
                 WHERE 1=1;
                 """
             )
 
             count = cast(int, self._cursor.fetchone()[0])
 
-            return OffsetPaginatedResults[BoardRecord](
-                items=boards, offset=offset, limit=limit, total=count
-            )
+            return OffsetPaginatedResults[BoardRecord](items=boards, offset=offset, limit=limit, total=count)
 
         except sqlite3.Error as e:
             self._conn.rollback()
             raise e
         finally:
             self._lock.release()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/boards.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/boards.py`

 * *Files 4% similar despite different names*

```diff
@@ -104,82 +104,62 @@
         board_name: str,
     ) -> BoardDTO:
         board_record = self._services.board_records.save(board_name)
         return board_record_to_dto(board_record, None, 0)
 
     def get_dto(self, board_id: str) -> BoardDTO:
         board_record = self._services.board_records.get(board_id)
-        cover_image = self._services.image_records.get_most_recent_image_for_board(
-            board_record.board_id
-        )
+        cover_image = self._services.image_records.get_most_recent_image_for_board(board_record.board_id)
         if cover_image:
             cover_image_name = cover_image.image_name
         else:
             cover_image_name = None
-        image_count = self._services.board_image_records.get_image_count_for_board(
-            board_id
-        )
+        image_count = self._services.board_image_records.get_image_count_for_board(board_id)
         return board_record_to_dto(board_record, cover_image_name, image_count)
 
     def update(
         self,
         board_id: str,
         changes: BoardChanges,
     ) -> BoardDTO:
         board_record = self._services.board_records.update(board_id, changes)
-        cover_image = self._services.image_records.get_most_recent_image_for_board(
-            board_record.board_id
-        )
+        cover_image = self._services.image_records.get_most_recent_image_for_board(board_record.board_id)
         if cover_image:
             cover_image_name = cover_image.image_name
         else:
             cover_image_name = None
 
-        image_count = self._services.board_image_records.get_image_count_for_board(
-            board_id
-        )
+        image_count = self._services.board_image_records.get_image_count_for_board(board_id)
         return board_record_to_dto(board_record, cover_image_name, image_count)
 
     def delete(self, board_id: str) -> None:
         self._services.board_records.delete(board_id)
 
-    def get_many(
-        self, offset: int = 0, limit: int = 10
-    ) -> OffsetPaginatedResults[BoardDTO]:
+    def get_many(self, offset: int = 0, limit: int = 10) -> OffsetPaginatedResults[BoardDTO]:
         board_records = self._services.board_records.get_many(offset, limit)
         board_dtos = []
         for r in board_records.items:
-            cover_image = self._services.image_records.get_most_recent_image_for_board(
-                r.board_id
-            )
+            cover_image = self._services.image_records.get_most_recent_image_for_board(r.board_id)
             if cover_image:
                 cover_image_name = cover_image.image_name
             else:
                 cover_image_name = None
 
-            image_count = self._services.board_image_records.get_image_count_for_board(
-                r.board_id
-            )
+            image_count = self._services.board_image_records.get_image_count_for_board(r.board_id)
             board_dtos.append(board_record_to_dto(r, cover_image_name, image_count))
 
-        return OffsetPaginatedResults[BoardDTO](
-            items=board_dtos, offset=offset, limit=limit, total=len(board_dtos)
-        )
+        return OffsetPaginatedResults[BoardDTO](items=board_dtos, offset=offset, limit=limit, total=len(board_dtos))
 
     def get_all(self) -> list[BoardDTO]:
         board_records = self._services.board_records.get_all()
         board_dtos = []
         for r in board_records:
-            cover_image = self._services.image_records.get_most_recent_image_for_board(
-                r.board_id
-            )
+            cover_image = self._services.image_records.get_most_recent_image_for_board(r.board_id)
             if cover_image:
                 cover_image_name = cover_image.image_name
             else:
                 cover_image_name = None
 
-            image_count = self._services.board_image_records.get_image_count_for_board(
-                r.board_id
-            )
+            image_count = self._services.board_image_records.get_image_count_for_board(r.board_id)
             board_dtos.append(board_record_to_dto(r, cover_image_name, image_count))
 
-        return board_dtos
+        return board_dtos
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/config.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/config.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # Copyright (c) 2023 Lincoln Stein (https://github.com/lstein) and the InvokeAI Development Team
 
-'''Invokeai configuration system.
+"""Invokeai configuration system.
 
 Arguments and fields are taken from the pydantic definition of the
 model.  Defaults can be set by creating a yaml configuration file that
 has a top-level key of "InvokeAI" and subheadings for each of the
 categories returned by `invokeai --help`. The file looks like this:
 
 [file: invokeai.yaml]
@@ -154,143 +154,164 @@
      Paths:
         root: /home/lstein/invokeai-main
         conf_path: configs/models.yaml
         legacy_conf_dir: configs/stable-diffusion
         outdir: outputs
      ...
 
-'''
+"""
 from __future__ import annotations
 import argparse
 import pydoc
 import os
 import sys
 from argparse import ArgumentParser
 from omegaconf import OmegaConf, DictConfig
 from pathlib import Path
 from pydantic import BaseSettings, Field, parse_obj_as
 from typing import ClassVar, Dict, List, Set, Literal, Union, get_origin, get_type_hints, get_args
 
-INIT_FILE = Path('invokeai.yaml')
-MODEL_CORE = Path('models/core')
-DB_FILE   = Path('invokeai.db')
-LEGACY_INIT_FILE = Path('invokeai.init')
+INIT_FILE = Path("invokeai.yaml")
+MODEL_CORE = Path("models/core")
+DB_FILE = Path("invokeai.db")
+LEGACY_INIT_FILE = Path("invokeai.init")
+
 
 class InvokeAISettings(BaseSettings):
-    '''
+    """
     Runtime configuration settings in which default values are
     read from an omegaconf .yaml file.
-    '''
-    initconf             : ClassVar[DictConfig] = None
-    argparse_groups      : ClassVar[Dict] = {}
+    """
+
+    initconf: ClassVar[DictConfig] = None
+    argparse_groups: ClassVar[Dict] = {}
 
-    def parse_args(self, argv: list=sys.argv[1:]):
+    def parse_args(self, argv: list = sys.argv[1:]):
         parser = self.get_parser()
         opt = parser.parse_args(argv)
         for name in self.__fields__:
             if name not in self._excluded():
-                setattr(self, name, getattr(opt,name))
+                setattr(self, name, getattr(opt, name))
 
-    def to_yaml(self)->str:
+    def to_yaml(self) -> str:
         """
         Return a YAML string representing our settings. This can be used
         as the contents of `invokeai.yaml` to restore settings later.
         """
         cls = self.__class__
-        type = get_args(get_type_hints(cls)['type'])[0]
-        field_dict = dict({type:dict()})
-        for name,field in self.__fields__.items():
+        type = get_args(get_type_hints(cls)["type"])[0]
+        field_dict = dict({type: dict()})
+        for name, field in self.__fields__.items():
             if name in cls._excluded_from_yaml():
                 continue
             category = field.field_info.extra.get("category") or "Uncategorized"
-            value = getattr(self,name)
+            value = getattr(self, name)
             if category not in field_dict[type]:
                 field_dict[type][category] = dict()
             # keep paths as strings to make it easier to read
-            field_dict[type][category][name] = str(value) if isinstance(value,Path) else value
+            field_dict[type][category][name] = str(value) if isinstance(value, Path) else value
         conf = OmegaConf.create(field_dict)
         return OmegaConf.to_yaml(conf)
 
     @classmethod
     def add_parser_arguments(cls, parser):
-        if 'type' in get_type_hints(cls):
-            settings_stanza = get_args(get_type_hints(cls)['type'])[0]
+        if "type" in get_type_hints(cls):
+            settings_stanza = get_args(get_type_hints(cls)["type"])[0]
         else:
             settings_stanza = "Uncategorized"
 
-        env_prefix = cls.Config.env_prefix if hasattr(cls.Config,'env_prefix') else settings_stanza.upper()
+        env_prefix = cls.Config.env_prefix if hasattr(cls.Config, "env_prefix") else settings_stanza.upper()
 
-        initconf = cls.initconf.get(settings_stanza) \
-            if cls.initconf and settings_stanza in cls.initconf \
-               else OmegaConf.create()
+        initconf = (
+            cls.initconf.get(settings_stanza)
+            if cls.initconf and settings_stanza in cls.initconf
+            else OmegaConf.create()
+        )
 
         # create an upcase version of the environment in
         # order to achieve case-insensitive environment
         # variables (the way Windows does)
         upcase_environ = dict()
-        for key,value in os.environ.items():
+        for key, value in os.environ.items():
             upcase_environ[key.upper()] = value
 
         fields = cls.__fields__
         cls.argparse_groups = {}
 
         for name, field in fields.items():
             if name not in cls._excluded():
                 current_default = field.default
 
-                category = field.field_info.extra.get("category","Uncategorized")
-                env_name = env_prefix + '_' + name
+                category = field.field_info.extra.get("category", "Uncategorized")
+                env_name = env_prefix + "_" + name
                 if category in initconf and name in initconf.get(category):
                     field.default = initconf.get(category).get(name)
                 if env_name.upper() in upcase_environ:
                     field.default = upcase_environ[env_name.upper()]
                 cls.add_field_argument(parser, name, field)
 
                 field.default = current_default
 
     @classmethod
-    def cmd_name(self, command_field: str='type')->str:
+    def cmd_name(self, command_field: str = "type") -> str:
         hints = get_type_hints(self)
         if command_field in hints:
             return get_args(hints[command_field])[0]
         else:
-            return 'Uncategorized'
+            return "Uncategorized"
 
     @classmethod
-    def get_parser(cls)->ArgumentParser:
+    def get_parser(cls) -> ArgumentParser:
         parser = PagingArgumentParser(
             prog=cls.cmd_name(),
             description=cls.__doc__,
         )
         cls.add_parser_arguments(parser)
         return parser
 
     @classmethod
     def add_subparser(cls, parser: argparse.ArgumentParser):
         parser.add_parser(cls.cmd_name(), help=cls.__doc__)
 
     @classmethod
-    def _excluded(self)->List[str]:
+    def _excluded(self) -> List[str]:
         # internal fields that shouldn't be exposed as command line options
-        return ['type','initconf']
-    
+        return ["type", "initconf"]
+
     @classmethod
-    def _excluded_from_yaml(self)->List[str]:
+    def _excluded_from_yaml(self) -> List[str]:
         # combination of deprecated parameters and internal ones that shouldn't be exposed as invokeai.yaml options
-        return ['type','initconf', 'gpu_mem_reserved', 'max_loaded_models', 'version', 'from_file', 'model', 'restore', 'root', 'nsfw_checker']
+        return [
+            "type",
+            "initconf",
+            "gpu_mem_reserved",
+            "max_loaded_models",
+            "version",
+            "from_file",
+            "model",
+            "restore",
+            "root",
+            "nsfw_checker",
+        ]
 
     class Config:
-        env_file_encoding = 'utf-8'
+        env_file_encoding = "utf-8"
         arbitrary_types_allowed = True
         case_sensitive = True
 
     @classmethod
-    def add_field_argument(cls, command_parser, name: str, field, default_override = None):
+    def add_field_argument(cls, command_parser, name: str, field, default_override=None):
         field_type = get_type_hints(cls).get(name)
-        default = default_override if default_override is not None else field.default if field.default_factory is None else field.default_factory()
+        default = (
+            default_override
+            if default_override is not None
+            else field.default
+            if field.default_factory is None
+            else field.default_factory()
+        )
         if category := field.field_info.extra.get("category"):
             if category not in cls.argparse_groups:
                 cls.argparse_groups[category] = command_parser.add_argument_group(category)
             argparse_group = cls.argparse_groups[category]
         else:
             argparse_group = command_parser
 
@@ -311,51 +332,55 @@
                 help=field.field_info.description,
             )
 
         elif get_origin(field_type) == list:
             argparse_group.add_argument(
                 f"--{name}",
                 dest=name,
-                nargs='*',
+                nargs="*",
                 type=field.type_,
                 default=default,
-                action=argparse.BooleanOptionalAction if field.type_==bool else 'store',
+                action=argparse.BooleanOptionalAction if field.type_ == bool else "store",
                 help=field.field_info.description,
             )
         else:
             argparse_group.add_argument(
                 f"--{name}",
                 dest=name,
                 type=field.type_,
                 default=default,
-                action=argparse.BooleanOptionalAction if field.type_==bool else 'store',
+                action=argparse.BooleanOptionalAction if field.type_ == bool else "store",
                 help=field.field_info.description,
             )
-def _find_root()->Path:
+
+
+def _find_root() -> Path:
     venv = Path(os.environ.get("VIRTUAL_ENV") or ".")
     if os.environ.get("INVOKEAI_ROOT"):
         root = Path(os.environ.get("INVOKEAI_ROOT")).resolve()
-    elif any([(venv.parent/x).exists() for x in [INIT_FILE, LEGACY_INIT_FILE, MODEL_CORE]]):
+    elif any([(venv.parent / x).exists() for x in [INIT_FILE, LEGACY_INIT_FILE, MODEL_CORE]]):
         root = (venv.parent).resolve()
     else:
         root = Path("~/invokeai").expanduser().resolve()
     return root
 
+
 class InvokeAIAppConfig(InvokeAISettings):
-    '''
-Generate images using Stable Diffusion. Use "invokeai" to launch
-the command-line client (recommended for experts only), or
-"invokeai-web" to launch the web server. Global options
-can be changed by editing the file "INVOKEAI_ROOT/invokeai.yaml" or by
-setting environment variables INVOKEAI_<setting>.
-     '''
+    """
+    Generate images using Stable Diffusion. Use "invokeai" to launch
+    the command-line client (recommended for experts only), or
+    "invokeai-web" to launch the web server. Global options
+    can be changed by editing the file "INVOKEAI_ROOT/invokeai.yaml" or by
+    setting environment variables INVOKEAI_<setting>.
+    """
+
     singleton_config: ClassVar[InvokeAIAppConfig] = None
     singleton_init: ClassVar[Dict] = None
 
-    #fmt: off
+    # fmt: off
     type: Literal["InvokeAI"] = "InvokeAI"
     host                : str = Field(default="127.0.0.1", description="IP address to bind to", category='Web Server')
     port                : int = Field(default=9090, description="Port to bind to", category='Web Server')
     allow_origins       : List[str] = Field(default=[], description="Allowed CORS origins", category='Web Server')
     allow_credentials   : bool = Field(default=True, description="Allow CORS credentials", category='Web Server')
     allow_methods       : List[str] = Field(default=["*"], description="Methods allowed for CORS", category='Web Server')
     allow_headers       : List[str] = Field(default=["*"], description="Headers allowed for CORS", category='Web Server')
@@ -395,24 +420,24 @@
 
     log_handlers        : List[str] = Field(default=["console"], description='Log handler. Valid options are "console", "file=<path>", "syslog=path|address:host:port", "http=<url>"', category="Logging")
     # note - would be better to read the log_format values from logging.py, but this creates circular dependencies issues
     log_format          : Literal[tuple(['plain','color','syslog','legacy'])] = Field(default="color", description='Log format. Use "plain" for text-only, "color" for colorized output, "legacy" for 2.3-style logging and "syslog" for syslog-style', category="Logging")
     log_level           : Literal[tuple(["debug","info","warning","error","critical"])] = Field(default="info", description="Emit logging messages at this level or  higher", category="Logging")
 
     version             : bool = Field(default=False, description="Show InvokeAI version and exit", category="Other")
-    #fmt: on
+    # fmt: on
 
-    def parse_args(self, argv: List[str]=None, conf: DictConfig = None, clobber=False):
-        '''
+    def parse_args(self, argv: List[str] = None, conf: DictConfig = None, clobber=False):
+        """
         Update settings with contents of init file, environment, and
         command-line settings.
         :param conf: alternate Omegaconf dictionary object
         :param argv: aternate sys.argv list
         :param clobber: ovewrite any initialization parameters passed during initialization
-        '''
+        """
         # Set the runtime root directory. We parse command-line switches here
         # in order to pick up the --root_dir option.
         super().parse_args(argv)
         if conf is None:
             try:
                 conf = OmegaConf.load(self.root_dir / INIT_FILE)
             except:
@@ -421,139 +446,143 @@
 
         # parse args again in order to pick up settings in configuration file
         super().parse_args(argv)
 
         if self.singleton_init and not clobber:
             hints = get_type_hints(self.__class__)
             for k in self.singleton_init:
-                setattr(self,k,parse_obj_as(hints[k],self.singleton_init[k]))
+                setattr(self, k, parse_obj_as(hints[k], self.singleton_init[k]))
 
     @classmethod
-    def get_config(cls,**kwargs)->InvokeAIAppConfig:
-        '''
+    def get_config(cls, **kwargs) -> InvokeAIAppConfig:
+        """
         This returns a singleton InvokeAIAppConfig configuration object.
-        '''
-        if cls.singleton_config is None \
-           or type(cls.singleton_config)!=cls \
-           or (kwargs and cls.singleton_init != kwargs):
+        """
+        if (
+            cls.singleton_config is None
+            or type(cls.singleton_config) != cls
+            or (kwargs and cls.singleton_init != kwargs)
+        ):
             cls.singleton_config = cls(**kwargs)
             cls.singleton_init = kwargs
         return cls.singleton_config
 
     @property
-    def root_path(self)->Path:
-        '''
+    def root_path(self) -> Path:
+        """
         Path to the runtime root directory
-        '''
+        """
         if self.root:
             return Path(self.root).expanduser().absolute()
         else:
             return self.find_root()
 
     @property
-    def root_dir(self)->Path:
-        '''
+    def root_dir(self) -> Path:
+        """
         Alias for above.
-        '''
+        """
         return self.root_path
 
-    def _resolve(self,partial_path:Path)->Path:
+    def _resolve(self, partial_path: Path) -> Path:
         return (self.root_path / partial_path).resolve()
 
     @property
-    def init_file_path(self)->Path:
-        '''
+    def init_file_path(self) -> Path:
+        """
         Path to invokeai.yaml
-        '''
+        """
         return self._resolve(INIT_FILE)
 
     @property
-    def output_path(self)->Path:
-        '''
+    def output_path(self) -> Path:
+        """
         Path to defaults outputs directory.
-        '''
+        """
         return self._resolve(self.outdir)
 
     @property
-    def db_path(self)->Path:
-        '''
+    def db_path(self) -> Path:
+        """
         Path to the invokeai.db file.
-        '''
+        """
         return self._resolve(self.db_dir) / DB_FILE
 
     @property
-    def model_conf_path(self)->Path:
-        '''
+    def model_conf_path(self) -> Path:
+        """
         Path to models configuration file.
-        '''
+        """
         return self._resolve(self.conf_path)
 
     @property
-    def legacy_conf_path(self)->Path:
-        '''
+    def legacy_conf_path(self) -> Path:
+        """
         Path to directory of legacy configuration files (e.g. v1-inference.yaml)
-        '''
+        """
         return self._resolve(self.legacy_conf_dir)
 
     @property
-    def models_path(self)->Path:
-        '''
+    def models_path(self) -> Path:
+        """
         Path to the models directory
-        '''
+        """
         return self._resolve(self.models_dir)
 
     @property
-    def autoconvert_path(self)->Path:
-        '''
+    def autoconvert_path(self) -> Path:
+        """
         Path to the directory containing models to be imported automatically at startup.
-        '''
+        """
         return self._resolve(self.autoconvert_dir) if self.autoconvert_dir else None
 
     # the following methods support legacy calls leftover from the Globals era
     @property
-    def full_precision(self)->bool:
+    def full_precision(self) -> bool:
         """Return true if precision set to float32"""
-        return self.precision=='float32'
+        return self.precision == "float32"
 
     @property
-    def disable_xformers(self)->bool:
+    def disable_xformers(self) -> bool:
         """Return true if xformers_enabled is false"""
         return not self.xformers_enabled
 
     @property
-    def try_patchmatch(self)->bool:
+    def try_patchmatch(self) -> bool:
         """Return true if patchmatch true"""
         return self.patchmatch
 
     @property
-    def nsfw_checker(self)->bool:
-        """ NSFW node is always active and disabled from Web UIe"""
+    def nsfw_checker(self) -> bool:
+        """NSFW node is always active and disabled from Web UIe"""
         return True
 
     @property
-    def invisible_watermark(self)->bool:
-        """ invisible watermark node is always active and disabled from Web UIe"""
+    def invisible_watermark(self) -> bool:
+        """invisible watermark node is always active and disabled from Web UIe"""
         return True
-    
+
     @staticmethod
-    def find_root()->Path:
-        '''
+    def find_root() -> Path:
+        """
         Choose the runtime root directory when not specified on command line or
         init file.
-        '''
+        """
         return _find_root()
 
 
 class PagingArgumentParser(argparse.ArgumentParser):
-    '''
+    """
     A custom ArgumentParser that uses pydoc to page its output.
     It also supports reading defaults from an init file.
-    '''
+    """
+
     def print_help(self, file=None):
         text = self.format_help()
         pydoc.pager(text)
 
-def get_invokeai_config(**kwargs)->InvokeAIAppConfig:
-    '''
+
+def get_invokeai_config(**kwargs) -> InvokeAIAppConfig:
+    """
     Legacy function which returns InvokeAIAppConfig.get_config()
-    '''
+    """
     return InvokeAIAppConfig.get_config(**kwargs)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/events.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/events.py`

 * *Files 1% similar despite different names*

```diff
@@ -40,17 +40,15 @@
         """Emitted when there is generation progress"""
         self.__emit_session_event(
             event_name="generator_progress",
             payload=dict(
                 graph_execution_state_id=graph_execution_state_id,
                 node=node,
                 source_node_id=source_node_id,
-                progress_image=progress_image.dict()
-                if progress_image is not None
-                else None,
+                progress_image=progress_image.dict() if progress_image is not None else None,
                 step=step,
                 total_steps=total_steps,
             ),
         )
 
     def emit_invocation_complete(
         self,
@@ -86,17 +84,15 @@
                 node=node,
                 source_node_id=source_node_id,
                 error_type=error_type,
                 error=error,
             ),
         )
 
-    def emit_invocation_started(
-        self, graph_execution_state_id: str, node: dict, source_node_id: str
-    ) -> None:
+    def emit_invocation_started(self, graph_execution_state_id: str, node: dict, source_node_id: str) -> None:
         """Emitted when an invocation has started"""
         self.__emit_session_event(
             event_name="invocation_started",
             payload=dict(
                 graph_execution_state_id=graph_execution_state_id,
                 node=node,
                 source_node_id=source_node_id,
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/graph.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/graph.py`

 * *Files 4% similar despite different names*

```diff
@@ -24,14 +24,15 @@
     BaseInvocationOutput,
     InvocationContext,
 )
 
 # in 3.10 this would be "from types import NoneType"
 NoneType = type(None)
 
+
 class EdgeConnection(BaseModel):
     node_id: str = Field(description="The id of the node for this edge connection")
     field: str = Field(description="The field for this connection")
 
     def __eq__(self, other):
         return (
             isinstance(other, self.__class__)
@@ -57,24 +58,26 @@
 
 def get_input_field(node: BaseInvocation, field: str) -> Any:
     node_type = type(node)
     node_inputs = get_type_hints(node_type)
     node_input_field = node_inputs.get(field) or None
     return node_input_field
 
+
 def is_union_subtype(t1, t2):
     t1_args = get_args(t1)
     t2_args = get_args(t2)
     if not t1_args:
         # t1 is a single type
         return t1 in t2_args
     else:
         # t1 is a Union, check that all of its types are in t2_args
         return all(arg in t2_args for arg in t1_args)
 
+
 def is_list_or_contains_list(t):
     t_args = get_args(t)
 
     # If the type is a List
     if get_origin(t) is list:
         return True
 
@@ -150,23 +153,25 @@
 
 # TODO: Create and use an Empty output?
 class GraphInvocationOutput(BaseInvocationOutput):
     type: Literal["graph_output"] = "graph_output"
 
     class Config:
         schema_extra = {
-            'required': [
-                'type',
-                'image',
+            "required": [
+                "type",
+                "image",
             ]
         }
 
+
 # TODO: Fill this out and move to invocations
 class GraphInvocation(BaseInvocation):
     """Execute a graph"""
+
     type: Literal["graph"] = "graph"
 
     # TODO: figure out how to create a default here
     graph: "Graph" = Field(description="The graph to run", default=None)
 
     def invoke(self, context: InvocationContext) -> GraphInvocationOutput:
         """Invoke with provided services and return outputs."""
@@ -178,50 +183,49 @@
 
     type: Literal["iterate_output"] = "iterate_output"
 
     item: Any = Field(description="The item being iterated over")
 
     class Config:
         schema_extra = {
-            'required': [
-                'type',
-                'item',
+            "required": [
+                "type",
+                "item",
             ]
         }
 
+
 # TODO: Fill this out and move to invocations
 class IterateInvocation(BaseInvocation):
     """Iterates over a list of items"""
+
     type: Literal["iterate"] = "iterate"
 
-    collection: list[Any] = Field(
-        description="The list of items to iterate over", default_factory=list
-    )
-    index: int = Field(
-        description="The index, will be provided on executed iterators", default=0
-    )
+    collection: list[Any] = Field(description="The list of items to iterate over", default_factory=list)
+    index: int = Field(description="The index, will be provided on executed iterators", default=0)
 
     def invoke(self, context: InvocationContext) -> IterateInvocationOutput:
         """Produces the outputs as values"""
         return IterateInvocationOutput(item=self.collection[self.index])
 
 
 class CollectInvocationOutput(BaseInvocationOutput):
     type: Literal["collect_output"] = "collect_output"
 
     collection: list[Any] = Field(description="The collection of input items")
 
     class Config:
         schema_extra = {
-            'required': [
-                'type',
-                'collection',
+            "required": [
+                "type",
+                "collection",
             ]
         }
 
+
 class CollectInvocation(BaseInvocation):
     """Collects values into a collection"""
 
     type: Literal["collect"] = "collect"
 
     item: Any = Field(
         description="The item to collect (all inputs must be of the same type)",
@@ -265,17 +269,15 @@
 
     def _get_graph_and_node(self, node_path: str) -> tuple["Graph", str]:
         """Returns the graph and node id for a node path."""
         # Materialized graphs may have nodes at the top level
         if node_path in self.nodes:
             return (self, node_path)
 
-        node_id = (
-            node_path if "." not in node_path else node_path[: node_path.index(".")]
-        )
+        node_id = node_path if "." not in node_path else node_path[: node_path.index(".")]
         if node_id not in self.nodes:
             raise NodeNotFoundError(f"Node {node_path} not found in graph")
 
         node = self.nodes[node_id]
 
         if not isinstance(node, GraphInvocation):
             # There's more node path left but this isn't a graph - failure
@@ -329,17 +331,15 @@
 
         # Validate all subgraphs
         for gn in (n for n in self.nodes.values() if isinstance(n, GraphInvocation)):
             if not gn.graph.is_valid():
                 return False
 
         # Validate all edges reference nodes in the graph
-        node_ids = set(
-            [e.source.node_id for e in self.edges] + [e.destination.node_id for e in self.edges]
-        )
+        node_ids = set([e.source.node_id for e in self.edges] + [e.destination.node_id for e in self.edges])
         if not all((self.has_node(node_id) for node_id in node_ids)):
             return False
 
         # Validate there are no cycles
         g = self.nx_graph_flat()
         if not nx.is_directed_acyclic_graph(g):
             return False
@@ -357,30 +357,22 @@
             )
         ):
             return False
 
         # Validate all iterators
         # TODO: may need to validate all iterators in subgraphs so edge connections in parent graphs will be available
         if not all(
-            (
-                self._is_iterator_connection_valid(n.id)
-                for n in self.nodes.values()
-                if isinstance(n, IterateInvocation)
-            )
+            (self._is_iterator_connection_valid(n.id) for n in self.nodes.values() if isinstance(n, IterateInvocation))
         ):
             return False
 
         # Validate all collectors
         # TODO: may need to validate all collectors in subgraphs so edge connections in parent graphs will be available
         if not all(
-            (
-                self._is_collector_connection_valid(n.id)
-                for n in self.nodes.values()
-                if isinstance(n, CollectInvocation)
-            )
+            (self._is_collector_connection_valid(n.id) for n in self.nodes.values() if isinstance(n, CollectInvocation))
         ):
             return False
 
         return True
 
     def _validate_edge(self, edge: Edge):
         """Validates that a new edge doesn't create a cycle in the graph"""
@@ -391,56 +383,59 @@
             to_node = self.get_node(edge.destination.node_id)
         except NodeNotFoundError:
             raise InvalidEdgeError("One or both nodes don't exist: {edge.source.node_id} -> {edge.destination.node_id}")
 
         # Validate that an edge to this node+field doesn't already exist
         input_edges = self._get_input_edges(edge.destination.node_id, edge.destination.field)
         if len(input_edges) > 0 and not isinstance(to_node, CollectInvocation):
-            raise InvalidEdgeError(f'Edge to node {edge.destination.node_id} field {edge.destination.field} already exists')
+            raise InvalidEdgeError(
+                f"Edge to node {edge.destination.node_id} field {edge.destination.field} already exists"
+            )
 
         # Validate that no cycles would be created
         g = self.nx_graph_flat()
         g.add_edge(edge.source.node_id, edge.destination.node_id)
         if not nx.is_directed_acyclic_graph(g):
-            raise InvalidEdgeError(f'Edge creates a cycle in the graph: {edge.source.node_id} -> {edge.destination.node_id}')
+            raise InvalidEdgeError(
+                f"Edge creates a cycle in the graph: {edge.source.node_id} -> {edge.destination.node_id}"
+            )
 
         # Validate that the field types are compatible
-        if not are_connections_compatible(
-            from_node, edge.source.field, to_node, edge.destination.field
-        ):
-            raise InvalidEdgeError(f'Fields are incompatible: cannot connect {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}')
+        if not are_connections_compatible(from_node, edge.source.field, to_node, edge.destination.field):
+            raise InvalidEdgeError(
+                f"Fields are incompatible: cannot connect {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}"
+            )
 
         # Validate if iterator output type matches iterator input type (if this edge results in both being set)
         if isinstance(to_node, IterateInvocation) and edge.destination.field == "collection":
-            if not self._is_iterator_connection_valid(
-                edge.destination.node_id, new_input=edge.source
-            ):
-                raise InvalidEdgeError(f'Iterator input type does not match iterator output type: {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}')
+            if not self._is_iterator_connection_valid(edge.destination.node_id, new_input=edge.source):
+                raise InvalidEdgeError(
+                    f"Iterator input type does not match iterator output type: {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}"
+                )
 
         # Validate if iterator input type matches output type (if this edge results in both being set)
         if isinstance(from_node, IterateInvocation) and edge.source.field == "item":
-            if not self._is_iterator_connection_valid(
-                edge.source.node_id, new_output=edge.destination
-            ):
-                raise InvalidEdgeError(f'Iterator output type does not match iterator input type:, {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}')
+            if not self._is_iterator_connection_valid(edge.source.node_id, new_output=edge.destination):
+                raise InvalidEdgeError(
+                    f"Iterator output type does not match iterator input type:, {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}"
+                )
 
         # Validate if collector input type matches output type (if this edge results in both being set)
         if isinstance(to_node, CollectInvocation) and edge.destination.field == "item":
-            if not self._is_collector_connection_valid(
-                edge.destination.node_id, new_input=edge.source
-            ):
-                raise InvalidEdgeError(f'Collector output type does not match collector input type: {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}')
+            if not self._is_collector_connection_valid(edge.destination.node_id, new_input=edge.source):
+                raise InvalidEdgeError(
+                    f"Collector output type does not match collector input type: {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}"
+                )
 
         # Validate if collector output type matches input type (if this edge results in both being set)
         if isinstance(from_node, CollectInvocation) and edge.source.field == "collection":
-            if not self._is_collector_connection_valid(
-                edge.source.node_id, new_output=edge.destination
-            ):
-                raise InvalidEdgeError(f'Collector input type does not match collector output type: {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}')
-
+            if not self._is_collector_connection_valid(edge.source.node_id, new_output=edge.destination):
+                raise InvalidEdgeError(
+                    f"Collector input type does not match collector output type: {edge.source.node_id}.{edge.source.field} to {edge.destination.node_id}.{edge.destination.field}"
+                )
 
     def has_node(self, node_path: str) -> bool:
         """Determines whether or not a node exists in the graph."""
         try:
             n = self.get_node(node_path)
             if n is not None:
                 return True
@@ -461,25 +456,21 @@
     def update_node(self, node_path: str, new_node: BaseInvocation) -> None:
         """Updates a node in the graph."""
         graph, node_id = self._get_graph_and_node(node_path)
         node = graph.nodes[node_id]
 
         # Ensure the node type matches the new node
         if type(node) != type(new_node):
-            raise TypeError(
-                f"Node {node_path} is type {type(node)} but new node is type {type(new_node)}"
-            )
+            raise TypeError(f"Node {node_path} is type {type(node)} but new node is type {type(new_node)}")
 
         # Ensure the new id is either the same or is not in the graph
         prefix = None if "." not in node_path else node_path[: node_path.rindex(".")]
         new_path = self._get_node_path(new_node.id, prefix=prefix)
         if new_node.id != node.id and self.has_node(new_path):
-            raise NodeAlreadyInGraphError(
-                "Node with id {new_node.id} already exists in graph"
-            )
+            raise NodeAlreadyInGraphError("Node with id {new_node.id} already exists in graph")
 
         # Set the new node in the graph
         graph.nodes[new_node.id] = new_node
         if new_node.id != node.id:
             input_edges = self._get_input_edges_and_graphs(node_path)
             output_edges = self._get_output_edges_and_graphs(node_path)
 
@@ -493,39 +484,33 @@
                     new_node.id
                     if "." not in edge.destination.node_id
                     else f'{edge.destination.node_id[edge.destination.node_id.rindex("."):]}.{new_node.id}'
                 )
                 graph.add_edge(
                     Edge(
                         source=edge.source,
-                        destination=EdgeConnection(
-                            node_id=new_graph_node_path, field=edge.destination.field
-                        )
+                        destination=EdgeConnection(node_id=new_graph_node_path, field=edge.destination.field),
                     )
                 )
 
             for graph, _, edge in output_edges:
                 # Remove the graph prefix from the node path
                 new_graph_node_path = (
                     new_node.id
                     if "." not in edge.source.node_id
                     else f'{edge.source.node_id[edge.source.node_id.rindex("."):]}.{new_node.id}'
                 )
                 graph.add_edge(
                     Edge(
-                        source=EdgeConnection(
-                            node_id=new_graph_node_path, field=edge.source.field
-                        ),
-                        destination=edge.destination
+                        source=EdgeConnection(node_id=new_graph_node_path, field=edge.source.field),
+                        destination=edge.destination,
                     )
                 )
 
-    def _get_input_edges(
-        self, node_path: str, field: Optional[str] = None
-    ) -> list[Edge]:
+    def _get_input_edges(self, node_path: str, field: Optional[str] = None) -> list[Edge]:
         """Gets all input edges for a node"""
         edges = self._get_input_edges_and_graphs(node_path)
 
         # Filter to edges that match the field
         filtered_edges = (e for e in edges if field is None or e[2].destination.field == field)
 
         # Create full node paths for each edge
@@ -534,52 +519,40 @@
                 source=EdgeConnection(
                     node_id=self._get_node_path(e.source.node_id, prefix=prefix),
                     field=e.source.field,
                 ),
                 destination=EdgeConnection(
                     node_id=self._get_node_path(e.destination.node_id, prefix=prefix),
                     field=e.destination.field,
-                )
+                ),
             )
             for _, prefix, e in filtered_edges
         ]
 
     def _get_input_edges_and_graphs(
         self, node_path: str, prefix: Optional[str] = None
     ) -> list[tuple["Graph", str, Edge]]:
         """Gets all input edges for a node along with the graph they are in and the graph's path"""
         edges = list()
 
         # Return any input edges that appear in this graph
-        edges.extend(
-            [(self, prefix, e) for e in self.edges if e.destination.node_id == node_path]
-        )
+        edges.extend([(self, prefix, e) for e in self.edges if e.destination.node_id == node_path])
 
-        node_id = (
-            node_path if "." not in node_path else node_path[: node_path.index(".")]
-        )
+        node_id = node_path if "." not in node_path else node_path[: node_path.index(".")]
         node = self.nodes[node_id]
 
         if isinstance(node, GraphInvocation):
             graph = node.graph
-            graph_path = (
-                node.id
-                if prefix is None or prefix == ""
-                else self._get_node_path(node.id, prefix=prefix)
-            )
-            graph_edges = graph._get_input_edges_and_graphs(
-                node_path[(len(node_id) + 1) :], prefix=graph_path
-            )
+            graph_path = node.id if prefix is None or prefix == "" else self._get_node_path(node.id, prefix=prefix)
+            graph_edges = graph._get_input_edges_and_graphs(node_path[(len(node_id) + 1) :], prefix=graph_path)
             edges.extend(graph_edges)
 
         return edges
 
-    def _get_output_edges(
-        self, node_path: str, field: str
-    ) -> list[Edge]:
+    def _get_output_edges(self, node_path: str, field: str) -> list[Edge]:
         """Gets all output edges for a node"""
         edges = self._get_output_edges_and_graphs(node_path)
 
         # Filter to edges that match the field
         filtered_edges = (e for e in edges if e[2].source.field == field)
 
         # Create full node paths for each edge
@@ -588,45 +561,35 @@
                 source=EdgeConnection(
                     node_id=self._get_node_path(e.source.node_id, prefix=prefix),
                     field=e.source.field,
                 ),
                 destination=EdgeConnection(
                     node_id=self._get_node_path(e.destination.node_id, prefix=prefix),
                     field=e.destination.field,
-                )
+                ),
             )
             for _, prefix, e in filtered_edges
         ]
 
     def _get_output_edges_and_graphs(
         self, node_path: str, prefix: Optional[str] = None
     ) -> list[tuple["Graph", str, Edge]]:
         """Gets all output edges for a node along with the graph they are in and the graph's path"""
         edges = list()
 
         # Return any input edges that appear in this graph
-        edges.extend(
-            [(self, prefix, e) for e in self.edges if e.source.node_id == node_path]
-        )
+        edges.extend([(self, prefix, e) for e in self.edges if e.source.node_id == node_path])
 
-        node_id = (
-            node_path if "." not in node_path else node_path[: node_path.index(".")]
-        )
+        node_id = node_path if "." not in node_path else node_path[: node_path.index(".")]
         node = self.nodes[node_id]
 
         if isinstance(node, GraphInvocation):
             graph = node.graph
-            graph_path = (
-                node.id
-                if prefix is None or prefix == ""
-                else self._get_node_path(node.id, prefix=prefix)
-            )
-            graph_edges = graph._get_output_edges_and_graphs(
-                node_path[(len(node_id) + 1) :], prefix=graph_path
-            )
+            graph_path = node.id if prefix is None or prefix == "" else self._get_node_path(node.id, prefix=prefix)
+            graph_edges = graph._get_output_edges_and_graphs(node_path[(len(node_id) + 1) :], prefix=graph_path)
             edges.extend(graph_edges)
 
         return edges
 
     def _is_iterator_connection_valid(
         self,
         node_path: str,
@@ -642,33 +605,24 @@
             outputs.append(new_output)
 
         # Only one input is allowed for iterators
         if len(inputs) > 1:
             return False
 
         # Get input and output fields (the fields linked to the iterator's input/output)
-        input_field = get_output_field(
-            self.get_node(inputs[0].node_id), inputs[0].field
-        )
-        output_fields = list(
-            [get_input_field(self.get_node(e.node_id), e.field) for e in outputs]
-        )
+        input_field = get_output_field(self.get_node(inputs[0].node_id), inputs[0].field)
+        output_fields = list([get_input_field(self.get_node(e.node_id), e.field) for e in outputs])
 
         # Input type must be a list
         if get_origin(input_field) != list:
             return False
 
         # Validate that all outputs match the input type
         input_field_item_type = get_args(input_field)[0]
-        if not all(
-            (
-                are_connection_types_compatible(input_field_item_type, f)
-                for f in output_fields
-            )
-        ):
+        if not all((are_connection_types_compatible(input_field_item_type, f) for f in output_fields)):
             return False
 
         return True
 
     def _is_collector_connection_valid(
         self,
         node_path: str,
@@ -680,43 +634,29 @@
 
         if new_input is not None:
             inputs.append(new_input)
         if new_output is not None:
             outputs.append(new_output)
 
         # Get input and output fields (the fields linked to the iterator's input/output)
-        input_fields = list(
-            [get_output_field(self.get_node(e.node_id), e.field) for e in inputs]
-        )
-        output_fields = list(
-            [get_input_field(self.get_node(e.node_id), e.field) for e in outputs]
-        )
+        input_fields = list([get_output_field(self.get_node(e.node_id), e.field) for e in inputs])
+        output_fields = list([get_input_field(self.get_node(e.node_id), e.field) for e in outputs])
 
         # Validate that all inputs are derived from or match a single type
         input_field_types = set(
             [
                 t
                 for input_field in input_fields
-                for t in (
-                    [input_field]
-                    if get_origin(input_field) == None
-                    else get_args(input_field)
-                )
+                for t in ([input_field] if get_origin(input_field) == None else get_args(input_field))
                 if t != NoneType
             ]
         )  # Get unique types
         type_tree = nx.DiGraph()
         type_tree.add_nodes_from(input_field_types)
-        type_tree.add_edges_from(
-            [
-                e
-                for e in itertools.permutations(input_field_types, 2)
-                if issubclass(e[1], e[0])
-            ]
-        )
+        type_tree.add_edges_from([e for e in itertools.permutations(input_field_types, 2) if issubclass(e[1], e[0])])
         type_degrees = type_tree.in_degree(type_tree.nodes)
         if sum((t[1] == 0 for t in type_degrees)) != 1:  # type: ignore
             return False  # There is more than one root type
 
         # Get the input root type
         input_root_type = next(t[0] for t in type_degrees if t[1] == 0)  # type: ignore
 
@@ -725,17 +665,15 @@
         #     return False
 
         # Verify that all outputs are lists
         if not all(is_list_or_contains_list(f) for f in output_fields):
             return False
 
         # Verify that all outputs match the input type (are a base class or the same class)
-        if not all(
-            (issubclass(input_root_type, get_args(f)[0]) for f in output_fields)
-        ):
+        if not all((issubclass(input_root_type, get_args(f)[0]) for f in output_fields)):
             return False
 
         return True
 
     def nx_graph(self) -> nx.DiGraph:
         """Returns a NetworkX DiGraph representing the layout of this graph"""
         # TODO: Cache this?
@@ -747,45 +685,35 @@
     def nx_graph_with_data(self) -> nx.DiGraph:
         """Returns a NetworkX DiGraph representing the data and layout of this graph"""
         g = nx.DiGraph()
         g.add_nodes_from([n for n in self.nodes.items()])
         g.add_edges_from(set([(e.source.node_id, e.destination.node_id) for e in self.edges]))
         return g
 
-    def nx_graph_flat(
-        self, nx_graph: Optional[nx.DiGraph] = None, prefix: Optional[str] = None
-    ) -> nx.DiGraph:
+    def nx_graph_flat(self, nx_graph: Optional[nx.DiGraph] = None, prefix: Optional[str] = None) -> nx.DiGraph:
         """Returns a flattened NetworkX DiGraph, including all subgraphs (but not with iterations expanded)"""
         g = nx_graph or nx.DiGraph()
 
         # Add all nodes from this graph except graph/iteration nodes
         g.add_nodes_from(
             [
                 self._get_node_path(n.id, prefix)
                 for n in self.nodes.values()
-                if not isinstance(n, GraphInvocation)
-                and not isinstance(n, IterateInvocation)
+                if not isinstance(n, GraphInvocation) and not isinstance(n, IterateInvocation)
             ]
         )
 
         # Expand graph nodes
-        for sgn in (
-            gn for gn in self.nodes.values() if isinstance(gn, GraphInvocation)
-        ):
+        for sgn in (gn for gn in self.nodes.values() if isinstance(gn, GraphInvocation)):
             g = sgn.graph.nx_graph_flat(g, self._get_node_path(sgn.id, prefix))
 
         # TODO: figure out if iteration nodes need to be expanded
 
         unique_edges = set([(e.source.node_id, e.destination.node_id) for e in self.edges])
-        g.add_edges_from(
-            [
-                (self._get_node_path(e[0], prefix), self._get_node_path(e[1], prefix))
-                for e in unique_edges
-            ]
-        )
+        g.add_edges_from([(self._get_node_path(e[0], prefix), self._get_node_path(e[1], prefix)) for e in unique_edges])
         return g
 
 
 class GraphExecutionState(BaseModel):
     """Tracks the state of a graph execution"""
 
     id: str = Field(description="The id of the execution state", default_factory=lambda: uuid.uuid4().__str__())
@@ -796,31 +724,27 @@
     # The graph of materialized nodes
     execution_graph: Graph = Field(
         description="The expanded graph of activated and executed nodes",
         default_factory=Graph,
     )
 
     # Nodes that have been executed
-    executed: set[str] = Field(
-        description="The set of node ids that have been executed", default_factory=set
-    )
+    executed: set[str] = Field(description="The set of node ids that have been executed", default_factory=set)
     executed_history: list[str] = Field(
         description="The list of node ids that have been executed, in order of execution",
         default_factory=list,
     )
 
     # The results of executed nodes
-    results: dict[
-        str, Annotated[InvocationOutputsUnion, Field(discriminator="type")]
-    ] = Field(description="The results of node executions", default_factory=dict)
+    results: dict[str, Annotated[InvocationOutputsUnion, Field(discriminator="type")]] = Field(
+        description="The results of node executions", default_factory=dict
+    )
 
     # Errors raised when executing nodes
-    errors: dict[str, str] = Field(
-        description="Errors raised when executing nodes", default_factory=dict
-    )
+    errors: dict[str, str] = Field(description="Errors raised when executing nodes", default_factory=dict)
 
     # Map of prepared/executed nodes to their original nodes
     prepared_source_mapping: dict[str, str] = Field(
         description="The map of prepared nodes to original graph nodes",
         default_factory=dict,
     )
 
@@ -828,24 +752,24 @@
     source_prepared_mapping: dict[str, set[str]] = Field(
         description="The map of original graph nodes to prepared nodes",
         default_factory=dict,
     )
 
     class Config:
         schema_extra = {
-            'required': [
-                'id',
-                'graph',
-                'execution_graph',
-                'executed',
-                'executed_history',
-                'results',
-                'errors',
-                'prepared_source_mapping',
-                'source_prepared_mapping',
+            "required": [
+                "id",
+                "graph",
+                "execution_graph",
+                "executed",
+                "executed_history",
+                "results",
+                "errors",
+                "prepared_source_mapping",
+                "source_prepared_mapping",
             ]
         }
 
     def next(self) -> Optional[BaseInvocation]:
         """Gets the next node ready to execute."""
 
         # TODO: enable multiple nodes to execute simultaneously by tracking currently executing nodes
@@ -895,57 +819,45 @@
         node_ids = set(self.graph.nx_graph_flat().nodes)
         return self.has_error() or all((k in self.executed for k in node_ids))
 
     def has_error(self) -> bool:
         """Returns true if the graph has any errors"""
         return len(self.errors) > 0
 
-    def _create_execution_node(
-        self, node_path: str, iteration_node_map: list[tuple[str, str]]
-    ) -> list[str]:
+    def _create_execution_node(self, node_path: str, iteration_node_map: list[tuple[str, str]]) -> list[str]:
         """Prepares an iteration node and connects all edges, returning the new node id"""
 
         node = self.graph.get_node(node_path)
 
         self_iteration_count = -1
 
         # If this is an iterator node, we must create a copy for each iteration
         if isinstance(node, IterateInvocation):
             # Get input collection edge (should error if there are no inputs)
-            input_collection_edge = next(
-                iter(self.graph._get_input_edges(node_path, "collection"))
-            )
+            input_collection_edge = next(iter(self.graph._get_input_edges(node_path, "collection")))
             input_collection_prepared_node_id = next(
-                n[1]
-                for n in iteration_node_map
-                if n[0] == input_collection_edge.source.node_id
-            )
-            input_collection_prepared_node_output = self.results[
-                input_collection_prepared_node_id
-            ]
-            input_collection = getattr(
-                input_collection_prepared_node_output, input_collection_edge.source.field
+                n[1] for n in iteration_node_map if n[0] == input_collection_edge.source.node_id
             )
+            input_collection_prepared_node_output = self.results[input_collection_prepared_node_id]
+            input_collection = getattr(input_collection_prepared_node_output, input_collection_edge.source.field)
             self_iteration_count = len(input_collection)
 
         new_nodes = list()
         if self_iteration_count == 0:
             # TODO: should this raise a warning? It might just happen if an empty collection is input, and should be valid.
             return new_nodes
 
         # Get all input edges
         input_edges = self.graph._get_input_edges(node_path)
 
         # Create new edges for this iteration
         # For collect nodes, this may contain multiple inputs to the same field
         new_edges = list()
         for edge in input_edges:
-            for input_node_id in (
-                n[1] for n in iteration_node_map if n[0] == edge.source.node_id
-            ):
+            for input_node_id in (n[1] for n in iteration_node_map if n[0] == edge.source.node_id):
                 new_edge = Edge(
                     source=EdgeConnection(node_id=input_node_id, field=edge.source.field),
                     destination=EdgeConnection(node_id="", field=edge.destination.field),
                 )
                 new_edges.append(new_edge)
 
         # Create a new node (or one for each iteration of this iterator)
@@ -978,31 +890,23 @@
             new_nodes.append(new_node.id)
 
         return new_nodes
 
     def _iterator_graph(self) -> nx.DiGraph:
         """Gets a DiGraph with edges to collectors removed so an ancestor search produces all active iterators for any node"""
         g = self.graph.nx_graph_flat()
-        collectors = (
-            n
-            for n in self.graph.nodes
-            if isinstance(self.graph.get_node(n), CollectInvocation)
-        )
+        collectors = (n for n in self.graph.nodes if isinstance(self.graph.get_node(n), CollectInvocation))
         for c in collectors:
             g.remove_edges_from(list(g.in_edges(c)))
         return g
 
     def _get_node_iterators(self, node_id: str) -> list[str]:
         """Gets iterators for a node"""
         g = self._iterator_graph()
-        iterators = [
-            n
-            for n in nx.ancestors(g, node_id)
-            if isinstance(self.graph.get_node(n), IterateInvocation)
-        ]
+        iterators = [n for n in nx.ancestors(g, node_id) if isinstance(self.graph.get_node(n), IterateInvocation)]
         return iterators
 
     def _prepare(self) -> Optional[str]:
         # Get flattened source graph
         g = self.graph.nx_graph_flat()
 
         # Find next node that:
@@ -1041,37 +945,26 @@
 
         # Create execution nodes
         next_node = self.graph.get_node(next_node_id)
         new_node_ids = list()
         if isinstance(next_node, CollectInvocation):
             # Collapse all iterator input mappings and create a single execution node for the collect invocation
             all_iteration_mappings = list(
-                itertools.chain(
-                    *(
-                        ((s, p) for p in self.source_prepared_mapping[s])
-                        for s in next_node_parents
-                    )
-                )
+                itertools.chain(*(((s, p) for p in self.source_prepared_mapping[s]) for s in next_node_parents))
             )
             # all_iteration_mappings = list(set(itertools.chain(*prepared_parent_mappings)))
-            create_results = self._create_execution_node(
-                next_node_id, all_iteration_mappings
-            )
+            create_results = self._create_execution_node(next_node_id, all_iteration_mappings)
             if create_results is not None:
                 new_node_ids.extend(create_results)
         else:  # Iterators or normal nodes
             # Get all iterator combinations for this node
             # Will produce a list of lists of prepared iterator nodes, from which results can be iterated
             iterator_nodes = self._get_node_iterators(next_node_id)
-            iterator_nodes_prepared = [
-                list(self.source_prepared_mapping[n]) for n in iterator_nodes
-            ]
-            iterator_node_prepared_combinations = list(
-                itertools.product(*iterator_nodes_prepared)
-            )
+            iterator_nodes_prepared = [list(self.source_prepared_mapping[n]) for n in iterator_nodes]
+            iterator_node_prepared_combinations = list(itertools.product(*iterator_nodes_prepared))
 
             # Select the correct prepared parents for each iteration
             # For every iterator, the parent must either not be a child of that iterator, or must match the prepared iteration for that iterator
             # TODO: Handle a node mapping to none
             eg = self.execution_graph.nx_graph_flat()
             prepared_parent_mappings = [[(n, self._get_iteration_node(n, g, eg, it)) for n in next_node_parents] for it in iterator_node_prepared_combinations]  # type: ignore
 
@@ -1092,55 +985,40 @@
     ) -> Optional[str]:
         """Gets the prepared version of the specified source node that matches every iteration specified"""
         prepared_nodes = self.source_prepared_mapping[source_node_path]
         if len(prepared_nodes) == 1:
             return next(iter(prepared_nodes))
 
         # Check if the requested node is an iterator
-        prepared_iterator = next(
-            (n for n in prepared_nodes if n in prepared_iterator_nodes), None
-        )
+        prepared_iterator = next((n for n in prepared_nodes if n in prepared_iterator_nodes), None)
         if prepared_iterator is not None:
             return prepared_iterator
 
         # Filter to only iterator nodes that are a parent of the specified node, in tuple format (prepared, source)
-        iterator_source_node_mapping = [
-            (n, self.prepared_source_mapping[n]) for n in prepared_iterator_nodes
-        ]
-        parent_iterators = [
-            itn
-            for itn in iterator_source_node_mapping
-            if nx.has_path(graph, itn[1], source_node_path)
-        ]
+        iterator_source_node_mapping = [(n, self.prepared_source_mapping[n]) for n in prepared_iterator_nodes]
+        parent_iterators = [itn for itn in iterator_source_node_mapping if nx.has_path(graph, itn[1], source_node_path)]
 
         return next(
-            (
-                n
-                for n in prepared_nodes
-                if all(
-                    nx.has_path(execution_graph, pit[0], n)
-                    for pit in parent_iterators
-                )
-            ),
+            (n for n in prepared_nodes if all(nx.has_path(execution_graph, pit[0], n) for pit in parent_iterators)),
             None,
         )
 
     def _get_next_node(self) -> Optional[BaseInvocation]:
         """Gets the deepest node that is ready to be executed"""
         g = self.execution_graph.nx_graph()
 
         # Depth-first search with pre-order traversal is a depth-first topological sort
         sorted_nodes = nx.dfs_preorder_nodes(g)
-        
+
         next_node = next(
             (
                 n
                 for n in sorted_nodes
-                if n not in self.executed # the node must not already be executed...
-                and all((e[0] in self.executed for e in g.in_edges(n))) # ...and all its inputs must be executed
+                if n not in self.executed  # the node must not already be executed...
+                and all((e[0] in self.executed for e in g.in_edges(n)))  # ...and all its inputs must be executed
             ),
             None,
         )
 
         if next_node is None:
             return None
 
@@ -1217,45 +1095,52 @@
 
 
 class ExposedNodeOutput(BaseModel):
     node_path: str = Field(description="The node path to the node with the output")
     field: str = Field(description="The field name of the output")
     alias: str = Field(description="The alias of the output")
 
+
 class LibraryGraph(BaseModel):
     id: str = Field(description="The unique identifier for this library graph", default_factory=uuid.uuid4)
     graph: Graph = Field(description="The graph")
     name: str = Field(description="The name of the graph")
     description: str = Field(description="The description of the graph")
     exposed_inputs: list[ExposedNodeInput] = Field(description="The inputs exposed by this graph", default_factory=list)
-    exposed_outputs: list[ExposedNodeOutput] = Field(description="The outputs exposed by this graph", default_factory=list)
+    exposed_outputs: list[ExposedNodeOutput] = Field(
+        description="The outputs exposed by this graph", default_factory=list
+    )
 
-    @validator('exposed_inputs', 'exposed_outputs')
+    @validator("exposed_inputs", "exposed_outputs")
     def validate_exposed_aliases(cls, v):
         if len(v) != len(set(i.alias for i in v)):
             raise ValueError("Duplicate exposed alias")
         return v
 
     @root_validator
     def validate_exposed_nodes(cls, values):
-        graph = values['graph']
+        graph = values["graph"]
 
         # Validate exposed inputs
-        for exposed_input in values['exposed_inputs']:
+        for exposed_input in values["exposed_inputs"]:
             if not graph.has_node(exposed_input.node_path):
                 raise ValueError(f"Exposed input node {exposed_input.node_path} does not exist")
             node = graph.get_node(exposed_input.node_path)
             if get_input_field(node, exposed_input.field) is None:
-                raise ValueError(f"Exposed input field {exposed_input.field} does not exist on node {exposed_input.node_path}")
+                raise ValueError(
+                    f"Exposed input field {exposed_input.field} does not exist on node {exposed_input.node_path}"
+                )
 
         # Validate exposed outputs
-        for exposed_output in values['exposed_outputs']:
+        for exposed_output in values["exposed_outputs"]:
             if not graph.has_node(exposed_output.node_path):
                 raise ValueError(f"Exposed output node {exposed_output.node_path} does not exist")
             node = graph.get_node(exposed_output.node_path)
             if get_output_field(node, exposed_output.field) is None:
-                raise ValueError(f"Exposed output field {exposed_output.field} does not exist on node {exposed_output.node_path}")
+                raise ValueError(
+                    f"Exposed output field {exposed_output.field} does not exist on node {exposed_output.node_path}"
+                )
 
         return values
 
 
 GraphInvocation.update_forward_refs()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/image_file_storage.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/image_file_storage.py`

 * *Files 2% similar despite different names*

```diff
@@ -81,17 +81,15 @@
     __max_cache_size: int
 
     def __init__(self, output_folder: Union[str, Path]):
         self.__cache = dict()
         self.__cache_ids = Queue()
         self.__max_cache_size = 10  # TODO: get this from config
 
-        self.__output_folder: Path = (
-            output_folder if isinstance(output_folder, Path) else Path(output_folder)
-        )
+        self.__output_folder: Path = output_folder if isinstance(output_folder, Path) else Path(output_folder)
         self.__thumbnails_folder = self.__output_folder / "thumbnails"
 
         # Validate required output folders at launch
         self.__validate_storage_folders()
 
     def get(self, image_name: str) -> PILImageType:
         try:
@@ -116,15 +114,15 @@
         thumbnail_size: int = 256,
     ) -> None:
         try:
             self.__validate_storage_folders()
             image_path = self.get_path(image_name)
 
             pnginfo = PngImagePlugin.PngInfo()
-            
+
             if metadata is not None:
                 pnginfo.add_text("invokeai_metadata", json.dumps(metadata))
             if graph is not None:
                 pnginfo.add_text("invokeai_graph", json.dumps(graph))
 
             image.save(image_path, "PNG", pnginfo=pnginfo)
             thumbnail_name = get_thumbnail_name(image_name)
@@ -179,14 +177,12 @@
 
     def __get_cache(self, image_name: Path) -> Optional[PILImageType]:
         return None if image_name not in self.__cache else self.__cache[image_name]
 
     def __set_cache(self, image_name: Path, image: PILImageType):
         if not image_name in self.__cache:
             self.__cache[image_name] = image
-            self.__cache_ids.put(
-                image_name
-            )  # TODO: this should refresh position for LRU cache
+            self.__cache_ids.put(image_name)  # TODO: this should refresh position for LRU cache
             if len(self.__cache) > self.__max_cache_size:
                 cache_id = self.__cache_ids.get()
                 if cache_id in self.__cache:
                     del self.__cache[cache_id]
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/image_record_storage.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/image_record_storage.py`

 * *Files 0% similar despite different names*

```diff
@@ -422,17 +422,15 @@
             count = cast(int, self._cursor.fetchone()[0])
         except sqlite3.Error as e:
             self._conn.rollback()
             raise e
         finally:
             self._lock.release()
 
-        return OffsetPaginatedResults(
-            items=images, offset=offset, limit=limit, total=count
-        )
+        return OffsetPaginatedResults(items=images, offset=offset, limit=limit, total=count)
 
     def delete(self, image_name: str) -> None:
         try:
             self._lock.acquire()
             self._cursor.execute(
                 """--sql
                 DELETE FROM images
@@ -462,15 +460,14 @@
             self._conn.commit()
         except sqlite3.Error as e:
             self._conn.rollback()
             raise ImageRecordDeleteException from e
         finally:
             self._lock.release()
 
-
     def delete_intermediates(self) -> list[str]:
         try:
             self._lock.acquire()
             self._cursor.execute(
                 """--sql
                 SELECT image_name FROM images
                 WHERE is_intermediate = TRUE;
@@ -501,17 +498,15 @@
         width: int,
         height: int,
         node_id: Optional[str],
         metadata: Optional[dict],
         is_intermediate: bool = False,
     ) -> datetime:
         try:
-            metadata_json = (
-                None if metadata is None else json.dumps(metadata)
-            )
+            metadata_json = None if metadata is None else json.dumps(metadata)
             self._lock.acquire()
             self._cursor.execute(
                 """--sql
                 INSERT OR IGNORE INTO images (
                     image_name,
                     image_origin,
                     image_category,
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/images.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/images.py`

 * *Files 4% similar despite different names*

```diff
@@ -213,20 +213,16 @@
                 is_intermediate=is_intermediate,
                 # Nullable fields
                 node_id=node_id,
                 metadata=metadata,
                 session_id=session_id,
             )
             if board_id is not None:
-                self._services.board_image_records.add_image_to_board(
-                    board_id=board_id, image_name=image_name
-                )
-            self._services.image_files.save(
-                image_name=image_name, image=image, metadata=metadata, graph=graph
-            )
+                self._services.board_image_records.add_image_to_board(board_id=board_id, image_name=image_name)
+            self._services.image_files.save(image_name=image_name, image=image, metadata=metadata, graph=graph)
             image_dto = self.get_dto(image_name)
 
             return image_dto
         except ImageRecordSaveException:
             self._services.logger.error("Failed to save image record")
             raise
         except ImageFileSaveException:
@@ -293,17 +289,15 @@
     def get_metadata(self, image_name: str) -> Optional[ImageMetadata]:
         try:
             image_record = self._services.image_records.get(image_name)
 
             if not image_record.session_id:
                 return ImageMetadata()
 
-            session_raw = self._services.graph_execution_manager.get_raw(
-                image_record.session_id
-            )
+            session_raw = self._services.graph_execution_manager.get_raw(image_record.session_id)
             graph = None
 
             if session_raw:
                 try:
                     graph = get_metadata_graph_from_raw_session(session_raw)
                 except Exception as e:
                     self._services.logger.warn(f"Failed to parse session graph: {e}")
@@ -360,17 +354,15 @@
 
             image_dtos = list(
                 map(
                     lambda r: image_record_to_dto(
                         r,
                         self._services.urls.get_image_url(r.image_name),
                         self._services.urls.get_image_url(r.image_name, True),
-                        self._services.board_image_records.get_board_for_image(
-                            r.image_name
-                        ),
+                        self._services.board_image_records.get_board_for_image(r.image_name),
                     ),
                     results.items,
                 )
             )
 
             return OffsetPaginatedResults[ImageDTO](
                 items=image_dtos,
@@ -394,19 +386,15 @@
             raise
         except Exception as e:
             self._services.logger.error("Problem deleting image record and file")
             raise e
 
     def delete_images_on_board(self, board_id: str):
         try:
-            image_names = (
-                self._services.board_image_records.get_all_board_image_names_for_board(
-                    board_id
-                )
-            )
+            image_names = self._services.board_image_records.get_all_board_image_names_for_board(board_id)
             for image_name in image_names:
                 self._services.image_files.delete(image_name)
             self._services.image_records.delete_many(image_names)
         except ImageRecordDeleteException:
             self._services.logger.error(f"Failed to delete image records")
             raise
         except ImageFileDeleteException:
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/invocation_queue.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/invocation_queue.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 import time
 from abc import ABC, abstractmethod
 from queue import Queue
 
 from pydantic import BaseModel, Field
 from typing import Optional
 
+
 class InvocationQueueItem(BaseModel):
     graph_execution_state_id: str = Field(description="The ID of the graph execution state")
     invocation_id: str = Field(description="The ID of the node being invoked")
     invoke_all: bool = Field(default=False)
     timestamp: float = Field(default_factory=time.time)
 
 
@@ -41,17 +42,19 @@
     def __init__(self):
         self.__queue = Queue()
         self.__cancellations = dict()
 
     def get(self) -> InvocationQueueItem:
         item = self.__queue.get()
 
-        while isinstance(item, InvocationQueueItem) \
-            and item.graph_execution_state_id in self.__cancellations \
-            and self.__cancellations[item.graph_execution_state_id] > item.timestamp:
+        while (
+            isinstance(item, InvocationQueueItem)
+            and item.graph_execution_state_id in self.__cancellations
+            and self.__cancellations[item.graph_execution_state_id] > item.timestamp
+        ):
             item = self.__queue.get()
 
         # Clear old items
         for graph_execution_state_id in list(self.__cancellations.keys()):
             if self.__cancellations[graph_execution_state_id] < item.timestamp:
                 del self.__cancellations[graph_execution_state_id]
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/invocation_services.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/invocation_services.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/invoker.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/invoker.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,26 +3,25 @@
 from abc import ABC
 from typing import Optional
 
 from .graph import Graph, GraphExecutionState
 from .invocation_queue import InvocationQueueItem
 from .invocation_services import InvocationServices
 
+
 class Invoker:
     """The invoker, used to execute invocations"""
 
     services: InvocationServices
 
     def __init__(self, services: InvocationServices):
         self.services = services
         self._start()
 
-    def invoke(
-        self, graph_execution_state: GraphExecutionState, invoke_all: bool = False
-    ) -> Optional[str]:
+    def invoke(self, graph_execution_state: GraphExecutionState, invoke_all: bool = False) -> Optional[str]:
         """Determines the next node to invoke and enqueues it, preparing if needed.
         Returns the id of the queued node, or `None` if there are no nodes left to enqueue."""
 
         # Get the next invocation
         invocation = graph_execution_state.next()
         if not invocation:
             return None
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/item_storage.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/item_storage.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,21 +5,23 @@
 from pydantic.generics import GenericModel
 
 T = TypeVar("T", bound=BaseModel)
 
 
 class PaginatedResults(GenericModel, Generic[T]):
     """Paginated results"""
-    #fmt: off
+
+    # fmt: off
     items: list[T] = Field(description="Items")
     page: int = Field(description="Current Page")
     pages: int = Field(description="Total number of pages")
     per_page: int = Field(description="Number of items per page")
     total: int = Field(description="Total number of items in result")
-    #fmt: on
+    # fmt: on
+
 
 class ItemStorageABC(ABC, Generic[T]):
     _on_changed_callbacks: list[Callable[[T], None]]
     _on_deleted_callbacks: list[Callable[[str], None]]
 
     def __init__(self) -> None:
         self._on_changed_callbacks = list()
@@ -44,17 +46,15 @@
 
     @abstractmethod
     def list(self, page: int = 0, per_page: int = 10) -> PaginatedResults[T]:
         """Gets a paginated list of items"""
         pass
 
     @abstractmethod
-    def search(
-        self, query: str, page: int = 0, per_page: int = 10
-    ) -> PaginatedResults[T]:
+    def search(self, query: str, page: int = 0, per_page: int = 10) -> PaginatedResults[T]:
         pass
 
     def on_changed(self, on_changed: Callable[[T], None]) -> None:
         """Register a callback for when an item is changed"""
         self._on_changed_callbacks.append(on_changed)
 
     def on_deleted(self, on_deleted: Callable[[str], None]) -> None:
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/latent_storage.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/latent_storage.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 from abc import ABC, abstractmethod
 from pathlib import Path
 from queue import Queue
 from typing import Dict, Union, Optional
 
 import torch
 
+
 class LatentsStorageBase(ABC):
     """Responsible for storing and retrieving latents."""
 
     @abstractmethod
     def get(self, name: str) -> torch.Tensor:
         pass
 
@@ -21,15 +22,15 @@
     @abstractmethod
     def delete(self, name: str) -> None:
         pass
 
 
 class ForwardCacheLatentsStorage(LatentsStorageBase):
     """Caches the latest N latents in memory, writing-thorugh to and reading from underlying storage"""
-    
+
     __cache: Dict[str, torch.Tensor]
     __cache_ids: Queue
     __max_cache_size: int
     __underlying_storage: LatentsStorageBase
 
     def __init__(self, underlying_storage: LatentsStorageBase, max_cache_size: int = 20):
         self.__underlying_storage = underlying_storage
@@ -83,12 +84,10 @@
         self.__output_folder.mkdir(parents=True, exist_ok=True)
         latent_path = self.get_path(name)
         torch.save(data, latent_path)
 
     def delete(self, name: str) -> None:
         latent_path = self.get_path(name)
         latent_path.unlink()
-        
 
     def get_path(self, name: str) -> Path:
         return self.__output_folder / name
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/model_manager_service.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/model_manager_service.py`

 * *Files 3% similar despite different names*

```diff
@@ -99,15 +99,15 @@
             model_name2: { etc }
           },
           model_type2:
             { model_name_n: etc
         }
         """
         pass
-    
+
     @abstractmethod
     def list_model(self, model_name: str, base_model: BaseModelType, model_type: ModelType) -> dict:
         """
         Return information about the model using the same format as list_models()
         """
         pass
 
@@ -121,15 +121,15 @@
     @abstractmethod
     def add_model(
         self,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
         model_attributes: dict,
-        clobber: bool = False
+        clobber: bool = False,
     ) -> AddModelResult:
         """
         Update the named model with a dictionary of attributes. Will fail with an
         assertion error if the name already exists. Pass clobber=True to overwrite.
         On a successful update, the config will be changed in memory. Will fail
         with an assertion error if provided attributes are incorrect or
         the model name is missing. Call commit() to write changes to disk.
@@ -144,20 +144,20 @@
         model_type: ModelType,
         model_attributes: dict,
     ) -> AddModelResult:
         """
         Update the named model with a dictionary of attributes. Will fail with a
         ModelNotFoundException if the name does not already exist.
 
-        On a successful update, the config will be changed in memory. Will fail 
-        with an assertion error if provided attributes are incorrect or 
+        On a successful update, the config will be changed in memory. Will fail
+        with an assertion error if provided attributes are incorrect or
         the model name is missing. Call commit() to write changes to disk.
         """
         pass
-    
+
     @abstractmethod
     def del_model(
         self,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
     ):
@@ -165,40 +165,39 @@
         Delete the named model from configuration. If delete_files is true,
         then the underlying weight file or diffusers directory will be deleted
         as well. Call commit() to write to disk.
         """
         pass
 
     @abstractmethod
-    def rename_model(self,
-                     model_name: str,
-                     base_model: BaseModelType,
-                     model_type: ModelType,
-                     new_name: str,
-                     ):
+    def rename_model(
+        self,
+        model_name: str,
+        base_model: BaseModelType,
+        model_type: ModelType,
+        new_name: str,
+    ):
         """
         Rename the indicated model.
         """
         pass
 
     @abstractmethod
-    def list_checkpoint_configs(
-        self
-    )->List[Path]:
+    def list_checkpoint_configs(self) -> List[Path]:
         """
         List the checkpoint config paths from ROOT/configs/stable-diffusion.
         """
         pass
 
     @abstractmethod
     def convert_model(
         self,
         model_name: str,
         base_model: BaseModelType,
-        model_type: Union[ModelType.Main,ModelType.Vae],
+        model_type: Union[ModelType.Main, ModelType.Vae],
     ) -> AddModelResult:
         """
         Convert a checkpoint file into a diffusers folder, deleting the cached
         version and deleting the original checkpoint file if it is in the models
         directory.
         :param model_name: Name of the model to convert
         :param base_model: Base model type
@@ -207,86 +206,93 @@
         This will raise a ValueError unless the model is not a checkpoint. It will
         also raise a ValueError in the event that there is a similarly-named diffusers
         directory already in place.
         """
         pass
 
     @abstractmethod
-    def heuristic_import(self,
-                         items_to_import: set[str],
-                         prediction_type_helper: Optional[Callable[[Path],SchedulerPredictionType]]=None,
-                         )->dict[str, AddModelResult]:
-        '''Import a list of paths, repo_ids or URLs. Returns the set of
+    def heuristic_import(
+        self,
+        items_to_import: set[str],
+        prediction_type_helper: Optional[Callable[[Path], SchedulerPredictionType]] = None,
+    ) -> dict[str, AddModelResult]:
+        """Import a list of paths, repo_ids or URLs. Returns the set of
         successfully imported items.
         :param items_to_import: Set of strings corresponding to models to be imported.
         :param prediction_type_helper: A callback that receives the Path of a Stable Diffusion 2 checkpoint model and returns a SchedulerPredictionType.
 
         The prediction type helper is necessary to distinguish between
         models based on Stable Diffusion 2 Base (requiring
         SchedulerPredictionType.Epsilson) and Stable Diffusion 768
         (requiring SchedulerPredictionType.VPrediction). It is
         generally impossible to do this programmatically, so the
         prediction_type_helper usually asks the user to choose.
 
         The result is a set of successfully installed models. Each element
         of the set is a dict corresponding to the newly-created OmegaConf stanza for
         that model.
-        '''
+        """
         pass
 
     @abstractmethod
     def merge_models(
-            self,
-            model_names: List[str] = Field(default=None, min_items=2, max_items=3, description="List of model names to merge"),
-            base_model: Union[BaseModelType,str] = Field(default=None, description="Base model shared by all models to be merged"),
-            merged_model_name: str = Field(default=None, description="Name of destination model after merging"),
-            alpha: Optional[float] = 0.5,
-            interp: Optional[MergeInterpolationMethod] = None,
-            force: Optional[bool] = False,
-            merge_dest_directory: Optional[Path] = None
+        self,
+        model_names: List[str] = Field(
+            default=None, min_items=2, max_items=3, description="List of model names to merge"
+        ),
+        base_model: Union[BaseModelType, str] = Field(
+            default=None, description="Base model shared by all models to be merged"
+        ),
+        merged_model_name: str = Field(default=None, description="Name of destination model after merging"),
+        alpha: Optional[float] = 0.5,
+        interp: Optional[MergeInterpolationMethod] = None,
+        force: Optional[bool] = False,
+        merge_dest_directory: Optional[Path] = None,
     ) -> AddModelResult:
         """
         Merge two to three diffusrs pipeline models and save as a new model.
         :param model_names: List of 2-3 models to merge
         :param base_model: Base model to use for all models
         :param merged_model_name: Name of destination merged model
         :param alpha: Alpha strength to apply to 2d and 3d model
-        :param interp: Interpolation method. None (default) 
+        :param interp: Interpolation method. None (default)
         :param merge_dest_directory: Save the merged model to the designated directory (with 'merged_model_name' appended)
         """
         pass
 
     @abstractmethod
-    def search_for_models(self, directory: Path)->List[Path]:
+    def search_for_models(self, directory: Path) -> List[Path]:
         """
         Return list of all models found in the designated directory.
         """
         pass
-        
+
     @abstractmethod
     def sync_to_config(self):
         """
-        Re-read models.yaml, rescan the models directory, and reimport models 
+        Re-read models.yaml, rescan the models directory, and reimport models
         in the autoimport directories. Call after making changes outside the
         model manager API.
         """
         pass
-        
+
     @abstractmethod
     def commit(self, conf_file: Optional[Path] = None) -> None:
         """
         Write current configuration out to the indicated file.
         If no conf_file is provided, then replaces the
         original file/database used to initialize the object.
         """
         pass
 
+
 # simple implementation
 class ModelManagerService(ModelManagerServiceBase):
     """Responsible for managing models on disk and in memory"""
+
     def __init__(
         self,
         config: InvokeAIAppConfig,
         logger: ModuleType,
     ):
         """
         Initialize with the path to the models.yaml config file.
@@ -294,49 +300,47 @@
         and sequential_offload boolean. Note that the default device
         type and precision are set up for a CUDA system running at half precision.
         """
         if config.model_conf_path and config.model_conf_path.exists():
             config_file = config.model_conf_path
         else:
             config_file = config.root_dir / "configs/models.yaml"
-            
-        logger.debug(f'Config file={config_file}')
+
+        logger.debug(f"Config file={config_file}")
 
         device = torch.device(choose_torch_device())
-        device_name =  torch.cuda.get_device_name() if device==torch.device('cuda') else ''
-        logger.info(f'GPU device = {device} {device_name}')
+        device_name = torch.cuda.get_device_name() if device == torch.device("cuda") else ""
+        logger.info(f"GPU device = {device} {device_name}")
 
         precision = config.precision
         if precision == "auto":
             precision = choose_precision(device)
-        dtype = torch.float32 if precision == 'float32' else torch.float16
+        dtype = torch.float32 if precision == "float32" else torch.float16
 
         # this is transitional backward compatibility
         # support for the deprecated `max_loaded_models`
         # configuration value. If present, then the
         # cache size is set to 2.5 GB times
         # the number of max_loaded_models. Otherwise
         # use new `max_cache_size` config setting
-        max_cache_size = config.max_cache_size \
-            if hasattr(config,'max_cache_size') \
-               else config.max_loaded_models * 2.5
+        max_cache_size = config.max_cache_size if hasattr(config, "max_cache_size") else config.max_loaded_models * 2.5
 
         logger.debug(f"Maximum RAM cache size: {max_cache_size} GiB")
 
         sequential_offload = config.sequential_guidance
 
         self.mgr = ModelManager(
             config=config_file,
             device_type=device,
             precision=dtype,
             max_cache_size=max_cache_size,
             sequential_offload=sequential_offload,
             logger=logger,
         )
-        logger.info('Model manager service initialized')
+        logger.info("Model manager service initialized")
 
     def get_model(
         self,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
         submodel: Optional[SubModelType] = None,
@@ -367,15 +371,15 @@
         if context:
             self._emit_load_event(
                 context=context,
                 model_name=model_name,
                 base_model=base_model,
                 model_type=model_type,
                 submodel=submodel,
-                model_info=model_info
+                model_info=model_info,
             )
 
         return model_info
 
     def model_exists(
         self,
         model_name: str,
@@ -401,107 +405,105 @@
     def model_names(self) -> List[Tuple[str, BaseModelType, ModelType]]:
         """
         Returns a list of all the model names known.
         """
         return self.mgr.model_names()
 
     def list_models(
-        self,
-        base_model: Optional[BaseModelType] = None,
-        model_type: Optional[ModelType] = None
+        self, base_model: Optional[BaseModelType] = None, model_type: Optional[ModelType] = None
     ) -> list[dict]:
         """
         Return a list of models.
         """
         return self.mgr.list_models(base_model, model_type)
 
     def list_model(self, model_name: str, base_model: BaseModelType, model_type: ModelType) -> dict:
         """
         Return information about the model using the same format as list_models()
         """
-        return self.mgr.list_model(model_name=model_name,
-                                   base_model=base_model,
-                                   model_type=model_type)
+        return self.mgr.list_model(model_name=model_name, base_model=base_model, model_type=model_type)
 
     def add_model(
         self,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
         model_attributes: dict,
         clobber: bool = False,
-    )->None:
+    ) -> None:
         """
         Update the named model with a dictionary of attributes. Will fail with an
         assertion error if the name already exists. Pass clobber=True to overwrite.
         On a successful update, the config will be changed in memory. Will fail
         with an assertion error if provided attributes are incorrect or
         the model name is missing. Call commit() to write changes to disk.
         """
-        self.logger.debug(f'add/update model {model_name}')        
+        self.logger.debug(f"add/update model {model_name}")
         return self.mgr.add_model(model_name, base_model, model_type, model_attributes, clobber)
 
     def update_model(
         self,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
         model_attributes: dict,
     ) -> AddModelResult:
         """
         Update the named model with a dictionary of attributes. Will fail with a
         ModelNotFoundException exception if the name does not already exist.
-        On a successful update, the config will be changed in memory. Will fail 
-        with an assertion error if provided attributes are incorrect or 
+        On a successful update, the config will be changed in memory. Will fail
+        with an assertion error if provided attributes are incorrect or
         the model name is missing. Call commit() to write changes to disk.
         """
-        self.logger.debug(f'update model {model_name}')
+        self.logger.debug(f"update model {model_name}")
         if not self.model_exists(model_name, base_model, model_type):
             raise ModelNotFoundException(f"Unknown model {model_name}")
         return self.add_model(model_name, base_model, model_type, model_attributes, clobber=True)
-    
+
     def del_model(
         self,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
     ):
         """
         Delete the named model from configuration. If delete_files is true,
         then the underlying weight file or diffusers directory will be deleted
         as well.
         """
-        self.logger.debug(f'delete model {model_name}')
+        self.logger.debug(f"delete model {model_name}")
         self.mgr.del_model(model_name, base_model, model_type)
         self.mgr.commit()
 
     def convert_model(
         self,
         model_name: str,
         base_model: BaseModelType,
-        model_type: Union[ModelType.Main,ModelType.Vae],
-        convert_dest_directory: Optional[Path] = Field(default=None, description="Optional directory location for merged model"),        
+        model_type: Union[ModelType.Main, ModelType.Vae],
+        convert_dest_directory: Optional[Path] = Field(
+            default=None, description="Optional directory location for merged model"
+        ),
     ) -> AddModelResult:
         """
         Convert a checkpoint file into a diffusers folder, deleting the cached
         version and deleting the original checkpoint file if it is in the models
         directory.
         :param model_name: Name of the model to convert
         :param base_model: Base model type
         :param model_type: Type of model ['vae' or 'main']
         :param convert_dest_directory: Save the converted model to the designated directory (`models/etc/etc` by default)
 
         This will raise a ValueError unless the model is not a checkpoint. It will
         also raise a ValueError in the event that there is a similarly-named diffusers
         directory already in place.
         """
-        self.logger.debug(f'convert model {model_name}')        
+        self.logger.debug(f"convert model {model_name}")
         return self.mgr.convert_model(model_name, base_model, model_type, convert_dest_directory)
 
-    def commit(self, conf_file: Optional[Path]=None):
+    def commit(self, conf_file: Optional[Path] = None):
         """
         Write current configuration out to the indicated file.
         If no conf_file is provided, then replaces the
         original file/database used to initialize the object.
         """
         return self.mgr.commit(conf_file)
 
@@ -520,125 +522,132 @@
         if model_info:
             context.services.events.emit_model_load_completed(
                 graph_execution_state_id=context.graph_execution_state_id,
                 model_name=model_name,
                 base_model=base_model,
                 model_type=model_type,
                 submodel=submodel,
-                model_info=model_info
+                model_info=model_info,
             )
         else:
             context.services.events.emit_model_load_started(
                 graph_execution_state_id=context.graph_execution_state_id,
                 model_name=model_name,
                 base_model=base_model,
                 model_type=model_type,
                 submodel=submodel,
             )
 
-
     @property
     def logger(self):
         return self.mgr.logger
 
-    def heuristic_import(self,
-                         items_to_import: set[str],
-                         prediction_type_helper: Optional[Callable[[Path],SchedulerPredictionType]]=None,
-                         )->dict[str, AddModelResult]:
-        '''Import a list of paths, repo_ids or URLs. Returns the set of
+    def heuristic_import(
+        self,
+        items_to_import: set[str],
+        prediction_type_helper: Optional[Callable[[Path], SchedulerPredictionType]] = None,
+    ) -> dict[str, AddModelResult]:
+        """Import a list of paths, repo_ids or URLs. Returns the set of
         successfully imported items.
         :param items_to_import: Set of strings corresponding to models to be imported.
         :param prediction_type_helper: A callback that receives the Path of a Stable Diffusion 2 checkpoint model and returns a SchedulerPredictionType.
 
         The prediction type helper is necessary to distinguish between
         models based on Stable Diffusion 2 Base (requiring
         SchedulerPredictionType.Epsilson) and Stable Diffusion 768
         (requiring SchedulerPredictionType.VPrediction). It is
         generally impossible to do this programmatically, so the
         prediction_type_helper usually asks the user to choose.
 
         The result is a set of successfully installed models. Each element
         of the set is a dict corresponding to the newly-created OmegaConf stanza for
         that model.
-        '''
-        return self.mgr.heuristic_import(items_to_import, prediction_type_helper)        
+        """
+        return self.mgr.heuristic_import(items_to_import, prediction_type_helper)
 
     def merge_models(
-            self,
-            model_names: List[str] = Field(default=None, min_items=2, max_items=3, description="List of model names to merge"),
-            base_model: Union[BaseModelType,str] = Field(default=None, description="Base model shared by all models to be merged"),
-            merged_model_name: str = Field(default=None, description="Name of destination model after merging"),
-            alpha: Optional[float] = 0.5,
-            interp: Optional[MergeInterpolationMethod] = None,
-            force: Optional[bool] = False,
-            merge_dest_directory: Optional[Path] = Field(default=None, description="Optional directory location for merged model"),
+        self,
+        model_names: List[str] = Field(
+            default=None, min_items=2, max_items=3, description="List of model names to merge"
+        ),
+        base_model: Union[BaseModelType, str] = Field(
+            default=None, description="Base model shared by all models to be merged"
+        ),
+        merged_model_name: str = Field(default=None, description="Name of destination model after merging"),
+        alpha: Optional[float] = 0.5,
+        interp: Optional[MergeInterpolationMethod] = None,
+        force: Optional[bool] = False,
+        merge_dest_directory: Optional[Path] = Field(
+            default=None, description="Optional directory location for merged model"
+        ),
     ) -> AddModelResult:
         """
         Merge two to three diffusrs pipeline models and save as a new model.
         :param model_names: List of 2-3 models to merge
         :param base_model: Base model to use for all models
         :param merged_model_name: Name of destination merged model
         :param alpha: Alpha strength to apply to 2d and 3d model
-        :param interp: Interpolation method. None (default) 
+        :param interp: Interpolation method. None (default)
         :param merge_dest_directory: Save the merged model to the designated directory (with 'merged_model_name' appended)
         """
         merger = ModelMerger(self.mgr)
         try:
             result = merger.merge_diffusion_models_and_save(
-                model_names = model_names,
-                base_model = base_model,
-                merged_model_name = merged_model_name,
-                alpha = alpha,
-                interp = interp,
-                force = force,
+                model_names=model_names,
+                base_model=base_model,
+                merged_model_name=merged_model_name,
+                alpha=alpha,
+                interp=interp,
+                force=force,
                 merge_dest_directory=merge_dest_directory,
             )
         except AssertionError as e:
             raise ValueError(e)
         return result
 
-    def search_for_models(self, directory: Path)->List[Path]:
+    def search_for_models(self, directory: Path) -> List[Path]:
         """
         Return list of all models found in the designated directory.
         """
         search = FindModels([directory], self.logger)
         return search.list_models()
 
     def sync_to_config(self):
         """
-        Re-read models.yaml, rescan the models directory, and reimport models 
+        Re-read models.yaml, rescan the models directory, and reimport models
         in the autoimport directories. Call after making changes outside the
         model manager API.
         """
         return self.mgr.sync_to_config()
 
-    def list_checkpoint_configs(self)->List[Path]:
+    def list_checkpoint_configs(self) -> List[Path]:
         """
         List the checkpoint config paths from ROOT/configs/stable-diffusion.
         """
         config = self.mgr.app_config
         conf_path = config.legacy_conf_path
         root_path = config.root_path
-        return [(conf_path / x).relative_to(root_path) for x in conf_path.glob('**/*.yaml')]
+        return [(conf_path / x).relative_to(root_path) for x in conf_path.glob("**/*.yaml")]
 
-    def rename_model(self,
-                     model_name: str,
-                     base_model: BaseModelType,
-                     model_type: ModelType,
-                     new_name: str = None,
-                     new_base: BaseModelType = None,
-                     ):
+    def rename_model(
+        self,
+        model_name: str,
+        base_model: BaseModelType,
+        model_type: ModelType,
+        new_name: str = None,
+        new_base: BaseModelType = None,
+    ):
         """
         Rename the indicated model. Can provide a new name and/or a new base.
         :param model_name: Current name of the model
         :param base_model: Current base of the model
         :param model_type: Model type (can't be changed)
         :param new_name: New name for the model
         :param new_base: New base for the model
         """
-        self.mgr.rename_model(base_model = base_model,
-                              model_type = model_type,
-                              model_name = model_name,
-                              new_name = new_name,
-                              new_base = new_base,
-                              )
-        
+        self.mgr.rename_model(
+            base_model=base_model,
+            model_type=model_type,
+            model_name=model_name,
+            new_name=new_name,
+            new_base=new_base,
+        )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/models/board_record.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/models/board_record.py`

 * *Files 7% similar despite different names*

```diff
@@ -7,38 +7,28 @@
 class BoardRecord(BaseModel):
     """Deserialized board record."""
 
     board_id: str = Field(description="The unique ID of the board.")
     """The unique ID of the board."""
     board_name: str = Field(description="The name of the board.")
     """The name of the board."""
-    created_at: Union[datetime, str] = Field(
-        description="The created timestamp of the board."
-    )
+    created_at: Union[datetime, str] = Field(description="The created timestamp of the board.")
     """The created timestamp of the image."""
-    updated_at: Union[datetime, str] = Field(
-        description="The updated timestamp of the board."
-    )
+    updated_at: Union[datetime, str] = Field(description="The updated timestamp of the board.")
     """The updated timestamp of the image."""
-    deleted_at: Union[datetime, str, None] = Field(
-        description="The deleted timestamp of the board."
-    )
+    deleted_at: Union[datetime, str, None] = Field(description="The deleted timestamp of the board.")
     """The updated timestamp of the image."""
-    cover_image_name: Optional[str] = Field(
-        description="The name of the cover image of the board."
-    )
+    cover_image_name: Optional[str] = Field(description="The name of the cover image of the board.")
     """The name of the cover image of the board."""
 
 
 class BoardDTO(BoardRecord):
     """Deserialized board record with cover image URL and image count."""
 
-    cover_image_name: Optional[str] = Field(
-        description="The name of the board's cover image."
-    )
+    cover_image_name: Optional[str] = Field(description="The name of the board's cover image.")
     """The URL of the thumbnail of the most recent image in the board."""
     image_count: int = Field(description="The number of images in the board.")
     """The number of images in the board."""
 
 
 def deserialize_board_record(board_dict: dict) -> BoardRecord:
     """Deserializes a board record."""
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/models/image_record.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/models/image_record.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,25 +16,19 @@
     """The origin of the image."""
     image_category: ImageCategory = Field(description="The category of the image.")
     """The category of the image."""
     width: int = Field(description="The width of the image in px.")
     """The actual width of the image in px. This may be different from the width in metadata."""
     height: int = Field(description="The height of the image in px.")
     """The actual height of the image in px. This may be different from the height in metadata."""
-    created_at: Union[datetime.datetime, str] = Field(
-        description="The created timestamp of the image."
-    )
+    created_at: Union[datetime.datetime, str] = Field(description="The created timestamp of the image.")
     """The created timestamp of the image."""
-    updated_at: Union[datetime.datetime, str] = Field(
-        description="The updated timestamp of the image."
-    )
+    updated_at: Union[datetime.datetime, str] = Field(description="The updated timestamp of the image.")
     """The updated timestamp of the image."""
-    deleted_at: Union[datetime.datetime, str, None] = Field(
-        description="The deleted timestamp of the image."
-    )
+    deleted_at: Union[datetime.datetime, str, None] = Field(description="The deleted timestamp of the image.")
     """The deleted timestamp of the image."""
     is_intermediate: bool = Field(description="Whether this is an intermediate image.")
     """Whether this is an intermediate image."""
     session_id: Optional[str] = Field(
         default=None,
         description="The session ID that generated this image, if it is a generated image.",
     )
@@ -51,26 +45,22 @@
 
     Only limited changes are valid:
       - `image_category`: change the category of an image
       - `session_id`: change the session associated with an image
       - `is_intermediate`: change the image's `is_intermediate` flag
     """
 
-    image_category: Optional[ImageCategory] = Field(
-        description="The image's new category."
-    )
+    image_category: Optional[ImageCategory] = Field(description="The image's new category.")
     """The image's new category."""
     session_id: Optional[StrictStr] = Field(
         default=None,
         description="The image's new session ID.",
     )
     """The image's new session ID."""
-    is_intermediate: Optional[StrictBool] = Field(
-        default=None, description="The image's new `is_intermediate` flag."
-    )
+    is_intermediate: Optional[StrictBool] = Field(default=None, description="The image's new `is_intermediate` flag.")
     """The image's new `is_intermediate` flag."""
 
 
 class ImageUrlsDTO(BaseModel):
     """The URLs for an image and its thumbnail."""
 
     image_name: str = Field(description="The unique name of the image.")
@@ -80,17 +70,15 @@
     thumbnail_url: str = Field(description="The URL of the image's thumbnail.")
     """The URL of the image's thumbnail."""
 
 
 class ImageDTO(ImageRecord, ImageUrlsDTO):
     """Deserialized image record, enriched for the frontend."""
 
-    board_id: Optional[str] = Field(
-        description="The id of the board the image belongs to, if one exists."
-    )
+    board_id: Optional[str] = Field(description="The id of the board the image belongs to, if one exists.")
     """The id of the board the image belongs to, if one exists."""
     pass
 
 
 def image_record_to_dto(
     image_record: ImageRecord, image_url: str, thumbnail_url: str, board_id: Optional[str]
 ) -> ImageDTO:
@@ -106,20 +94,16 @@
 def deserialize_image_record(image_dict: dict) -> ImageRecord:
     """Deserializes an image record."""
 
     # Retrieve all the values, setting "reasonable" defaults if they are not present.
 
     # TODO: do we really need to handle default values here? ideally the data is the correct shape...
     image_name = image_dict.get("image_name", "unknown")
-    image_origin = ResourceOrigin(
-        image_dict.get("image_origin", ResourceOrigin.INTERNAL.value)
-    )
-    image_category = ImageCategory(
-        image_dict.get("image_category", ImageCategory.GENERAL.value)
-    )
+    image_origin = ResourceOrigin(image_dict.get("image_origin", ResourceOrigin.INTERNAL.value))
+    image_category = ImageCategory(image_dict.get("image_category", ImageCategory.GENERAL.value))
     width = image_dict.get("width", 0)
     height = image_dict.get("height", 0)
     session_id = image_dict.get("session_id", None)
     node_id = image_dict.get("node_id", None)
     created_at = image_dict.get("created_at", get_iso_timestamp())
     updated_at = image_dict.get("updated_at", get_iso_timestamp())
     deleted_at = image_dict.get("deleted_at", get_iso_timestamp())
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/processor.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/processor.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,14 +4,16 @@
 
 from ..invocations.baseinvocation import InvocationContext
 from .invocation_queue import InvocationQueueItem
 from .invoker import InvocationProcessorABC, Invoker
 from ..models.exceptions import CanceledException
 
 import invokeai.backend.util.logging as logger
+
+
 class DefaultInvocationProcessor(InvocationProcessorABC):
     __invoker_thread: Thread
     __stop_event: Event
     __invoker: Invoker
     __threadLimit: BoundedSemaphore
 
     def start(self, invoker) -> None:
@@ -20,17 +22,15 @@
         self.__invoker = invoker
         self.__stop_event = Event()
         self.__invoker_thread = Thread(
             name="invoker_processor",
             target=self.__process,
             kwargs=dict(stop_event=self.__stop_event),
         )
-        self.__invoker_thread.daemon = (
-            True  # TODO: make async and do not use threads
-        )
+        self.__invoker_thread.daemon = True  # TODO: make async and do not use threads
         self.__invoker_thread.start()
 
     def stop(self, *args, **kwargs) -> None:
         self.__stop_event.set()
 
     def __process(self, stop_event: Event):
         try:
@@ -43,32 +43,28 @@
 
                 if not queue_item:  # Probably stopping
                     # do not hammer the queue
                     time.sleep(0.5)
                     continue
 
                 try:
-                    graph_execution_state = (
-                        self.__invoker.services.graph_execution_manager.get(
-                            queue_item.graph_execution_state_id
-                        )
+                    graph_execution_state = self.__invoker.services.graph_execution_manager.get(
+                        queue_item.graph_execution_state_id
                     )
                 except Exception as e:
                     self.__invoker.services.logger.error("Exception while retrieving session:\n%s" % e)
                     self.__invoker.services.events.emit_session_retrieval_error(
                         graph_execution_state_id=queue_item.graph_execution_state_id,
                         error_type=e.__class__.__name__,
                         error=traceback.format_exc(),
                     )
                     continue
-                
+
                 try:
-                    invocation = graph_execution_state.execution_graph.get_node(
-                        queue_item.invocation_id
-                    )
+                    invocation = graph_execution_state.execution_graph.get_node(queue_item.invocation_id)
                 except Exception as e:
                     self.__invoker.services.logger.error("Exception while retrieving invocation:\n%s" % e)
                     self.__invoker.services.events.emit_invocation_retrieval_error(
                         graph_execution_state_id=queue_item.graph_execution_state_id,
                         node_id=queue_item.invocation_id,
                         error_type=e.__class__.__name__,
                         error=traceback.format_exc(),
@@ -78,39 +74,35 @@
                 # get the source node id to provide to clients (the prepared node id is not as useful)
                 source_node_id = graph_execution_state.prepared_source_mapping[invocation.id]
 
                 # Send starting event
                 self.__invoker.services.events.emit_invocation_started(
                     graph_execution_state_id=graph_execution_state.id,
                     node=invocation.dict(),
-                    source_node_id=source_node_id
+                    source_node_id=source_node_id,
                 )
 
                 # Invoke
                 try:
                     outputs = invocation.invoke(
                         InvocationContext(
                             services=self.__invoker.services,
                             graph_execution_state_id=graph_execution_state.id,
                         )
                     )
 
                     # Check queue to see if this is canceled, and skip if so
-                    if self.__invoker.services.queue.is_canceled(
-                        graph_execution_state.id
-                    ):
+                    if self.__invoker.services.queue.is_canceled(graph_execution_state.id):
                         continue
 
                     # Save outputs and history
                     graph_execution_state.complete(invocation.id, outputs)
 
                     # Save the state changes
-                    self.__invoker.services.graph_execution_manager.set(
-                        graph_execution_state
-                    )
+                    self.__invoker.services.graph_execution_manager.set(graph_execution_state)
 
                     # Send complete event
                     self.__invoker.services.events.emit_invocation_complete(
                         graph_execution_state_id=graph_execution_state.id,
                         node=invocation.dict(),
                         source_node_id=source_node_id,
                         result=outputs.dict(),
@@ -126,52 +118,46 @@
                     error = traceback.format_exc()
                     logger.error(error)
 
                     # Save error
                     graph_execution_state.set_node_error(invocation.id, error)
 
                     # Save the state changes
-                    self.__invoker.services.graph_execution_manager.set(
-                        graph_execution_state
-                    )
+                    self.__invoker.services.graph_execution_manager.set(graph_execution_state)
 
                     self.__invoker.services.logger.error("Error while invoking:\n%s" % e)
                     # Send error event
                     self.__invoker.services.events.emit_invocation_error(
                         graph_execution_state_id=graph_execution_state.id,
                         node=invocation.dict(),
                         source_node_id=source_node_id,
                         error_type=e.__class__.__name__,
                         error=error,
                     )
 
                     pass
 
                 # Check queue to see if this is canceled, and skip if so
-                if self.__invoker.services.queue.is_canceled(
-                    graph_execution_state.id
-                ):
+                if self.__invoker.services.queue.is_canceled(graph_execution_state.id):
                     continue
 
                 # Queue any further commands if invoking all
                 is_complete = graph_execution_state.is_complete()
                 if queue_item.invoke_all and not is_complete:
                     try:
                         self.__invoker.invoke(graph_execution_state, invoke_all=True)
                     except Exception as e:
                         self.__invoker.services.logger.error("Error while invoking:\n%s" % e)
                         self.__invoker.services.events.emit_invocation_error(
                             graph_execution_state_id=graph_execution_state.id,
                             node=invocation.dict(),
                             source_node_id=source_node_id,
                             error_type=e.__class__.__name__,
-                            error=traceback.format_exc()
+                            error=traceback.format_exc(),
                         )
                 elif is_complete:
-                    self.__invoker.services.events.emit_graph_execution_complete(
-                        graph_execution_state.id
-                    )
+                    self.__invoker.services.events.emit_graph_execution_complete(graph_execution_state.id)
 
         except KeyboardInterrupt:
             pass  # Log something? KeyboardInterrupt is probably not going to be seen by the processor
         finally:
             self.__threadLimit.release()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/resource_name.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/resource_name.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/sqlite.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/sqlite.py`

 * *Files 4% similar despite different names*

```diff
@@ -62,47 +62,41 @@
         finally:
             self._lock.release()
         self._on_changed(item)
 
     def get(self, id: str) -> Optional[T]:
         try:
             self._lock.acquire()
-            self._cursor.execute(
-                f"""SELECT item FROM {self._table_name} WHERE id = ?;""", (str(id),)
-            )
+            self._cursor.execute(f"""SELECT item FROM {self._table_name} WHERE id = ?;""", (str(id),))
             result = self._cursor.fetchone()
         finally:
             self._lock.release()
 
         if not result:
             return None
 
         return self._parse_item(result[0])
 
     def get_raw(self, id: str) -> Optional[str]:
         try:
             self._lock.acquire()
-            self._cursor.execute(
-                f"""SELECT item FROM {self._table_name} WHERE id = ?;""", (str(id),)
-            )
+            self._cursor.execute(f"""SELECT item FROM {self._table_name} WHERE id = ?;""", (str(id),))
             result = self._cursor.fetchone()
         finally:
             self._lock.release()
 
         if not result:
             return None
 
         return result[0]
 
     def delete(self, id: str):
         try:
             self._lock.acquire()
-            self._cursor.execute(
-                f"""DELETE FROM {self._table_name} WHERE id = ?;""", (str(id),)
-            )
+            self._cursor.execute(f"""DELETE FROM {self._table_name} WHERE id = ?;""", (str(id),))
             self._conn.commit()
         finally:
             self._lock.release()
         self._on_deleted(id)
 
     def list(self, page: int = 0, per_page: int = 10) -> PaginatedResults[T]:
         try:
@@ -118,21 +112,17 @@
             self._cursor.execute(f"""SELECT count(*) FROM {self._table_name};""")
             count = self._cursor.fetchone()[0]
         finally:
             self._lock.release()
 
         pageCount = int(count / per_page) + 1
 
-        return PaginatedResults[T](
-            items=items, page=page, pages=pageCount, per_page=per_page, total=count
-        )
-
-    def search(
-        self, query: str, page: int = 0, per_page: int = 10
-    ) -> PaginatedResults[T]:
+        return PaginatedResults[T](items=items, page=page, pages=pageCount, per_page=per_page, total=count)
+
+    def search(self, query: str, page: int = 0, per_page: int = 10) -> PaginatedResults[T]:
         try:
             self._lock.acquire()
             self._cursor.execute(
                 f"""SELECT item FROM {self._table_name} WHERE item LIKE ? LIMIT ? OFFSET ?;""",
                 (f"%{query}%", per_page, page * per_page),
             )
             result = self._cursor.fetchall()
@@ -145,10 +135,8 @@
             )
             count = self._cursor.fetchone()[0]
         finally:
             self._lock.release()
 
         pageCount = int(count / per_page) + 1
 
-        return PaginatedResults[T](
-            items=items, page=page, pages=pageCount, per_page=per_page, total=count
-        )
+        return PaginatedResults[T](items=items, page=page, pages=pageCount, per_page=per_page, total=count)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/services/urls.py` & `InvokeAI-3.0.1rc2/invokeai/app/services/urls.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/util/controlnet_utils.py` & `InvokeAI-3.0.1rc2/invokeai/app/util/controlnet_utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,43 +13,27 @@
 # High Quality Edge Thinning using Pure Python
 # Written by Lvmin Zhangu
 # 2023 April
 # Stanford University
 # If you use this, please Cite "High Quality Edge Thinning using Pure Python", Lvmin Zhang, In Mikubill/sd-webui-controlnet.
 
 lvmin_kernels_raw = [
-    np.array([
-        [-1, -1, -1],
-        [0, 1, 0],
-        [1, 1, 1]
-    ], dtype=np.int32),
-    np.array([
-        [0, -1, -1],
-        [1, 1, -1],
-        [0, 1, 0]
-    ], dtype=np.int32)
+    np.array([[-1, -1, -1], [0, 1, 0], [1, 1, 1]], dtype=np.int32),
+    np.array([[0, -1, -1], [1, 1, -1], [0, 1, 0]], dtype=np.int32),
 ]
 
 lvmin_kernels = []
 lvmin_kernels += [np.rot90(x, k=0, axes=(0, 1)) for x in lvmin_kernels_raw]
 lvmin_kernels += [np.rot90(x, k=1, axes=(0, 1)) for x in lvmin_kernels_raw]
 lvmin_kernels += [np.rot90(x, k=2, axes=(0, 1)) for x in lvmin_kernels_raw]
 lvmin_kernels += [np.rot90(x, k=3, axes=(0, 1)) for x in lvmin_kernels_raw]
 
 lvmin_prunings_raw = [
-    np.array([
-        [-1, -1, -1],
-        [-1, 1, -1],
-        [0, 0, -1]
-    ], dtype=np.int32),
-    np.array([
-        [-1, -1, -1],
-        [-1, 1, -1],
-        [-1, 0, 0]
-    ], dtype=np.int32)
+    np.array([[-1, -1, -1], [-1, 1, -1], [0, 0, -1]], dtype=np.int32),
+    np.array([[-1, -1, -1], [-1, 1, -1], [-1, 0, 0]], dtype=np.int32),
 ]
 
 lvmin_prunings = []
 lvmin_prunings += [np.rot90(x, k=0, axes=(0, 1)) for x in lvmin_prunings_raw]
 lvmin_prunings += [np.rot90(x, k=1, axes=(0, 1)) for x in lvmin_prunings_raw]
 lvmin_prunings += [np.rot90(x, k=2, axes=(0, 1)) for x in lvmin_prunings_raw]
 lvmin_prunings += [np.rot90(x, k=3, axes=(0, 1)) for x in lvmin_prunings_raw]
@@ -95,18 +79,18 @@
 
 
 ################################################################################
 # copied from Mikubill/sd-webui-controlnet external_code.py and modified for InvokeAI
 ################################################################################
 # FIXME: not using yet, if used in the future will most likely require modification of preprocessors
 def pixel_perfect_resolution(
-        image: np.ndarray,
-        target_H: int,
-        target_W: int,
-        resize_mode: str,
+    image: np.ndarray,
+    target_H: int,
+    target_W: int,
+    resize_mode: str,
 ) -> int:
     """
     Calculate the estimated resolution for resizing an image while preserving aspect ratio.
 
     The function first calculates scaling factors for height and width of the image based on the target
     height and width. Then, based on the chosen resize mode, it either takes the smaller or the larger
     scaling factor to estimate the new resolution.
@@ -131,15 +115,15 @@
     raw_H, raw_W, _ = image.shape
 
     k0 = float(target_H) / float(raw_H)
     k1 = float(target_W) / float(raw_W)
 
     if resize_mode == "fill_resize":
         estimation = min(k0, k1) * float(min(raw_H, raw_W))
-    else: # "crop_resize" or "just_resize" (or possibly "just_resize_simple"?)
+    else:  # "crop_resize" or "just_resize" (or possibly "just_resize_simple"?)
         estimation = max(k0, k1) * float(min(raw_H, raw_W))
 
     # print(f"Pixel Perfect Computation:")
     # print(f"resize_mode = {resize_mode}")
     # print(f"raw_H = {raw_H}")
     # print(f"raw_W = {raw_W}")
     # print(f"target_H = {target_H}")
@@ -150,21 +134,15 @@
 
 
 ###########################################################################
 # Copied from detectmap_proc method in scripts/detectmap_proc.py in Mikubill/sd-webui-controlnet
 #    modified for InvokeAI
 ###########################################################################
 # def detectmap_proc(detected_map, module, resize_mode, h, w):
-def np_img_resize(
-    np_img: np.ndarray,
-    resize_mode: str,
-    h: int,
-    w: int,
-    device: torch.device = torch.device('cpu')
-):
+def np_img_resize(np_img: np.ndarray, resize_mode: str, h: int, w: int, device: torch.device = torch.device("cpu")):
     # if 'inpaint' in module:
     #     np_img = np_img.astype(np.float32)
     # else:
     #     np_img = HWC3(np_img)
     np_img = HWC3(np_img)
 
     def safe_numpy(x):
@@ -180,23 +158,22 @@
     def get_pytorch_control(x):
         # A very safe method to make sure that Apple/Mac works
         y = x
 
         # below is very boring but do not change these. If you change these Apple or Mac may fail.
         y = torch.from_numpy(y)
         y = y.float() / 255.0
-        y = rearrange(y, 'h w c -> 1 c h w')
+        y = rearrange(y, "h w c -> 1 c h w")
         y = y.clone()
         # y = y.to(devices.get_device_for("controlnet"))
         y = y.to(device)
         y = y.clone()
         return y
 
-    def high_quality_resize(x: np.ndarray,
-                            size):
+    def high_quality_resize(x: np.ndarray, size):
         # Written by lvmin
         # Super high-quality control map up-scaling, considering binary, seg, and one-pixel edges
         inpaint_mask = None
         if x.ndim == 3 and x.shape[2] == 4:
             inpaint_mask = x[:, :, 3]
             x = x[:, :, 0:3]
 
@@ -240,15 +217,15 @@
             inpaint_mask = (inpaint_mask > 127).astype(np.float32) * 255.0
             inpaint_mask = inpaint_mask[:, :, None].clip(0, 255).astype(np.uint8)
             y = np.concatenate([y, inpaint_mask], axis=2)
 
         return y
 
     # if resize_mode == external_code.ResizeMode.RESIZE:
-    if resize_mode == "just_resize": # RESIZE
+    if resize_mode == "just_resize":  # RESIZE
         np_img = high_quality_resize(np_img, (w, h))
         np_img = safe_numpy(np_img)
         return get_pytorch_control(np_img), np_img
 
     old_h, old_w, _ = np_img.shape
     old_w = float(old_w)
     old_h = float(old_h)
@@ -266,28 +243,29 @@
             # Inpaint hijack
             high_quality_border_color[3] = 255
         high_quality_background = np.tile(high_quality_border_color[None, None], [h, w, 1])
         np_img = high_quality_resize(np_img, (safeint(old_w * k), safeint(old_h * k)))
         new_h, new_w, _ = np_img.shape
         pad_h = max(0, (h - new_h) // 2)
         pad_w = max(0, (w - new_w) // 2)
-        high_quality_background[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = np_img
+        high_quality_background[pad_h : pad_h + new_h, pad_w : pad_w + new_w] = np_img
         np_img = high_quality_background
         np_img = safe_numpy(np_img)
         return get_pytorch_control(np_img), np_img
-    else: # resize_mode == "crop_resize"  (INNER_FIT)
+    else:  # resize_mode == "crop_resize"  (INNER_FIT)
         k = max(k0, k1)
         np_img = high_quality_resize(np_img, (safeint(old_w * k), safeint(old_h * k)))
         new_h, new_w, _ = np_img.shape
         pad_h = max(0, (new_h - h) // 2)
         pad_w = max(0, (new_w - w) // 2)
-        np_img = np_img[pad_h:pad_h + h, pad_w:pad_w + w]
+        np_img = np_img[pad_h : pad_h + h, pad_w : pad_w + w]
         np_img = safe_numpy(np_img)
         return get_pytorch_control(np_img), np_img
 
+
 def prepare_control_image(
     # image used to be Union[PIL.Image.Image, List[PIL.Image.Image], torch.Tensor, List[torch.Tensor]]
     # but now should be able to assume that image is a single PIL.Image, which simplifies things
     image: Image,
     # FIXME: need to fix hardwiring of width and height, change to basing on latents dimensions?
     # latents_to_match_resolution, # TorchTensor of shape (batch_size, 3, height, width)
     width=512,  # should be 8 * latent.shape[3]
@@ -297,34 +275,36 @@
     device="cuda",
     dtype=torch.float16,
     do_classifier_free_guidance=True,
     control_mode="balanced",
     resize_mode="just_resize_simple",
 ):
     # FIXME: implement "crop_resize_simple" and "fill_resize_simple", or pull them out
-    if (resize_mode == "just_resize_simple" or
-        resize_mode == "crop_resize_simple" or
-        resize_mode == "fill_resize_simple"):
+    if (
+        resize_mode == "just_resize_simple"
+        or resize_mode == "crop_resize_simple"
+        or resize_mode == "fill_resize_simple"
+    ):
         image = image.convert("RGB")
-        if (resize_mode == "just_resize_simple"):
+        if resize_mode == "just_resize_simple":
             image = image.resize((width, height), resample=PIL_INTERPOLATION["lanczos"])
-        elif (resize_mode == "crop_resize_simple"): # not yet implemented
+        elif resize_mode == "crop_resize_simple":  # not yet implemented
             pass
-        elif (resize_mode == "fill_resize_simple"): # not yet implemented
+        elif resize_mode == "fill_resize_simple":  # not yet implemented
             pass
         nimage = np.array(image)
         nimage = nimage[None, :]
         nimage = np.concatenate([nimage], axis=0)
         # normalizing RGB values to [0,1] range (in PIL.Image they are [0-255])
         nimage = np.array(nimage).astype(np.float32) / 255.0
         nimage = nimage.transpose(0, 3, 1, 2)
         timage = torch.from_numpy(nimage)
 
     # use fancy lvmin controlnet resizing
-    elif (resize_mode == "just_resize" or resize_mode == "crop_resize" or resize_mode == "fill_resize"):
+    elif resize_mode == "just_resize" or resize_mode == "crop_resize" or resize_mode == "fill_resize":
         nimage = np.array(image)
         timage, nimage = np_img_resize(
             np_img=nimage,
             resize_mode=resize_mode,
             h=height,
             w=width,
             # device=torch.device('cpu')
@@ -332,11 +312,11 @@
         )
     else:
         pass
         print("ERROR: invalid resize_mode ==> ", resize_mode)
         exit(1)
 
     timage = timage.to(device=device, dtype=dtype)
-    cfg_injection = (control_mode == "more_control" or control_mode == "unbalanced")
+    cfg_injection = control_mode == "more_control" or control_mode == "unbalanced"
     if do_classifier_free_guidance and not cfg_injection:
         timage = torch.cat([timage] * 2)
     return timage
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/util/metadata.py` & `InvokeAI-3.0.1rc2/invokeai/app/util/metadata.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/app/util/step_callback.py` & `InvokeAI-3.0.1rc2/invokeai/app/util/step_callback.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,27 +5,24 @@
 from ..invocations.baseinvocation import InvocationContext
 from ...backend.util.util import image_to_dataURL
 from ...backend.generator.base import Generator
 from ...backend.stable_diffusion import PipelineIntermediateState
 from invokeai.app.services.config import InvokeAIAppConfig
 
 
-def sample_to_lowres_estimated_image(samples, latent_rgb_factors, smooth_matrix = None):
+def sample_to_lowres_estimated_image(samples, latent_rgb_factors, smooth_matrix=None):
     latent_image = samples[0].permute(1, 2, 0) @ latent_rgb_factors
 
     if smooth_matrix is not None:
         latent_image = latent_image.unsqueeze(0).permute(3, 0, 1, 2)
-        latent_image = torch.nn.functional.conv2d(latent_image, smooth_matrix.reshape((1,1,3,3)), padding=1)
+        latent_image = torch.nn.functional.conv2d(latent_image, smooth_matrix.reshape((1, 1, 3, 3)), padding=1)
         latent_image = latent_image.permute(1, 2, 3, 0).squeeze(0)
 
     latents_ubyte = (
-        ((latent_image + 1) / 2)
-        .clamp(0, 1)  # change scale from -1..1 to 0..1
-        .mul(0xFF)  # to 0..255
-        .byte()
+        ((latent_image + 1) / 2).clamp(0, 1).mul(0xFF).byte()  # change scale from -1..1 to 0..1  # to 0..255
     ).cpu()
 
     return Image.fromarray(latents_ubyte.numpy())
 
 
 def stable_diffusion_step_callback(
     context: InvocationContext,
@@ -88,42 +85,43 @@
         node=node,
         source_node_id=source_node_id,
         progress_image=ProgressImage(width=width, height=height, dataURL=dataURL),
         step=intermediate_state.step,
         total_steps=node["steps"],
     )
 
+
 def stable_diffusion_xl_step_callback(
     context: InvocationContext,
     node: dict,
     source_node_id: str,
     sample,
     step,
     total_steps,
 ):
     if context.services.queue.is_canceled(context.graph_execution_state_id):
         raise CanceledException
 
     sdxl_latent_rgb_factors = torch.tensor(
         [
             #   R        G        B
-            [ 0.3816,  0.4930,  0.5320],
-            [-0.3753,  0.1631,  0.1739],
-            [ 0.1770,  0.3588, -0.2048],
+            [0.3816, 0.4930, 0.5320],
+            [-0.3753, 0.1631, 0.1739],
+            [0.1770, 0.3588, -0.2048],
             [-0.4350, -0.2644, -0.4289],
         ],
         dtype=sample.dtype,
         device=sample.device,
     )
 
     sdxl_smooth_matrix = torch.tensor(
         [
-            #[ 0.0478,  0.1285,  0.0478],
-            #[ 0.1285,  0.2948,  0.1285],
-            #[ 0.0478,  0.1285,  0.0478],
+            # [ 0.0478,  0.1285,  0.0478],
+            # [ 0.1285,  0.2948,  0.1285],
+            # [ 0.0478,  0.1285,  0.0478],
             [0.0358, 0.0964, 0.0358],
             [0.0964, 0.4711, 0.0964],
             [0.0358, 0.0964, 0.0358],
         ],
         dtype=sample.dtype,
         device=sample.device,
     )
@@ -139,8 +137,8 @@
     context.services.events.emit_generator_progress(
         graph_execution_state_id=context.graph_execution_state_id,
         node=node,
         source_node_id=source_node_id,
         progress_image=ProgressImage(width=width, height=height, dataURL=dataURL),
         step=step,
         total_steps=total_steps,
-    )
+    )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/generator/base.py` & `InvokeAI-3.0.1rc2/invokeai/backend/generator/base.py`

 * *Files 7% similar despite different names*

```diff
@@ -29,69 +29,74 @@
 from ..image_util import configure_model_padding
 from ..util.util import rand_perlin_2d
 from ..stable_diffusion.diffusers_pipeline import StableDiffusionGeneratorPipeline
 from ..stable_diffusion.schedulers import SCHEDULER_MAP
 
 downsampling = 8
 
+
 @dataclass
 class InvokeAIGeneratorBasicParams:
-    seed: Optional[int]=None
-    width: int=512
-    height: int=512
-    cfg_scale: float=7.5
-    steps: int=20
-    ddim_eta: float=0.0
-    scheduler: str='ddim'
-    precision: str='float16'
-    perlin: float=0.0
-    threshold: float=0.0
-    seamless: bool=False
-    seamless_axes: List[str]=field(default_factory=lambda: ['x', 'y'])
-    h_symmetry_time_pct: Optional[float]=None
-    v_symmetry_time_pct: Optional[float]=None
+    seed: Optional[int] = None
+    width: int = 512
+    height: int = 512
+    cfg_scale: float = 7.5
+    steps: int = 20
+    ddim_eta: float = 0.0
+    scheduler: str = "ddim"
+    precision: str = "float16"
+    perlin: float = 0.0
+    threshold: float = 0.0
+    seamless: bool = False
+    seamless_axes: List[str] = field(default_factory=lambda: ["x", "y"])
+    h_symmetry_time_pct: Optional[float] = None
+    v_symmetry_time_pct: Optional[float] = None
     variation_amount: float = 0.0
-    with_variations: list=field(default_factory=list)
+    with_variations: list = field(default_factory=list)
+
 
 @dataclass
 class InvokeAIGeneratorOutput:
-    '''
+    """
     InvokeAIGeneratorOutput is a dataclass that contains the outputs of a generation
     operation, including the image, its seed, the model name used to generate the image
     and the model hash, as well as all the generate() parameters that went into
     generating the image (in .params, also available as attributes)
-    '''
+    """
+
     image: Image.Image
     seed: int
     model_hash: str
     attention_maps_images: List[Image.Image]
     params: Namespace
 
+
 # we are interposing a wrapper around the original Generator classes so that
 # old code that calls Generate will continue to work.
 class InvokeAIGenerator(metaclass=ABCMeta):
-    def __init__(self,
-                 model_info: dict,
-                 params: InvokeAIGeneratorBasicParams=InvokeAIGeneratorBasicParams(),
-                 **kwargs,
-                 ):
-        self.model_info=model_info
-        self.params=params
+    def __init__(
+        self,
+        model_info: dict,
+        params: InvokeAIGeneratorBasicParams = InvokeAIGeneratorBasicParams(),
+        **kwargs,
+    ):
+        self.model_info = model_info
+        self.params = params
         self.kwargs = kwargs
 
     def generate(
         self,
         conditioning: tuple,
         scheduler,
-        callback: Optional[Callable]=None,
-        step_callback: Optional[Callable]=None,
-        iterations: int=1,
+        callback: Optional[Callable] = None,
+        step_callback: Optional[Callable] = None,
+        iterations: int = 1,
         **keyword_args,
-    )->Iterator[InvokeAIGeneratorOutput]:
-        '''
+    ) -> Iterator[InvokeAIGeneratorOutput]:
+        """
         Return an iterator across the indicated number of generations.
         Each time the iterator is called it will return an InvokeAIGeneratorOutput
         object. Use like this:
 
            outputs = txt2img.generate(prompt='banana sushi', iterations=5)
            for result in outputs:
                print(result.image, result.seed)
@@ -103,133 +108,135 @@
 
         Pass None to get an infinite iterator.
 
            outputs = txt2img.generate(prompt='banana sushi', iterations=None)
            for o in outputs:
                print(o.image, o.seed)
 
-        '''
+        """
         generator_args = dataclasses.asdict(self.params)
         generator_args.update(keyword_args)
 
         model_info = self.model_info
         model_name = model_info.name
         model_hash = model_info.hash
         with model_info.context as model:
             gen_class = self._generator_class()
             generator = gen_class(model, self.params.precision, **self.kwargs)
             if self.params.variation_amount > 0:
-                generator.set_variation(generator_args.get('seed'),
-                                        generator_args.get('variation_amount'),
-                                        generator_args.get('with_variations')
-                                        )
+                generator.set_variation(
+                    generator_args.get("seed"),
+                    generator_args.get("variation_amount"),
+                    generator_args.get("with_variations"),
+                )
 
             if isinstance(model, DiffusionPipeline):
                 for component in [model.unet, model.vae]:
-                    configure_model_padding(component,
-                                            generator_args.get('seamless',False),
-                                            generator_args.get('seamless_axes')
-                                            )
+                    configure_model_padding(
+                        component, generator_args.get("seamless", False), generator_args.get("seamless_axes")
+                    )
             else:
-                configure_model_padding(model,
-                                        generator_args.get('seamless',False),
-                                        generator_args.get('seamless_axes')
-                                        )
+                configure_model_padding(
+                    model, generator_args.get("seamless", False), generator_args.get("seamless_axes")
+                )
 
             iteration_count = range(iterations) if iterations else itertools.count(start=0, step=1)
             for i in iteration_count:
                 results = generator.generate(
                     conditioning=conditioning,
                     step_callback=step_callback,
                     sampler=scheduler,
                     **generator_args,
                 )
                 output = InvokeAIGeneratorOutput(
                     image=results[0][0],
                     seed=results[0][1],
                     attention_maps_images=results[0][2],
-                    model_hash = model_hash,
-                    params=Namespace(model_name=model_name,**generator_args),
+                    model_hash=model_hash,
+                    params=Namespace(model_name=model_name, **generator_args),
                 )
                 if callback:
                     callback(output)
             yield output
 
     @classmethod
-    def schedulers(self)->List[str]:
-        '''
+    def schedulers(self) -> List[str]:
+        """
         Return list of all the schedulers that we currently handle.
-        '''
+        """
         return list(SCHEDULER_MAP.keys())
 
     def load_generator(self, model: StableDiffusionGeneratorPipeline, generator_class: Type[Generator]):
         return generator_class(model, self.params.precision)
 
     @classmethod
-    def _generator_class(cls)->Type[Generator]:
-        '''
+    def _generator_class(cls) -> Type[Generator]:
+        """
         In derived classes return the name of the generator to apply.
         If you don't override will return the name of the derived
         class, which nicely parallels the generator class names.
-        '''
+        """
         return Generator
 
+
 # ------------------------------------
 class Img2Img(InvokeAIGenerator):
-    def generate(self,
-               init_image: Union[Image.Image, torch.FloatTensor],
-               strength: float=0.75,
-               **keyword_args
-               )->Iterator[InvokeAIGeneratorOutput]:
-        return super().generate(init_image=init_image,
-                                strength=strength,
-                                **keyword_args
-                                )
+    def generate(
+        self, init_image: Union[Image.Image, torch.FloatTensor], strength: float = 0.75, **keyword_args
+    ) -> Iterator[InvokeAIGeneratorOutput]:
+        return super().generate(init_image=init_image, strength=strength, **keyword_args)
+
     @classmethod
     def _generator_class(cls):
         from .img2img import Img2Img
+
         return Img2Img
 
+
 # ------------------------------------
 # Takes all the arguments of Img2Img and adds the mask image and the seam/infill stuff
 class Inpaint(Img2Img):
-    def generate(self,
-                 mask_image: Union[Image.Image, torch.FloatTensor],
-                 # Seam settings - when 0, doesn't fill seam
-                 seam_size: int = 96,
-                 seam_blur: int = 16,
-                 seam_strength: float = 0.7,
-                 seam_steps: int = 30,
-                 tile_size: int = 32,
-                 inpaint_replace=False,
-                 infill_method=None,
-                 inpaint_width=None,
-                 inpaint_height=None,
-                 inpaint_fill: tuple(int) = (0x7F, 0x7F, 0x7F, 0xFF),
-                 **keyword_args
-                 )->Iterator[InvokeAIGeneratorOutput]:
+    def generate(
+        self,
+        mask_image: Union[Image.Image, torch.FloatTensor],
+        # Seam settings - when 0, doesn't fill seam
+        seam_size: int = 96,
+        seam_blur: int = 16,
+        seam_strength: float = 0.7,
+        seam_steps: int = 30,
+        tile_size: int = 32,
+        inpaint_replace=False,
+        infill_method=None,
+        inpaint_width=None,
+        inpaint_height=None,
+        inpaint_fill: tuple(int) = (0x7F, 0x7F, 0x7F, 0xFF),
+        **keyword_args,
+    ) -> Iterator[InvokeAIGeneratorOutput]:
         return super().generate(
             mask_image=mask_image,
             seam_size=seam_size,
             seam_blur=seam_blur,
             seam_strength=seam_strength,
             seam_steps=seam_steps,
             tile_size=tile_size,
             inpaint_replace=inpaint_replace,
             infill_method=infill_method,
             inpaint_width=inpaint_width,
             inpaint_height=inpaint_height,
             inpaint_fill=inpaint_fill,
-            **keyword_args
+            **keyword_args,
         )
+
     @classmethod
     def _generator_class(cls):
         from .inpaint import Inpaint
+
         return Inpaint
 
+
 class Generator:
     downsampling_factor: int
     latent_channels: int
     precision: str
     model: DiffusionPipeline
 
     def __init__(self, model: DiffusionPipeline, precision: str, **kwargs):
@@ -247,17 +254,15 @@
 
     # this is going to be overridden in img2img.py, txt2img.py and inpaint.py
     def get_make_image(self, **kwargs):
         """
         Returns a function returning an image derived from the prompt and the initial image
         Return value depends on the seed at the time you call it
         """
-        raise NotImplementedError(
-            "image_iterator() must be implemented in a descendent class"
-        )
+        raise NotImplementedError("image_iterator() must be implemented in a descendent class")
 
     def set_variation(self, seed, variation_amount, with_variations):
         self.seed = seed
         self.variation_amount = variation_amount
         self.with_variations = with_variations
 
     def generate(
@@ -276,17 +281,15 @@
         v_symmetry_time_pct=None,
         free_gpu_mem: bool = False,
         **kwargs,
     ):
         scope = nullcontext
         self.free_gpu_mem = free_gpu_mem
         attention_maps_images = []
-        attention_maps_callback = lambda saver: attention_maps_images.append(
-            saver.get_stacked_maps_image()
-        )
+        attention_maps_callback = lambda saver: attention_maps_images.append(saver.get_stacked_maps_image())
         make_image = self.get_make_image(
             sampler=sampler,
             init_image=init_image,
             width=width,
             height=height,
             step_callback=step_callback,
             threshold=threshold,
@@ -323,32 +326,26 @@
 
                 # Pass on the seed in case a layer beneath us needs to generate noise on its own.
                 image = make_image(x_T, seed)
 
                 results.append([image, seed, attention_maps_images])
 
                 if image_callback is not None:
-                    attention_maps_image = (
-                        None
-                        if len(attention_maps_images) == 0
-                        else attention_maps_images[-1]
-                    )
+                    attention_maps_image = None if len(attention_maps_images) == 0 else attention_maps_images[-1]
                     image_callback(
                         image,
                         seed,
                         first_seed=first_seed,
                         attention_maps_image=attention_maps_image,
                     )
 
                 seed = self.new_seed()
 
                 # Free up memory from the last generation.
-                clear_cuda_cache = (
-                    kwargs["clear_cuda_cache"] if "clear_cuda_cache" in kwargs else None
-                )
+                clear_cuda_cache = kwargs["clear_cuda_cache"] if "clear_cuda_cache" in kwargs else None
                 if clear_cuda_cache is not None:
                     clear_cuda_cache()
 
         return results
 
     def sample_to_image(self, samples) -> Image.Image:
         """
@@ -367,22 +364,16 @@
         mask_blur_radius: int = 8,
     ) -> Image.Image:
         if init_image is None or init_mask is None:
             return result
 
         # Get the original alpha channel of the mask if there is one.
         # Otherwise it is some other black/white image format ('1', 'L' or 'RGB')
-        pil_init_mask = (
-            init_mask.getchannel("A")
-            if init_mask.mode == "RGBA"
-            else init_mask.convert("L")
-        )
-        pil_init_image = init_image.convert(
-            "RGBA"
-        )  # Add an alpha channel if one doesn't exist
+        pil_init_mask = init_mask.getchannel("A") if init_mask.mode == "RGBA" else init_mask.convert("L")
+        pil_init_image = init_image.convert("RGBA")  # Add an alpha channel if one doesn't exist
 
         # Build an image with only visible pixels from source to use as reference for color-matching.
         init_rgb_pixels = np.asarray(init_image.convert("RGB"), dtype=np.uint8)
         init_a_pixels = np.asarray(pil_init_image.getchannel("A"), dtype=np.uint8)
         init_mask_pixels = np.asarray(pil_init_mask, dtype=np.uint8)
 
         # Get numpy version of result
@@ -400,18 +391,15 @@
             gen_std = np_image_masked.std(axis=0)
 
             # Color correct
             np_matched_result = np_image.copy()
             np_matched_result[:, :, :] = (
                 (
                     (
-                        (
-                            np_matched_result[:, :, :].astype(np.float32)
-                            - gen_means[None, None, :]
-                        )
+                        (np_matched_result[:, :, :].astype(np.float32) - gen_means[None, None, :])
                         / gen_std[None, None, :]
                     )
                     * init_std[None, None, :]
                     + init_means[None, None, :]
                 )
                 .clip(0, 255)
                 .astype(np.uint8)
@@ -429,17 +417,15 @@
                 iterations=int(mask_blur_radius / 2),
             )
             pmd = Image.fromarray(nmd, mode="L")
             blurred_init_mask = pmd.filter(ImageFilter.BoxBlur(mask_blur_radius))
         else:
             blurred_init_mask = pil_init_mask
 
-        multiplied_blurred_init_mask = ImageChops.multiply(
-            blurred_init_mask, self.pil_image.split()[-1]
-        )
+        multiplied_blurred_init_mask = ImageChops.multiply(blurred_init_mask, self.pil_image.split()[-1])
 
         # Paste original on color-corrected generation (using blurred mask)
         matched_result.paste(init_image, (0, 0), mask=multiplied_blurred_init_mask)
         return matched_result
 
     @staticmethod
     def sample_to_lowres_estimated_image(samples):
@@ -457,18 +443,15 @@
             ],
             dtype=samples.dtype,
             device=samples.device,
         )
 
         latent_image = samples[0].permute(1, 2, 0) @ v1_5_latent_rgb_factors
         latents_ubyte = (
-            ((latent_image + 1) / 2)
-            .clamp(0, 1)  # change scale from -1..1 to 0..1
-            .mul(0xFF)  # to 0..255
-            .byte()
+            ((latent_image + 1) / 2).clamp(0, 1).mul(0xFF).byte()  # change scale from -1..1 to 0..1  # to 0..255
         ).cpu()
 
         return Image.fromarray(latents_ubyte.numpy())
 
     def generate_initial_noise(self, seed, width, height):
         initial_noise = None
         if self.variation_amount > 0 or len(self.with_variations) > 0:
@@ -490,17 +473,15 @@
         # limit noise to only the diffusion image channels, not the mask channels
         input_channels = min(self.latent_channels, 4)
         # round up to the nearest block of 8
         temp_width = int((width + 7) / 8) * 8
         temp_height = int((height + 7) / 8) * 8
         noise = torch.stack(
             [
-                rand_perlin_2d(
-                    (temp_height, temp_width), (8, 8), device=self.model.device
-                ).to(fixdevice)
+                rand_perlin_2d((temp_height, temp_width), (8, 8), device=self.model.device).to(fixdevice)
                 for _ in range(input_channels)
             ],
             dim=0,
         ).to(self.model.device)
         return noise[0:4, 0:height, 0:width]
 
     def new_seed(self):
@@ -569,12 +550,10 @@
                 height // self.downsampling_factor,
                 width // self.downsampling_factor,
             ],
             dtype=self.torch_dtype(),
             device=device,
         )
         if self.perlin > 0.0:
-            perlin_noise = self.get_perlin_noise(
-                width // self.downsampling_factor, height // self.downsampling_factor
-            )
+            perlin_noise = self.get_perlin_noise(width // self.downsampling_factor, height // self.downsampling_factor)
             x = (1 - self.perlin) * x + self.perlin * perlin_noise
         return x
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/generator/img2img.py` & `InvokeAI-3.0.1rc2/invokeai/backend/generator/img2img.py`

 * *Files 3% similar despite different names*

```diff
@@ -73,25 +73,20 @@
                 strength,
                 steps,
                 conditioning_data,
                 noise_func=self.get_noise_like,
                 callback=step_callback,
                 seed=seed,
             )
-            if (
-                pipeline_output.attention_map_saver is not None
-                and attention_maps_callback is not None
-            ):
+            if pipeline_output.attention_map_saver is not None and attention_maps_callback is not None:
                 attention_maps_callback(pipeline_output.attention_map_saver)
             return pipeline.numpy_to_pil(pipeline_output.images)[0]
 
         return make_image
 
     def get_noise_like(self, like: torch.Tensor):
         device = like.device
         x = torch.randn_like(like, device=device)
         if self.perlin > 0.0:
             shape = like.shape
-            x = (1 - self.perlin) * x + self.perlin * self.get_perlin_noise(
-                shape[3], shape[2]
-            )
+            x = (1 - self.perlin) * x + self.perlin * self.get_perlin_noise(shape[3], shape[2])
         return x
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/generator/inpaint.py` & `InvokeAI-3.0.1rc2/invokeai/backend/generator/inpaint.py`

 * *Files 2% similar despite different names*

```diff
@@ -64,23 +64,19 @@
             return im
 
         # Skip patchmatch if patchmatch isn't available
         if not PatchMatch.patchmatch_available():
             return im
 
         # Patchmatch (note, we may want to expose patch_size? Increasing it significantly impacts performance though)
-        im_patched_np = PatchMatch.inpaint(
-            im.convert("RGB"), ImageOps.invert(im.split()[-1]), patch_size=3
-        )
+        im_patched_np = PatchMatch.inpaint(im.convert("RGB"), ImageOps.invert(im.split()[-1]), patch_size=3)
         im_patched = Image.fromarray(im_patched_np, mode="RGB")
         return im_patched
 
-    def tile_fill_missing(
-        self, im: Image.Image, tile_size: int = 16, seed: Optional[int] = None
-    ) -> Image.Image:
+    def tile_fill_missing(self, im: Image.Image, tile_size: int = 16, seed: Optional[int] = None) -> Image.Image:
         # Only fill if there's an alpha layer
         if im.mode != "RGBA":
             return im
 
         a = np.asarray(im, dtype=np.uint8)
 
         tile_size_tuple = (tile_size, tile_size)
@@ -123,34 +119,28 @@
                 tiles_all.shape[4],
             )
         )
         si = Image.fromarray(st, mode="RGBA")
 
         return si
 
-    def mask_edge(
-        self, mask: Image.Image, edge_size: int, edge_blur: int
-    ) -> Image.Image:
+    def mask_edge(self, mask: Image.Image, edge_size: int, edge_blur: int) -> Image.Image:
         npimg = np.asarray(mask, dtype=np.uint8)
 
         # Detect any partially transparent regions
-        npgradient = np.uint8(
-            255 * (1.0 - np.floor(np.abs(0.5 - np.float32(npimg) / 255.0) * 2.0))
-        )
+        npgradient = np.uint8(255 * (1.0 - np.floor(np.abs(0.5 - np.float32(npimg) / 255.0) * 2.0)))
 
         # Detect hard edges
         npedge = cv2.Canny(npimg, threshold1=100, threshold2=200)
 
         # Combine
         npmask = npgradient + npedge
 
         # Expand
-        npmask = cv2.dilate(
-            npmask, np.ones((3, 3), np.uint8), iterations=int(edge_size / 2)
-        )
+        npmask = cv2.dilate(npmask, np.ones((3, 3), np.uint8), iterations=int(edge_size / 2))
 
         new_mask = Image.fromarray(npmask)
 
         if edge_blur > 0:
             new_mask = new_mask.filter(ImageFilter.BoxBlur(edge_blur))
 
         return ImageOps.invert(new_mask)
@@ -238,33 +228,27 @@
         if isinstance(init_image, Image.Image):
             self.pil_image = init_image.copy()
 
             # Do infill
             if infill_method == "patchmatch" and PatchMatch.patchmatch_available():
                 init_filled = self.infill_patchmatch(self.pil_image.copy())
             elif infill_method == "tile":
-                init_filled = self.tile_fill_missing(
-                    self.pil_image.copy(), seed=self.seed, tile_size=tile_size
-                )
+                init_filled = self.tile_fill_missing(self.pil_image.copy(), seed=self.seed, tile_size=tile_size)
             elif infill_method == "solid":
                 solid_bg = Image.new("RGBA", init_image.size, inpaint_fill)
                 init_filled = Image.alpha_composite(solid_bg, init_image)
             else:
-                raise ValueError(
-                    f"Non-supported infill type {infill_method}", infill_method
-                )
+                raise ValueError(f"Non-supported infill type {infill_method}", infill_method)
             init_filled.paste(init_image, (0, 0), init_image.split()[-1])
 
             # Resize if requested for inpainting
             if inpaint_width and inpaint_height:
                 init_filled = init_filled.resize((inpaint_width, inpaint_height))
 
-            debug_image(
-                init_filled, "init_filled", debug_status=self.enable_image_debugging
-            )
+            debug_image(init_filled, "init_filled", debug_status=self.enable_image_debugging)
 
             # Create init tensor
             init_image = image_resized_to_grid_as_tensor(init_filled.convert("RGB"))
 
         if isinstance(mask_image, Image.Image):
             self.pil_mask = mask_image.copy()
             debug_image(
@@ -285,52 +269,45 @@
                 mask_image = mask_image.resize((inpaint_width, inpaint_height))
 
             debug_image(
                 mask_image,
                 "mask_image AFTER multiply with pil_image",
                 debug_status=self.enable_image_debugging,
             )
-            mask: torch.FloatTensor = image_resized_to_grid_as_tensor(
-                mask_image, normalize=False
-            )
+            mask: torch.FloatTensor = image_resized_to_grid_as_tensor(mask_image, normalize=False)
         else:
             mask: torch.FloatTensor = mask_image
 
         self.mask_blur_radius = mask_blur_radius
 
         # noinspection PyTypeChecker
         pipeline: StableDiffusionGeneratorPipeline = self.model
 
         # todo: support cross-attention control
         uc, c, _ = conditioning
-        conditioning_data = ConditioningData(
-            uc, c, cfg_scale
-        ).add_scheduler_args_if_applicable(pipeline.scheduler, eta=ddim_eta)
+        conditioning_data = ConditioningData(uc, c, cfg_scale).add_scheduler_args_if_applicable(
+            pipeline.scheduler, eta=ddim_eta
+        )
 
         def make_image(x_T: torch.Tensor, seed: int):
             pipeline_output = pipeline.inpaint_from_embeddings(
                 init_image=init_image,
                 mask=1 - mask,  # expects white means "paint here."
                 strength=strength,
                 num_inference_steps=steps,
                 conditioning_data=conditioning_data,
                 noise_func=self.get_noise_like,
                 callback=step_callback,
                 seed=seed,
             )
 
-            if (
-                pipeline_output.attention_map_saver is not None
-                and attention_maps_callback is not None
-            ):
+            if pipeline_output.attention_map_saver is not None and attention_maps_callback is not None:
                 attention_maps_callback(pipeline_output.attention_map_saver)
 
-            result = self.postprocess_size_and_mask(
-                pipeline.numpy_to_pil(pipeline_output.images)[0]
-            )
+            result = self.postprocess_size_and_mask(pipeline.numpy_to_pil(pipeline_output.images)[0])
 
             # Seam paint if this is our first pass (seam_size set to 0 during seam painting)
             if seam_size > 0:
                 old_image = self.pil_image or init_image
                 old_mask = self.pil_mask or mask_image
 
                 result = self.seam_paint(
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/image_util/__init__.py` & `InvokeAI-3.0.1rc2/invokeai/backend/image_util/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -4,17 +4,15 @@
 from .patchmatch import PatchMatch
 from .pngwriter import PngWriter, PromptFormatter, retrieve_metadata, write_metadata
 from .seamless import configure_model_padding
 from .txt2mask import Txt2Mask
 from .util import InitImageResizer, make_grid
 
 
-def debug_image(
-    debug_image, debug_text, debug_show=True, debug_result=False, debug_status=False
-):
+def debug_image(debug_image, debug_text, debug_show=True, debug_result=False, debug_status=False):
     if not debug_status:
         return
 
     image_copy = debug_image.copy().convert("RGBA")
     ImageDraw.Draw(image_copy).text((5, 5), debug_text, (255, 0, 0))
 
     if debug_show:
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/image_util/patchmatch.py` & `InvokeAI-3.0.1rc2/invokeai/backend/image_util/patchmatch.py`

 * *Files 0% similar despite different names*

```diff
@@ -3,16 +3,18 @@
 wraps the actual patchmatch object. It respects the global
 "try_patchmatch" attribute, so that patchmatch loading can
 be suppressed or deferred
 """
 import numpy as np
 import invokeai.backend.util.logging as logger
 from invokeai.app.services.config import InvokeAIAppConfig
+
 config = InvokeAIAppConfig.get_config()
 
+
 class PatchMatch:
     """
     Thin class wrapper around the patchmatch function.
     """
 
     patch_match = None
     tried_load: bool = False
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/image_util/pngwriter.py` & `InvokeAI-3.0.1rc2/invokeai/backend/image_util/pngwriter.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,17 +30,15 @@
             "0000000.0.png",
         )
         basecount = int(existing_name.split(".", 1)[0]) + 1
         return f"{basecount:06}"
 
     # saves image named _image_ to outdir/name, writing metadata from prompt
     # returns full path of output
-    def save_image_and_prompt_to_png(
-        self, image, dream_prompt, name, metadata=None, compress_level=6
-    ):
+    def save_image_and_prompt_to_png(self, image, dream_prompt, name, metadata=None, compress_level=6):
         path = os.path.join(self.outdir, name)
         info = PngImagePlugin.PngInfo()
         info.add_text("Dream", dream_prompt)
         if metadata:
             info.add_text("sd-metadata", json.dumps(metadata))
         image.save(path, "PNG", pnginfo=info, compress_level=compress_level)
         return path
@@ -110,12 +108,10 @@
         if opt.gfpgan_strength:
             switches.append(f"-G{opt.gfpgan_strength}")
         if opt.upscale:
             switches.append(f'-U {" ".join([str(u) for u in opt.upscale])}')
         if opt.variation_amount > 0:
             switches.append(f"-v{opt.variation_amount}")
         if opt.with_variations:
-            formatted_variations = ",".join(
-                f"{seed}:{weight}" for seed, weight in opt.with_variations
-            )
+            formatted_variations = ",".join(f"{seed}:{weight}" for seed, weight in opt.with_variations)
             switches.append(f"-V{formatted_variations}")
         return " ".join(switches)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/image_util/safety_checker.py` & `InvokeAI-3.0.1rc2/invokeai/backend/image_util/safety_checker.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,57 +5,58 @@
 """
 import numpy as np
 from PIL import Image
 from invokeai.backend import SilenceWarnings
 from invokeai.app.services.config import InvokeAIAppConfig
 from invokeai.backend.util.devices import choose_torch_device
 import invokeai.backend.util.logging as logger
+
 config = InvokeAIAppConfig.get_config()
 
-CHECKER_PATH = 'core/convert/stable-diffusion-safety-checker'
+CHECKER_PATH = "core/convert/stable-diffusion-safety-checker"
+
 
 class SafetyChecker:
     """
     Wrapper around SafetyChecker model.
     """
+
     safety_checker = None
     feature_extractor = None
     tried_load: bool = False
 
     @classmethod
     def _load_safety_checker(self):
         if self.tried_load:
             return
-        
+
         if config.nsfw_checker:
             try:
                 from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker
                 from transformers import AutoFeatureExtractor
-                self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(
-                    config.models_path / CHECKER_PATH
-                )
-                self.feature_extractor = AutoFeatureExtractor.from_pretrained(
-                    config.models_path / CHECKER_PATH)
-                logger.info('NSFW checker initialized')
+
+                self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(config.models_path / CHECKER_PATH)
+                self.feature_extractor = AutoFeatureExtractor.from_pretrained(config.models_path / CHECKER_PATH)
+                logger.info("NSFW checker initialized")
             except Exception as e:
-                logger.warning(f'Could not load NSFW checker: {str(e)}')
+                logger.warning(f"Could not load NSFW checker: {str(e)}")
         else:
-            logger.info('NSFW checker loading disabled')
+            logger.info("NSFW checker loading disabled")
         self.tried_load = True
 
     @classmethod
     def safety_checker_available(self) -> bool:
         self._load_safety_checker()
         return self.safety_checker is not None
 
     @classmethod
     def has_nsfw_concept(self, image: Image) -> bool:
         if not self.safety_checker_available():
             return False
-        
+
         device = choose_torch_device()
         features = self.feature_extractor([image], return_tensors="pt")
         features.to(device)
         self.safety_checker.to(device)
         x_image = np.array(image).astype(np.float32) / 255.0
         x_image = x_image[None].transpose(0, 3, 1, 2)
         with SilenceWarnings():
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/image_util/seamless.py` & `InvokeAI-3.0.1rc2/invokeai/backend/image_util/seamless.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,20 +1,16 @@
 import torch.nn as nn
 
 
 def _conv_forward_asymmetric(self, input, weight, bias):
     """
     Patch for Conv2d._conv_forward that supports asymmetric padding
     """
-    working = nn.functional.pad(
-        input, self.asymmetric_padding["x"], mode=self.asymmetric_padding_mode["x"]
-    )
-    working = nn.functional.pad(
-        working, self.asymmetric_padding["y"], mode=self.asymmetric_padding_mode["y"]
-    )
+    working = nn.functional.pad(input, self.asymmetric_padding["x"], mode=self.asymmetric_padding_mode["x"])
+    working = nn.functional.pad(working, self.asymmetric_padding["y"], mode=self.asymmetric_padding_mode["y"])
     return nn.functional.conv2d(
         working,
         weight,
         bias,
         self.stride,
         nn.modules.utils._pair(0),
         self.dilation,
@@ -28,26 +24,22 @@
     """
     # TODO: get an explicit interface for this in diffusers: https://github.com/huggingface/diffusers/issues/556
     for m in model.modules():
         if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
             if seamless:
                 m.asymmetric_padding_mode = {}
                 m.asymmetric_padding = {}
-                m.asymmetric_padding_mode["x"] = (
-                    "circular" if ("x" in seamless_axes) else "constant"
-                )
+                m.asymmetric_padding_mode["x"] = "circular" if ("x" in seamless_axes) else "constant"
                 m.asymmetric_padding["x"] = (
                     m._reversed_padding_repeated_twice[0],
                     m._reversed_padding_repeated_twice[1],
                     0,
                     0,
                 )
-                m.asymmetric_padding_mode["y"] = (
-                    "circular" if ("y" in seamless_axes) else "constant"
-                )
+                m.asymmetric_padding_mode["y"] = "circular" if ("y" in seamless_axes) else "constant"
                 m.asymmetric_padding["y"] = (
                     0,
                     0,
                     m._reversed_padding_repeated_twice[2],
                     m._reversed_padding_repeated_twice[3],
                 )
                 m._conv_forward = _conv_forward_asymmetric.__get__(m, nn.Conv2d)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/image_util/txt2mask.py` & `InvokeAI-3.0.1rc2/invokeai/backend/image_util/txt2mask.py`

 * *Files 4% similar despite different names*

```diff
@@ -35,47 +35,38 @@
 import invokeai.backend.util.logging as logger
 from invokeai.app.services.config import InvokeAIAppConfig
 
 CLIPSEG_MODEL = "CIDAS/clipseg-rd64-refined"
 CLIPSEG_SIZE = 352
 config = InvokeAIAppConfig.get_config()
 
+
 class SegmentedGrayscale(object):
     def __init__(self, image: Image, heatmap: torch.Tensor):
         self.heatmap = heatmap
         self.image = image
 
     def to_grayscale(self, invert: bool = False) -> Image:
-        return self._rescale(
-            Image.fromarray(
-                np.uint8(255 - self.heatmap * 255 if invert else self.heatmap * 255)
-            )
-        )
+        return self._rescale(Image.fromarray(np.uint8(255 - self.heatmap * 255 if invert else self.heatmap * 255)))
 
     def to_mask(self, threshold: float = 0.5) -> Image:
         discrete_heatmap = self.heatmap.lt(threshold).int()
-        return self._rescale(
-            Image.fromarray(np.uint8(discrete_heatmap * 255), mode="L")
-        )
+        return self._rescale(Image.fromarray(np.uint8(discrete_heatmap * 255), mode="L"))
 
     def to_transparent(self, invert: bool = False) -> Image:
         transparent_image = self.image.copy()
         # For img2img, we want the selected regions to be transparent,
         # but to_grayscale() returns the opposite. Thus invert.
         gs = self.to_grayscale(not invert)
         transparent_image.putalpha(gs)
         return transparent_image
 
     # unscales and uncrops the 352x352 heatmap so that it matches the image again
     def _rescale(self, heatmap: Image) -> Image:
-        size = (
-            self.image.width
-            if (self.image.width > self.image.height)
-            else self.image.height
-        )
+        size = self.image.width if (self.image.width > self.image.height) else self.image.height
         resized_image = heatmap.resize((size, size), resample=Image.Resampling.LANCZOS)
         return resized_image.crop((0, 0, self.image.width, self.image.height))
 
 
 class Txt2Mask(object):
     """
     Create new Txt2Mask object. The optional device argument can be one of
@@ -83,37 +74,31 @@
     """
 
     def __init__(self, device="cpu", refined=False):
         logger.info("Initializing clipseg model for text to mask inference")
 
         # BUG: we are not doing anything with the device option at this time
         self.device = device
-        self.processor = AutoProcessor.from_pretrained(
-            CLIPSEG_MODEL, cache_dir=config.cache_dir
-        )
-        self.model = CLIPSegForImageSegmentation.from_pretrained(
-            CLIPSEG_MODEL, cache_dir=config.cache_dir
-        )
+        self.processor = AutoProcessor.from_pretrained(CLIPSEG_MODEL, cache_dir=config.cache_dir)
+        self.model = CLIPSegForImageSegmentation.from_pretrained(CLIPSEG_MODEL, cache_dir=config.cache_dir)
 
     @torch.no_grad()
     def segment(self, image, prompt: str) -> SegmentedGrayscale:
         """
         Given a prompt string such as "a bagel", tries to identify the object in the
         provided image and returns a SegmentedGrayscale object in which the brighter
         pixels indicate where the object is inferred to be.
         """
         if type(image) is str:
             image = Image.open(image).convert("RGB")
 
         image = ImageOps.exif_transpose(image)
         img = self._scale_and_crop(image)
 
-        inputs = self.processor(
-            text=[prompt], images=[img], padding=True, return_tensors="pt"
-        )
+        inputs = self.processor(text=[prompt], images=[img], padding=True, return_tensors="pt")
         outputs = self.model(**inputs)
         heatmap = torch.sigmoid(outputs.logits)
         return SegmentedGrayscale(image, heatmap)
 
     def _scale_and_crop(self, image: Image) -> Image:
         scaled_image = Image.new("RGB", (CLIPSEG_SIZE, CLIPSEG_SIZE))
         if image.width > image.height:  # width is constraint
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/image_util/util.py` & `InvokeAI-3.0.1rc2/invokeai/backend/image_util/util.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/install/check_root.py` & `InvokeAI-3.0.1rc2/invokeai/backend/install/check_root.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,32 +2,35 @@
 Check that the invokeai_root is correctly configured and exit if not.
 """
 import sys
 from invokeai.app.services.config import (
     InvokeAIAppConfig,
 )
 
+
 def check_invokeai_root(config: InvokeAIAppConfig):
     try:
-        assert config.model_conf_path.exists(), f'{config.model_conf_path} not found'
-        assert config.db_path.parent.exists(), f'{config.db_path.parent} not found'
-        assert config.models_path.exists(), f'{config.models_path} not found'
+        assert config.model_conf_path.exists(), f"{config.model_conf_path} not found"
+        assert config.db_path.parent.exists(), f"{config.db_path.parent} not found"
+        assert config.models_path.exists(), f"{config.models_path} not found"
         for model in [
-                'CLIP-ViT-bigG-14-laion2B-39B-b160k',
-                'bert-base-uncased',
-                'clip-vit-large-patch14',
-                'sd-vae-ft-mse',
-                'stable-diffusion-2-clip',
-                'stable-diffusion-safety-checker']:
-            path = config.models_path / f'core/convert/{model}'
-            assert path.exists(), f'{path} is missing'
+            "CLIP-ViT-bigG-14-laion2B-39B-b160k",
+            "bert-base-uncased",
+            "clip-vit-large-patch14",
+            "sd-vae-ft-mse",
+            "stable-diffusion-2-clip",
+            "stable-diffusion-safety-checker",
+        ]:
+            path = config.models_path / f"core/convert/{model}"
+            assert path.exists(), f"{path} is missing"
     except Exception as e:
         print()
-        print(f'An exception has occurred: {str(e)}')
-        print('== STARTUP ABORTED ==')
-        print('** One or more necessary files is missing from your InvokeAI root directory **')
-        print('** Please rerun the configuration script to fix this problem. **')
-        print('** From the launcher, selection option [7]. **')
-        print('** From the command line, activate the virtual environment and run "invokeai-configure --yes --skip-sd-weights" **')
-        input('Press any key to continue...')
+        print(f"An exception has occurred: {str(e)}")
+        print("== STARTUP ABORTED ==")
+        print("** One or more necessary files is missing from your InvokeAI root directory **")
+        print("** Please rerun the configuration script to fix this problem. **")
+        print("** From the launcher, selection option [7]. **")
+        print(
+            '** From the command line, activate the virtual environment and run "invokeai-configure --yes --skip-sd-weights" **'
+        )
+        input("Press any key to continue...")
         sys.exit(0)
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/install/invokeai_configure.py` & `InvokeAI-3.0.1rc2/invokeai/backend/install/invokeai_configure.py`

 * *Files 5% similar despite different names*

```diff
@@ -56,40 +56,39 @@
 )
 from invokeai.backend.install.legacy_arg_parsing import legacy_parser
 from invokeai.backend.install.model_install_backend import (
     hf_download_from_pretrained,
     InstallSelections,
     ModelInstall,
 )
-from invokeai.backend.model_management.model_probe import (
-    ModelType, BaseModelType
-    )
+from invokeai.backend.model_management.model_probe import ModelType, BaseModelType
 
 warnings.filterwarnings("ignore")
 transformers.logging.set_verbosity_error()
 
 
 # --------------------------globals-----------------------
 
 config = InvokeAIAppConfig.get_config()
 
 Model_dir = "models"
 
 Default_config_file = config.model_conf_path
 SD_Configs = config.legacy_conf_path
 
-PRECISION_CHOICES = ['auto','float16','float32']
+PRECISION_CHOICES = ["auto", "float16", "float32"]
 
 INIT_FILE_PREAMBLE = """# InvokeAI initialization file
 # This is the InvokeAI initialization file, which contains command-line default values.
 # Feel free to edit. If anything goes wrong, you can re-initialize this file by deleting
 # or renaming it and then running invokeai-configure again.
 """
 
-logger=InvokeAILogger.getLogger()
+logger = InvokeAILogger.getLogger()
+
 
 # --------------------------------------------
 def postscript(errors: None):
     if not any(errors):
         message = f"""
 ** INVOKEAI INSTALLATION SUCCESSFUL **
 If you installed manually from source or with 'pip install': activate the virtual environment
@@ -104,15 +103,17 @@
 If you installed using an installation script, run:
   {config.root_path}/invoke.{"bat" if sys.platform == "win32" else "sh"}
 
 Add the '--help' argument to see all of the command-line switches available for use.
 """
 
     else:
-        message = "\n** There were errors during installation. It is possible some of the models were not fully downloaded.\n"
+        message = (
+            "\n** There were errors during installation. It is possible some of the models were not fully downloaded.\n"
+        )
         for err in errors:
             message += f"\t - {err}\n"
         message += "Please check the logs above and correct any issues."
 
     print(message)
 
 
@@ -165,146 +166,148 @@
 
 # ---------------------------------------------
 def download_with_progress_bar(model_url: str, model_dest: str, label: str = "the"):
     try:
         logger.info(f"Installing {label} model file {model_url}...")
         if not os.path.exists(model_dest):
             os.makedirs(os.path.dirname(model_dest), exist_ok=True)
-            request.urlretrieve(
-                model_url, model_dest, ProgressBar(os.path.basename(model_dest))
-            )
+            request.urlretrieve(model_url, model_dest, ProgressBar(os.path.basename(model_dest)))
             logger.info("...downloaded successfully")
         else:
             logger.info("...exists")
     except Exception:
         logger.info("...download failed")
         logger.info(f"Error downloading {label} model")
         print(traceback.format_exc(), file=sys.stderr)
 
 
 def download_conversion_models():
-    target_dir = config.root_path / 'models/core/convert'
+    target_dir = config.root_path / "models/core/convert"
     kwargs = dict()  # for future use
     try:
-        logger.info('Downloading core tokenizers and text encoders')
+        logger.info("Downloading core tokenizers and text encoders")
 
         # bert
         with warnings.catch_warnings():
             warnings.filterwarnings("ignore", category=DeprecationWarning)
             bert = BertTokenizerFast.from_pretrained("bert-base-uncased", **kwargs)
-            bert.save_pretrained(target_dir / 'bert-base-uncased', safe_serialization=True)
-        
+            bert.save_pretrained(target_dir / "bert-base-uncased", safe_serialization=True)
+
         # sd-1
-        repo_id = 'openai/clip-vit-large-patch14'
-        hf_download_from_pretrained(CLIPTokenizer, repo_id, target_dir / 'clip-vit-large-patch14')
-        hf_download_from_pretrained(CLIPTextModel, repo_id, target_dir / 'clip-vit-large-patch14')
+        repo_id = "openai/clip-vit-large-patch14"
+        hf_download_from_pretrained(CLIPTokenizer, repo_id, target_dir / "clip-vit-large-patch14")
+        hf_download_from_pretrained(CLIPTextModel, repo_id, target_dir / "clip-vit-large-patch14")
 
         # sd-2
         repo_id = "stabilityai/stable-diffusion-2"
         pipeline = CLIPTokenizer.from_pretrained(repo_id, subfolder="tokenizer", **kwargs)
-        pipeline.save_pretrained(target_dir / 'stable-diffusion-2-clip' / 'tokenizer', safe_serialization=True)
+        pipeline.save_pretrained(target_dir / "stable-diffusion-2-clip" / "tokenizer", safe_serialization=True)
 
         pipeline = CLIPTextModel.from_pretrained(repo_id, subfolder="text_encoder", **kwargs)
-        pipeline.save_pretrained(target_dir / 'stable-diffusion-2-clip' / 'text_encoder', safe_serialization=True)
+        pipeline.save_pretrained(target_dir / "stable-diffusion-2-clip" / "text_encoder", safe_serialization=True)
 
         # sd-xl - tokenizer_2
         repo_id = "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"
-        _, model_name = repo_id.split('/')
+        _, model_name = repo_id.split("/")
         pipeline = CLIPTokenizer.from_pretrained(repo_id, **kwargs)
         pipeline.save_pretrained(target_dir / model_name, safe_serialization=True)
-        
+
         pipeline = CLIPTextConfig.from_pretrained(repo_id, **kwargs)
         pipeline.save_pretrained(target_dir / model_name, safe_serialization=True)
-        
+
         # VAE
-        logger.info('Downloading stable diffusion VAE')
-        vae = AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-mse', **kwargs)
-        vae.save_pretrained(target_dir / 'sd-vae-ft-mse', safe_serialization=True)
+        logger.info("Downloading stable diffusion VAE")
+        vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse", **kwargs)
+        vae.save_pretrained(target_dir / "sd-vae-ft-mse", safe_serialization=True)
 
         # safety checking
-        logger.info('Downloading safety checker')
+        logger.info("Downloading safety checker")
         repo_id = "CompVis/stable-diffusion-safety-checker"
-        pipeline = AutoFeatureExtractor.from_pretrained(repo_id,**kwargs)
-        pipeline.save_pretrained(target_dir / 'stable-diffusion-safety-checker', safe_serialization=True)
+        pipeline = AutoFeatureExtractor.from_pretrained(repo_id, **kwargs)
+        pipeline.save_pretrained(target_dir / "stable-diffusion-safety-checker", safe_serialization=True)
 
-        pipeline = StableDiffusionSafetyChecker.from_pretrained(repo_id,**kwargs)
-        pipeline.save_pretrained(target_dir / 'stable-diffusion-safety-checker', safe_serialization=True)
+        pipeline = StableDiffusionSafetyChecker.from_pretrained(repo_id, **kwargs)
+        pipeline.save_pretrained(target_dir / "stable-diffusion-safety-checker", safe_serialization=True)
     except KeyboardInterrupt:
         raise
     except Exception as e:
         logger.error(str(e))
 
+
 # ---------------------------------------------
 def download_realesrgan():
     logger.info("Installing ESRGAN Upscaling models...")
     URLs = [
         dict(
-            url = "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth",
-            dest = "core/upscaling/realesrgan/RealESRGAN_x4plus.pth",
-            description = "RealESRGAN_x4plus.pth",
+            url="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth",
+            dest="core/upscaling/realesrgan/RealESRGAN_x4plus.pth",
+            description="RealESRGAN_x4plus.pth",
         ),
         dict(
-            url = "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth",
-            dest = "core/upscaling/realesrgan/RealESRGAN_x4plus_anime_6B.pth",
-            description = "RealESRGAN_x4plus_anime_6B.pth",
+            url="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth",
+            dest="core/upscaling/realesrgan/RealESRGAN_x4plus_anime_6B.pth",
+            description="RealESRGAN_x4plus_anime_6B.pth",
         ),
         dict(
-            url= "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/ESRGAN_SRx4_DF2KOST_official-ff704c30.pth",
-            dest= "core/upscaling/realesrgan/ESRGAN_SRx4_DF2KOST_official-ff704c30.pth",
-            description = "ESRGAN_SRx4_DF2KOST_official.pth",
+            url="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/ESRGAN_SRx4_DF2KOST_official-ff704c30.pth",
+            dest="core/upscaling/realesrgan/ESRGAN_SRx4_DF2KOST_official-ff704c30.pth",
+            description="ESRGAN_SRx4_DF2KOST_official.pth",
         ),
         dict(
-            url= "https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth",
-            dest= "core/upscaling/realesrgan/RealESRGAN_x2plus.pth",
-            description = "RealESRGAN_x2plus.pth",
+            url="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth",
+            dest="core/upscaling/realesrgan/RealESRGAN_x2plus.pth",
+            description="RealESRGAN_x2plus.pth",
         ),
     ]
     for model in URLs:
-        download_with_progress_bar(model['url'], config.models_path / model['dest'], model['description'])
+        download_with_progress_bar(model["url"], config.models_path / model["dest"], model["description"])
+
 
 # ---------------------------------------------
 def download_support_models():
     download_realesrgan()
     download_conversion_models()
 
+
 # -------------------------------------
 def get_root(root: str = None) -> str:
     if root:
         return root
     elif os.environ.get("INVOKEAI_ROOT"):
         return os.environ.get("INVOKEAI_ROOT")
     else:
         return str(config.root_path)
 
+
 # -------------------------------------
 class editOptsForm(CyclingForm, npyscreen.FormMultiPage):
     # for responsive resizing - disabled
     # FIX_MINIMUM_SIZE_WHEN_CREATED = False
 
     def create(self):
         program_opts = self.parentApp.program_opts
         old_opts = self.parentApp.invokeai_opts
-        first_time = not (config.root_path / 'invokeai.yaml').exists()
+        first_time = not (config.root_path / "invokeai.yaml").exists()
         access_token = HfFolder.get_token()
         window_width, window_height = get_terminal_size()
         label = """Configure startup settings. You can come back and change these later. 
 Use ctrl-N and ctrl-P to move to the <N>ext and <P>revious fields.
 Use cursor arrows to make a checkbox selection, and space to toggle.
 """
-        for i in textwrap.wrap(label,width=window_width-6):
+        for i in textwrap.wrap(label, width=window_width - 6):
             self.add_widget_intelligent(
                 npyscreen.FixedText,
                 value=i,
                 editable=False,
                 color="CONTROL",
             )
 
         self.nextrely += 1
         label = """HuggingFace access token (OPTIONAL) for automatic model downloads. See https://huggingface.co/settings/tokens."""
-        for line in textwrap.wrap(label,width=window_width-6):
+        for line in textwrap.wrap(label, width=window_width - 6):
             self.add_widget_intelligent(
                 npyscreen.FixedText,
                 value=line,
                 editable=False,
                 color="CONTROL",
             )
 
@@ -339,38 +342,36 @@
             npyscreen.Checkbox,
             name="Enable xformers support",
             value=old_opts.xformers_enabled,
             max_width=30,
             relx=50,
             scroll_exit=True,
         )
-        self.nextrely -=1
+        self.nextrely -= 1
         self.always_use_cpu = self.add_widget_intelligent(
             npyscreen.Checkbox,
             name="Force CPU to be used on GPU systems",
             value=old_opts.always_use_cpu,
             relx=80,
             scroll_exit=True,
         )
-        precision = old_opts.precision or (
-            "float32" if program_opts.full_precision else "auto"
-        )
-        self.nextrely +=1
+        precision = old_opts.precision or ("float32" if program_opts.full_precision else "auto")
+        self.nextrely += 1
         self.add_widget_intelligent(
             npyscreen.TitleFixedText,
             name="Floating Point Precision",
             begin_entry_at=0,
             editable=False,
             color="CONTROL",
             scroll_exit=True,
         )
-        self.nextrely -=1
+        self.nextrely -= 1
         self.precision = self.add_widget_intelligent(
             SingleSelectColumns,
-            columns = 3,
+            columns=3,
             name="Precision",
             values=PRECISION_CHOICES,
             value=PRECISION_CHOICES.index(precision),
             begin_entry_at=3,
             max_height=2,
             max_width=80,
             scroll_exit=True,
@@ -394,52 +395,48 @@
             use_two_lines=False,
             labelColor="GOOD",
             begin_entry_at=40,
             max_height=3,
             scroll_exit=True,
         )
         self.autoimport_dirs = {}
-        self.autoimport_dirs['autoimport_dir'] = self.add_widget_intelligent(
-                FileBox,
-                name=f'Folder to recursively scan for new checkpoints, ControlNets, LoRAs and TI models',
-                value=str(config.root_path / config.autoimport_dir),
-                select_dir=True,
-                must_exist=False,
-                use_two_lines=False,
-                labelColor="GOOD",
-                begin_entry_at=32,
-                max_height = 3,
-                scroll_exit=True
-            )
+        self.autoimport_dirs["autoimport_dir"] = self.add_widget_intelligent(
+            FileBox,
+            name=f"Folder to recursively scan for new checkpoints, ControlNets, LoRAs and TI models",
+            value=str(config.root_path / config.autoimport_dir),
+            select_dir=True,
+            must_exist=False,
+            use_two_lines=False,
+            labelColor="GOOD",
+            begin_entry_at=32,
+            max_height=3,
+            scroll_exit=True,
+        )
         self.nextrely += 1
         label = """BY DOWNLOADING THE STABLE DIFFUSION WEIGHT FILES, YOU AGREE TO HAVE READ
 AND ACCEPTED THE CREATIVEML RESPONSIBLE AI LICENSES LOCATED AT
 https://huggingface.co/spaces/CompVis/stable-diffusion-license and
 https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/LICENSE.md
 """
-        for i in textwrap.wrap(label,width=window_width-6):
+        for i in textwrap.wrap(label, width=window_width - 6):
             self.add_widget_intelligent(
                 npyscreen.FixedText,
                 value=i,
                 editable=False,
                 color="CONTROL",
             )
         self.license_acceptance = self.add_widget_intelligent(
             npyscreen.Checkbox,
             name="I accept the CreativeML Responsible AI Licenses",
             value=not first_time,
             relx=2,
             scroll_exit=True,
         )
         self.nextrely += 1
-        label = (
-            "DONE"
-            if program_opts.skip_sd_weights or program_opts.default_only
-            else "NEXT"
-        )
+        label = "DONE" if program_opts.skip_sd_weights or program_opts.default_only else "NEXT"
         self.ok_button = self.add_widget_intelligent(
             CenteredButtonPress,
             name=label,
             relx=(window_width - len(label)) // 2,
             when_pressed_function=self.on_ok,
         )
 
@@ -450,21 +447,19 @@
             if hasattr(self.parentApp, "model_select"):
                 self.parentApp.setNextForm("MODELS")
             else:
                 self.parentApp.setNextForm(None)
             self.editing = False
         else:
             self.editing = True
-            
+
     def validate_field_values(self, opt: Namespace) -> bool:
         bad_fields = []
         if not opt.license_acceptance:
-            bad_fields.append(
-                "Please accept the license terms before proceeding to model downloads"
-            )
+            bad_fields.append("Please accept the license terms before proceeding to model downloads")
         if not Path(opt.outdir).parent.exists():
             bad_fields.append(
                 f"The output directory does not seem to be valid. Please check that {str(Path(opt.outdir).parent)} is an existing directory."
             )
         if len(bad_fields) > 0:
             message = "The following problems were detected and must be corrected:\n"
             for problem in bad_fields:
@@ -474,32 +469,32 @@
         else:
             return True
 
     def marshall_arguments(self):
         new_opts = Namespace()
 
         for attr in [
-                "outdir",
-                "free_gpu_mem",
-                "max_cache_size",
-                "xformers_enabled",
-                "always_use_cpu",
+            "outdir",
+            "free_gpu_mem",
+            "max_cache_size",
+            "xformers_enabled",
+            "always_use_cpu",
         ]:
             setattr(new_opts, attr, getattr(self, attr).value)
 
         for attr in self.autoimport_dirs:
             directory = Path(self.autoimport_dirs[attr].value)
             if directory.is_relative_to(config.root_path):
                 directory = directory.relative_to(config.root_path)
             setattr(new_opts, attr, directory)
 
         new_opts.hf_token = self.hf_token.value
         new_opts.license_acceptance = self.license_acceptance.value
         new_opts.precision = PRECISION_CHOICES[self.precision.value[0]]
-        
+
         return new_opts
 
 
 class EditOptApplication(npyscreen.NPSAppManaged):
     def __init__(self, program_opts: Namespace, invokeai_opts: Namespace):
         super().__init__()
         self.program_opts = program_opts
@@ -530,98 +525,91 @@
 
 
 def edit_opts(program_opts: Namespace, invokeai_opts: Namespace) -> argparse.Namespace:
     editApp = EditOptApplication(program_opts, invokeai_opts)
     editApp.run()
     return editApp.new_opts()
 
+
 def default_startup_options(init_file: Path) -> Namespace:
     opts = InvokeAIAppConfig.get_config()
     return opts
 
+
 def default_user_selections(program_opts: Namespace) -> InstallSelections:
-    
     try:
         installer = ModelInstall(config)
     except omegaconf.errors.ConfigKeyError:
-        logger.warning('Your models.yaml file is corrupt or out of date. Reinitializing')
+        logger.warning("Your models.yaml file is corrupt or out of date. Reinitializing")
         initialize_rootdir(config.root_path, True)
         installer = ModelInstall(config)
-        
+
     models = installer.all_models()
     return InstallSelections(
         install_models=[models[installer.default_model()].path or models[installer.default_model()].repo_id]
         if program_opts.default_only
         else [models[x].path or models[x].repo_id for x in installer.recommended_models()]
         if program_opts.yes_to_all
         else list(),
     )
 
+
 # -------------------------------------
 def initialize_rootdir(root: Path, yes_to_all: bool = False):
     logger.info("Initializing InvokeAI runtime directory")
-    for name in (
-            "models",
-            "databases",
-            "text-inversion-output",
-            "text-inversion-training-data",
-            "configs"
-    ):
+    for name in ("models", "databases", "text-inversion-output", "text-inversion-training-data", "configs"):
         os.makedirs(os.path.join(root, name), exist_ok=True)
     for model_type in ModelType:
-        Path(root, 'autoimport', model_type.value).mkdir(parents=True, exist_ok=True)
+        Path(root, "autoimport", model_type.value).mkdir(parents=True, exist_ok=True)
 
     configs_src = Path(configs.__path__[0])
     configs_dest = root / "configs"
     if not os.path.samefile(configs_src, configs_dest):
         shutil.copytree(configs_src, configs_dest, dirs_exist_ok=True)
 
-    dest = root / 'models'
+    dest = root / "models"
     for model_base in BaseModelType:
         for model_type in ModelType:
             path = dest / model_base.value / model_type.value
             path.mkdir(parents=True, exist_ok=True)
-    path = dest / 'core'
+    path = dest / "core"
     path.mkdir(parents=True, exist_ok=True)
 
     maybe_create_models_yaml(root)
 
+
 def maybe_create_models_yaml(root: Path):
-    models_yaml = root / 'configs' / 'models.yaml'
+    models_yaml = root / "configs" / "models.yaml"
     if models_yaml.exists():
-        if OmegaConf.load(models_yaml).get('__metadata__'):  # up to date
+        if OmegaConf.load(models_yaml).get("__metadata__"):  # up to date
             return
         else:
-            logger.info('Creating new models.yaml, original saved as models.yaml.orig')
-            models_yaml.rename(models_yaml.parent / 'models.yaml.orig')
-    
-    with open(models_yaml,'w') as yaml_file:
-        yaml_file.write(yaml.dump({'__metadata__':
-                                   {'version':'3.0.0'}
-                                   }
-                                  )
-                        )
-        
-# -------------------------------------
-def run_console_ui(
-    program_opts: Namespace, initfile: Path = None
-) -> (Namespace, Namespace):
+            logger.info("Creating new models.yaml, original saved as models.yaml.orig")
+            models_yaml.rename(models_yaml.parent / "models.yaml.orig")
+
+    with open(models_yaml, "w") as yaml_file:
+        yaml_file.write(yaml.dump({"__metadata__": {"version": "3.0.0"}}))
+
+
+# -------------------------------------
+def run_console_ui(program_opts: Namespace, initfile: Path = None) -> (Namespace, Namespace):
     # parse_args() will read from init file if present
     invokeai_opts = default_startup_options(initfile)
     invokeai_opts.root = program_opts.root
 
     # The third argument is needed in the Windows 11 environment to
     # launch a console window running this program.
     set_min_terminal_size(MIN_COLS, MIN_LINES)
 
     # the install-models application spawns a subprocess to install
     # models, and will crash unless this is set before running.
     import torch
+
     torch.multiprocessing.set_start_method("spawn")
-    
+
     editApp = EditOptApplication(program_opts, invokeai_opts)
     editApp.run()
     if editApp.user_cancelled:
         return (None, None)
     else:
         return (editApp.new_opts, editApp.install_selections)
 
@@ -630,88 +618,94 @@
 def write_opts(opts: Namespace, init_file: Path):
     """
     Update the invokeai.yaml file with values from current settings.
     """
     # this will load current settings
     new_config = InvokeAIAppConfig.get_config()
     new_config.root = config.root
-    
-    for key,value in opts.__dict__.items():
-        if hasattr(new_config,key):
-            setattr(new_config,key,value)
 
-    with open(init_file,'w', encoding='utf-8') as file:
+    for key, value in opts.__dict__.items():
+        if hasattr(new_config, key):
+            setattr(new_config, key, value)
+
+    with open(init_file, "w", encoding="utf-8") as file:
         file.write(new_config.to_yaml())
 
-    if hasattr(opts,'hf_token') and opts.hf_token:
+    if hasattr(opts, "hf_token") and opts.hf_token:
         HfLogin(opts.hf_token)
 
+
 # -------------------------------------
 def default_output_dir() -> Path:
     return config.root_path / "outputs"
 
+
 # -------------------------------------
 def write_default_options(program_opts: Namespace, initfile: Path):
     opt = default_startup_options(initfile)
     write_opts(opt, initfile)
 
+
 # -------------------------------------
 # Here we bring in
 # the legacy Args object in order to parse
 # the old init file and write out the new
 # yaml format.
-def migrate_init_file(legacy_format:Path):
-    old = legacy_parser.parse_args([f'@{str(legacy_format)}'])
+def migrate_init_file(legacy_format: Path):
+    old = legacy_parser.parse_args([f"@{str(legacy_format)}"])
     new = InvokeAIAppConfig.get_config()
 
     fields = list(get_type_hints(InvokeAIAppConfig).keys())
     for attr in fields:
-        if hasattr(old,attr):
-            setattr(new,attr,getattr(old,attr))
+        if hasattr(old, attr):
+            setattr(new, attr, getattr(old, attr))
 
     # a few places where the field names have changed and we have to
     # manually add in the new names/values
     new.xformers_enabled = old.xformers
     new.conf_path = old.conf
     new.root = legacy_format.parent.resolve()
 
-    invokeai_yaml = legacy_format.parent / 'invokeai.yaml'
-    with open(invokeai_yaml,"w", encoding="utf-8") as outfile:
+    invokeai_yaml = legacy_format.parent / "invokeai.yaml"
+    with open(invokeai_yaml, "w", encoding="utf-8") as outfile:
         outfile.write(new.to_yaml())
 
-    legacy_format.replace(legacy_format.parent / 'invokeai.init.orig')
+    legacy_format.replace(legacy_format.parent / "invokeai.init.orig")
+
 
 # -------------------------------------
 def migrate_models(root: Path):
     from invokeai.backend.install.migrate_to_3 import do_migrate
+
     do_migrate(root, root)
 
-def migrate_if_needed(opt: Namespace, root: Path)->bool:
+
+def migrate_if_needed(opt: Namespace, root: Path) -> bool:
     # We check for to see if the runtime directory is correctly initialized.
-    old_init_file = root / 'invokeai.init'
-    new_init_file = root / 'invokeai.yaml'
-    old_hub = root / 'models/hub'
-    migration_needed =  (old_init_file.exists() and not new_init_file.exists()) and old_hub.exists()
-    
-    if migration_needed:
-        if opt.yes_to_all or \
-            yes_or_no(f'{str(config.root_path)} appears to be a 2.3 format root directory. Convert to version 3.0?'):
+    old_init_file = root / "invokeai.init"
+    new_init_file = root / "invokeai.yaml"
+    old_hub = root / "models/hub"
+    migration_needed = (old_init_file.exists() and not new_init_file.exists()) and old_hub.exists()
 
-            logger.info('** Migrating invokeai.init to invokeai.yaml')
+    if migration_needed:
+        if opt.yes_to_all or yes_or_no(
+            f"{str(config.root_path)} appears to be a 2.3 format root directory. Convert to version 3.0?"
+        ):
+            logger.info("** Migrating invokeai.init to invokeai.yaml")
             migrate_init_file(old_init_file)
-            config.parse_args(argv=[],conf=OmegaConf.load(new_init_file))
+            config.parse_args(argv=[], conf=OmegaConf.load(new_init_file))
 
             if old_hub.exists():
                 migrate_models(config.root_path)
         else:
-            print('Cannot continue without conversion. Aborting.')
-    
+            print("Cannot continue without conversion. Aborting.")
+
     return migration_needed
 
-    
+
 # -------------------------------------
 def main():
     parser = argparse.ArgumentParser(description="InvokeAI model downloader")
     parser.add_argument(
         "--skip-sd-weights",
         dest="skip_sd_weights",
         action=argparse.BooleanOptionalAction,
@@ -760,17 +754,17 @@
         default=None,
         help="path to root of install directory",
     )
     opt = parser.parse_args()
 
     invoke_args = []
     if opt.root:
-        invoke_args.extend(['--root',opt.root])
+        invoke_args.extend(["--root", opt.root])
     if opt.full_precision:
-        invoke_args.extend(['--precision','float32'])
+        invoke_args.extend(["--precision", "float32"])
     config.parse_args(invoke_args)
     logger = InvokeAILogger().getLogger(config=config)
 
     errors = set()
 
     try:
         # if we do a root migration/upgrade, then we are keeping previous
@@ -778,44 +772,40 @@
         if migrate_if_needed(opt, config.root_path):
             sys.exit(0)
 
         # run this unconditionally in case new directories need to be added
         initialize_rootdir(config.root_path, opt.yes_to_all)
 
         models_to_download = default_user_selections(opt)
-        new_init_file = config.root_path / 'invokeai.yaml'
+        new_init_file = config.root_path / "invokeai.yaml"
         if opt.yes_to_all:
             write_default_options(opt, new_init_file)
-            init_options = Namespace(
-                precision="float32" if opt.full_precision else "float16"
-            )
+            init_options = Namespace(precision="float32" if opt.full_precision else "float16")
         else:
             init_options, models_to_download = run_console_ui(opt, new_init_file)
             if init_options:
                 write_opts(init_options, new_init_file)
             else:
-                logger.info(
-                    '\n** CANCELLED AT USER\'S REQUEST. USE THE "invoke.sh" LAUNCHER TO RUN LATER **\n'
-                )
+                logger.info('\n** CANCELLED AT USER\'S REQUEST. USE THE "invoke.sh" LAUNCHER TO RUN LATER **\n')
                 sys.exit(0)
-                
+
         if opt.skip_support_models:
             logger.info("Skipping support models at user's request")
         else:
             logger.info("Installing support models")
             download_support_models()
 
         if opt.skip_sd_weights:
             logger.warning("Skipping diffusion weights download per user request")
         elif models_to_download:
             process_and_execute(opt, models_to_download)
 
         postscript(errors=errors)
         if not opt.yes_to_all:
-            input('Press any key to continue...')
+            input("Press any key to continue...")
     except KeyboardInterrupt:
         print("\nGoodbye! Come back soon.")
 
 
 # -------------------------------------
 if __name__ == "__main__":
     main()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/install/legacy_arg_parsing.py` & `InvokeAI-3.0.1rc2/invokeai/backend/install/legacy_arg_parsing.py`

 * *Files 15% similar despite different names*

```diff
@@ -43,365 +43,336 @@
 PRECISION_CHOICES = [
     "auto",
     "float32",
     "autocast",
     "float16",
 ]
 
+
 class FileArgumentParser(ArgumentParser):
     """
     Supports reading defaults from an init file.
     """
+
     def convert_arg_line_to_args(self, arg_line):
         return shlex.split(arg_line, comments=True)
 
 
 legacy_parser = FileArgumentParser(
-    description=
-    """
+    description="""
 Generate images using Stable Diffusion.
     Use --web to launch the web interface.
     Use --from_file to load prompts from a file path or standard input ("-").
     Otherwise you will be dropped into an interactive command prompt (type -h for help.)
     Other command-line arguments are defaults that can usually be overridden
     prompt the command prompt.
     """,
-    fromfile_prefix_chars='@',
+    fromfile_prefix_chars="@",
 )
-general_group    = legacy_parser.add_argument_group('General')
-model_group      = legacy_parser.add_argument_group('Model selection')
-file_group       = legacy_parser.add_argument_group('Input/output')
-web_server_group = legacy_parser.add_argument_group('Web server')
-render_group     = legacy_parser.add_argument_group('Rendering')
-postprocessing_group     = legacy_parser.add_argument_group('Postprocessing')
-deprecated_group = legacy_parser.add_argument_group('Deprecated options')
+general_group = legacy_parser.add_argument_group("General")
+model_group = legacy_parser.add_argument_group("Model selection")
+file_group = legacy_parser.add_argument_group("Input/output")
+web_server_group = legacy_parser.add_argument_group("Web server")
+render_group = legacy_parser.add_argument_group("Rendering")
+postprocessing_group = legacy_parser.add_argument_group("Postprocessing")
+deprecated_group = legacy_parser.add_argument_group("Deprecated options")
 
-deprecated_group.add_argument('--laion400m')
-deprecated_group.add_argument('--weights') # deprecated
-general_group.add_argument(
-    '--version','-V',
-    action='store_true',
-    help='Print InvokeAI version number'
-)
+deprecated_group.add_argument("--laion400m")
+deprecated_group.add_argument("--weights")  # deprecated
+general_group.add_argument("--version", "-V", action="store_true", help="Print InvokeAI version number")
 model_group.add_argument(
-    '--root_dir',
+    "--root_dir",
     default=None,
     help='Path to directory containing "models", "outputs" and "configs". If not present will read from environment variable INVOKEAI_ROOT. Defaults to ~/invokeai.',
 )
 model_group.add_argument(
-    '--config',
-    '-c',
-    '-config',
-    dest='conf',
-    default='./configs/models.yaml',
-    help='Path to configuration file for alternate models.',
+    "--config",
+    "-c",
+    "-config",
+    dest="conf",
+    default="./configs/models.yaml",
+    help="Path to configuration file for alternate models.",
 )
 model_group.add_argument(
-    '--model',
+    "--model",
     help='Indicates which diffusion model to load (defaults to "default" stanza in configs/models.yaml)',
 )
 model_group.add_argument(
-    '--weight_dirs',
-    nargs='+',
+    "--weight_dirs",
+    nargs="+",
     type=str,
-    help='List of one or more directories that will be auto-scanned for new model weights to import',
+    help="List of one or more directories that will be auto-scanned for new model weights to import",
 )
 model_group.add_argument(
-    '--png_compression','-z',
+    "--png_compression",
+    "-z",
     type=int,
     default=6,
-    choices=range(0,9),
-    dest='png_compression',
-    help='level of PNG compression, from 0 (none) to 9 (maximum). Default is 6.'
+    choices=range(0, 9),
+    dest="png_compression",
+    help="level of PNG compression, from 0 (none) to 9 (maximum). Default is 6.",
 )
 model_group.add_argument(
-    '-F',
-    '--full_precision',
-    dest='full_precision',
-    action='store_true',
-    help='Deprecated way to set --precision=float32',
+    "-F",
+    "--full_precision",
+    dest="full_precision",
+    action="store_true",
+    help="Deprecated way to set --precision=float32",
 )
 model_group.add_argument(
-    '--max_loaded_models',
-    dest='max_loaded_models',
+    "--max_loaded_models",
+    dest="max_loaded_models",
     type=int,
     default=2,
-    help='Maximum number of models to keep in memory for fast switching, including the one in GPU',
+    help="Maximum number of models to keep in memory for fast switching, including the one in GPU",
 )
 model_group.add_argument(
-    '--free_gpu_mem',
-    dest='free_gpu_mem',
-    action='store_true',
-    help='Force free gpu memory before final decoding',
+    "--free_gpu_mem",
+    dest="free_gpu_mem",
+    action="store_true",
+    help="Force free gpu memory before final decoding",
 )
 model_group.add_argument(
-    '--sequential_guidance',
-    dest='sequential_guidance',
-    action='store_true',
-    help="Calculate guidance in serial instead of in parallel, lowering memory requirement "
-         "at the expense of speed",
+    "--sequential_guidance",
+    dest="sequential_guidance",
+    action="store_true",
+    help="Calculate guidance in serial instead of in parallel, lowering memory requirement " "at the expense of speed",
 )
 model_group.add_argument(
-    '--xformers',
+    "--xformers",
     action=argparse.BooleanOptionalAction,
     default=True,
-    help='Enable/disable xformers support (default enabled if installed)',
+    help="Enable/disable xformers support (default enabled if installed)",
 )
 model_group.add_argument(
-    "--always_use_cpu",
-    dest="always_use_cpu",
-    action="store_true",
-    help="Force use of CPU even if GPU is available"
+    "--always_use_cpu", dest="always_use_cpu", action="store_true", help="Force use of CPU even if GPU is available"
 )
 model_group.add_argument(
-    '--precision',
-    dest='precision',
+    "--precision",
+    dest="precision",
     type=str,
     choices=PRECISION_CHOICES,
-    metavar='PRECISION',
+    metavar="PRECISION",
     help=f'Set model precision. Defaults to auto selected based on device. Options: {", ".join(PRECISION_CHOICES)}',
-    default='auto',
+    default="auto",
 )
 model_group.add_argument(
-    '--ckpt_convert',
+    "--ckpt_convert",
     action=argparse.BooleanOptionalAction,
-    dest='ckpt_convert',
+    dest="ckpt_convert",
     default=True,
-    help='Deprecated option. Legacy ckpt files are now always converted to diffusers when loaded.'
+    help="Deprecated option. Legacy ckpt files are now always converted to diffusers when loaded.",
 )
 model_group.add_argument(
-    '--internet',
+    "--internet",
     action=argparse.BooleanOptionalAction,
-    dest='internet_available',
+    dest="internet_available",
     default=True,
-    help='Indicate whether internet is available for just-in-time model downloading (default: probe automatically).',
+    help="Indicate whether internet is available for just-in-time model downloading (default: probe automatically).",
 )
 model_group.add_argument(
-    '--nsfw_checker',
-    '--safety_checker',
+    "--nsfw_checker",
+    "--safety_checker",
     action=argparse.BooleanOptionalAction,
-    dest='safety_checker',
+    dest="safety_checker",
     default=False,
-    help='Check for and blur potentially NSFW images. Use --no-nsfw_checker to disable.',
+    help="Check for and blur potentially NSFW images. Use --no-nsfw_checker to disable.",
 )
 model_group.add_argument(
-    '--autoimport',
+    "--autoimport",
     default=None,
     type=str,
-    help='Check the indicated directory for .ckpt/.safetensors weights files at startup and import directly',
+    help="Check the indicated directory for .ckpt/.safetensors weights files at startup and import directly",
 )
 model_group.add_argument(
-    '--autoconvert',
+    "--autoconvert",
     default=None,
     type=str,
-    help='Check the indicated directory for .ckpt/.safetensors weights files at startup and import as optimized diffuser models',
+    help="Check the indicated directory for .ckpt/.safetensors weights files at startup and import as optimized diffuser models",
 )
 model_group.add_argument(
-    '--patchmatch',
+    "--patchmatch",
     action=argparse.BooleanOptionalAction,
     default=True,
-    help='Load the patchmatch extension for outpainting. Use --no-patchmatch to disable.',
+    help="Load the patchmatch extension for outpainting. Use --no-patchmatch to disable.",
 )
 file_group.add_argument(
-    '--from_file',
-    dest='infile',
+    "--from_file",
+    dest="infile",
     type=str,
-    help='If specified, load prompts from this file',
+    help="If specified, load prompts from this file",
 )
 file_group.add_argument(
-    '--outdir',
-    '-o',
+    "--outdir",
+    "-o",
     type=str,
-    help='Directory to save generated images and a log of prompts and seeds. Default: ROOTDIR/outputs',
-    default='outputs',
+    help="Directory to save generated images and a log of prompts and seeds. Default: ROOTDIR/outputs",
+    default="outputs",
 )
 file_group.add_argument(
-    '--prompt_as_dir',
-    '-p',
-    action='store_true',
-    help='Place images in subdirectories named after the prompt.',
+    "--prompt_as_dir",
+    "-p",
+    action="store_true",
+    help="Place images in subdirectories named after the prompt.",
 )
 render_group.add_argument(
-    '--fnformat',
-    default='{prefix}.{seed}.png',
+    "--fnformat",
+    default="{prefix}.{seed}.png",
     type=str,
-    help='Overwrite the filename format. You can use any argument as wildcard enclosed in curly braces. Default is {prefix}.{seed}.png',
-)
-render_group.add_argument(
-    '-s',
-    '--steps',
-    type=int,
-    default=50,
-    help='Number of steps'
+    help="Overwrite the filename format. You can use any argument as wildcard enclosed in curly braces. Default is {prefix}.{seed}.png",
 )
+render_group.add_argument("-s", "--steps", type=int, default=50, help="Number of steps")
 render_group.add_argument(
-    '-W',
-    '--width',
+    "-W",
+    "--width",
     type=int,
-    help='Image width, multiple of 64',
+    help="Image width, multiple of 64",
 )
 render_group.add_argument(
-    '-H',
-    '--height',
+    "-H",
+    "--height",
     type=int,
-    help='Image height, multiple of 64',
+    help="Image height, multiple of 64",
 )
 render_group.add_argument(
-    '-C',
-    '--cfg_scale',
+    "-C",
+    "--cfg_scale",
     default=7.5,
     type=float,
     help='Classifier free guidance (CFG) scale - higher numbers cause generator to "try" harder.',
 )
 render_group.add_argument(
-    '--sampler',
-    '-A',
-    '-m',
-    dest='sampler_name',
+    "--sampler",
+    "-A",
+    "-m",
+    dest="sampler_name",
     type=str,
     choices=SAMPLER_CHOICES,
-    metavar='SAMPLER_NAME',
+    metavar="SAMPLER_NAME",
     help=f'Set the default sampler. Supported samplers: {", ".join(SAMPLER_CHOICES)}',
-    default='k_lms',
+    default="k_lms",
 )
 render_group.add_argument(
-    '--log_tokenization',
-    '-t',
-    action='store_true',
-    help='shows how the prompt is split into tokens'
+    "--log_tokenization", "-t", action="store_true", help="shows how the prompt is split into tokens"
 )
 render_group.add_argument(
-    '-f',
-    '--strength',
+    "-f",
+    "--strength",
     type=float,
-    help='img2img strength for noising/unnoising. 0.0 preserves image exactly, 1.0 replaces it completely',
+    help="img2img strength for noising/unnoising. 0.0 preserves image exactly, 1.0 replaces it completely",
 )
 render_group.add_argument(
-    '-T',
-    '-fit',
-    '--fit',
+    "-T",
+    "-fit",
+    "--fit",
     action=argparse.BooleanOptionalAction,
-    help='If specified, will resize the input image to fit within the dimensions of width x height (512x512 default)',
+    help="If specified, will resize the input image to fit within the dimensions of width x height (512x512 default)",
 )
 
+render_group.add_argument("--grid", "-g", action=argparse.BooleanOptionalAction, help="generate a grid")
 render_group.add_argument(
-    '--grid',
-    '-g',
-    action=argparse.BooleanOptionalAction,
-    help='generate a grid'
-)
-render_group.add_argument(
-    '--embedding_directory',
-    '--embedding_path',
-    dest='embedding_path',
-    default='embeddings',
+    "--embedding_directory",
+    "--embedding_path",
+    dest="embedding_path",
+    default="embeddings",
     type=str,
-    help='Path to a directory containing .bin and/or .pt files, or a single .bin/.pt file. You may use subdirectories. (default is ROOTDIR/embeddings)'
+    help="Path to a directory containing .bin and/or .pt files, or a single .bin/.pt file. You may use subdirectories. (default is ROOTDIR/embeddings)",
 )
 render_group.add_argument(
-    '--lora_directory',
-    dest='lora_path',
-    default='loras',
+    "--lora_directory",
+    dest="lora_path",
+    default="loras",
     type=str,
-    help='Path to a directory containing LoRA files; subdirectories are not supported. (default is ROOTDIR/loras)'
+    help="Path to a directory containing LoRA files; subdirectories are not supported. (default is ROOTDIR/loras)",
 )
 render_group.add_argument(
-    '--embeddings',
+    "--embeddings",
     action=argparse.BooleanOptionalAction,
     default=True,
-    help='Enable embedding directory (default). Use --no-embeddings to disable.',
+    help="Enable embedding directory (default). Use --no-embeddings to disable.",
 )
+render_group.add_argument("--enable_image_debugging", action="store_true", help="Generates debugging image to display")
 render_group.add_argument(
-    '--enable_image_debugging',
-    action='store_true',
-    help='Generates debugging image to display'
-)
-render_group.add_argument(
-    '--karras_max',
+    "--karras_max",
     type=int,
     default=None,
-    help="control the point at which the K* samplers will shift from using the Karras noise schedule (good for low step counts) to the LatentDiffusion noise schedule (good for high step counts). Set to 0 to use LatentDiffusion for all step values, and to a high value (e.g. 1000) to use Karras for all step values. [29]."
+    help="control the point at which the K* samplers will shift from using the Karras noise schedule (good for low step counts) to the LatentDiffusion noise schedule (good for high step counts). Set to 0 to use LatentDiffusion for all step values, and to a high value (e.g. 1000) to use Karras for all step values. [29].",
 )
 # Restoration related args
 postprocessing_group.add_argument(
-    '--no_restore',
-    dest='restore',
-    action='store_false',
-    help='Disable face restoration with GFPGAN or codeformer',
+    "--no_restore",
+    dest="restore",
+    action="store_false",
+    help="Disable face restoration with GFPGAN or codeformer",
 )
 postprocessing_group.add_argument(
-    '--no_upscale',
-    dest='esrgan',
-    action='store_false',
-    help='Disable upscaling with ESRGAN',
+    "--no_upscale",
+    dest="esrgan",
+    action="store_false",
+    help="Disable upscaling with ESRGAN",
 )
 postprocessing_group.add_argument(
-    '--esrgan_bg_tile',
+    "--esrgan_bg_tile",
     type=int,
     default=400,
-    help='Tile size for background sampler, 0 for no tile during testing. Default: 400.',
+    help="Tile size for background sampler, 0 for no tile during testing. Default: 400.",
 )
 postprocessing_group.add_argument(
-    '--esrgan_denoise_str',
+    "--esrgan_denoise_str",
     type=float,
     default=0.75,
-    help='esrgan denoise str. 0 is no denoise, 1 is max denoise.  Default: 0.75',
+    help="esrgan denoise str. 0 is no denoise, 1 is max denoise.  Default: 0.75",
 )
 postprocessing_group.add_argument(
-    '--gfpgan_model_path',
+    "--gfpgan_model_path",
     type=str,
-    default='./models/gfpgan/GFPGANv1.4.pth',
-    help='Indicates the path to the GFPGAN model',
+    default="./models/gfpgan/GFPGANv1.4.pth",
+    help="Indicates the path to the GFPGAN model",
 )
 web_server_group.add_argument(
-    '--web',
-    dest='web',
-    action='store_true',
-    help='Start in web server mode.',
+    "--web",
+    dest="web",
+    action="store_true",
+    help="Start in web server mode.",
 )
 web_server_group.add_argument(
-    '--web_develop',
-    dest='web_develop',
-    action='store_true',
-    help='Start in web server development mode.',
+    "--web_develop",
+    dest="web_develop",
+    action="store_true",
+    help="Start in web server development mode.",
 )
 web_server_group.add_argument(
     "--web_verbose",
     action="store_true",
     help="Enables verbose logging",
 )
 web_server_group.add_argument(
     "--cors",
     nargs="*",
     type=str,
     help="Additional allowed origins, comma-separated",
 )
 web_server_group.add_argument(
-    '--host',
+    "--host",
     type=str,
-    default='127.0.0.1',
-    help='Web server: Host or IP to listen on. Set to 0.0.0.0 to accept traffic from other devices on your network.'
+    default="127.0.0.1",
+    help="Web server: Host or IP to listen on. Set to 0.0.0.0 to accept traffic from other devices on your network.",
 )
+web_server_group.add_argument("--port", type=int, default="9090", help="Web server: Port to listen on")
 web_server_group.add_argument(
-    '--port',
-    type=int,
-    default='9090',
-    help='Web server: Port to listen on'
-)
-web_server_group.add_argument(
-    '--certfile',
+    "--certfile",
     type=str,
     default=None,
-    help='Web server: Path to certificate file to use for SSL. Use together with --keyfile'
+    help="Web server: Path to certificate file to use for SSL. Use together with --keyfile",
 )
 web_server_group.add_argument(
-    '--keyfile',
+    "--keyfile",
     type=str,
     default=None,
-    help='Web server: Path to private key file to use for SSL. Use together with --certfile'
+    help="Web server: Path to private key file to use for SSL. Use together with --certfile",
 )
 web_server_group.add_argument(
-    '--gui',
-    dest='gui',
-    action='store_true',
-    help='Start InvokeAI GUI',
+    "--gui",
+    dest="gui",
+    action="store_true",
+    help="Start InvokeAI GUI",
 )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/install/migrate_to_3.py` & `InvokeAI-3.0.1rc2/invokeai/backend/install/migrate_to_3.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,11 @@
-'''
+"""
 Migrate the models directory and models.yaml file from an existing
 InvokeAI 2.3 installation to 3.0.0.
-'''
+"""
 
 import os
 import argparse
 import shutil
 import yaml
 
 import transformers
@@ -25,105 +25,105 @@
     AutoFeatureExtractor,
     BertTokenizerFast,
 )
 
 import invokeai.backend.util.logging as logger
 from invokeai.app.services.config import InvokeAIAppConfig
 from invokeai.backend.model_management import ModelManager
-from invokeai.backend.model_management.model_probe import (
-    ModelProbe, ModelType, BaseModelType, ModelProbeInfo
-    )
+from invokeai.backend.model_management.model_probe import ModelProbe, ModelType, BaseModelType, ModelProbeInfo
 
 warnings.filterwarnings("ignore")
 transformers.logging.set_verbosity_error()
 diffusers.logging.set_verbosity_error()
 
+
 # holder for paths that we will migrate
 @dataclass
 class ModelPaths:
     models: Path
     embeddings: Path
     loras: Path
     controlnets: Path
 
+
 class MigrateTo3(object):
-    def __init__(self,
-                 from_root: Path,
-                 to_models: Path,
-                 model_manager: ModelManager,
-                 src_paths: ModelPaths,
-                 ):
+    def __init__(
+        self,
+        from_root: Path,
+        to_models: Path,
+        model_manager: ModelManager,
+        src_paths: ModelPaths,
+    ):
         self.root_directory = from_root
         self.dest_models = to_models
         self.mgr = model_manager
         self.src_paths = src_paths
-        
+
     @classmethod
     def initialize_yaml(cls, yaml_file: Path):
-        with open(yaml_file, 'w') as file:
-            file.write(
-                yaml.dump(
-                    {
-                        '__metadata__': {'version':'3.0.0'}
-                    }
-                )
-            )
-    
+        with open(yaml_file, "w") as file:
+            file.write(yaml.dump({"__metadata__": {"version": "3.0.0"}}))
+
     def create_directory_structure(self):
-        '''
+        """
         Create the basic directory structure for the models folder.
-        '''
-        for model_base in [BaseModelType.StableDiffusion1,BaseModelType.StableDiffusion2]:
-            for model_type in [ModelType.Main, ModelType.Vae, ModelType.Lora,
-                               ModelType.ControlNet,ModelType.TextualInversion]:
+        """
+        for model_base in [BaseModelType.StableDiffusion1, BaseModelType.StableDiffusion2]:
+            for model_type in [
+                ModelType.Main,
+                ModelType.Vae,
+                ModelType.Lora,
+                ModelType.ControlNet,
+                ModelType.TextualInversion,
+            ]:
                 path = self.dest_models / model_base.value / model_type.value
                 path.mkdir(parents=True, exist_ok=True)
-        path = self.dest_models / 'core'
+        path = self.dest_models / "core"
         path.mkdir(parents=True, exist_ok=True)
 
     @staticmethod
-    def copy_file(src:Path,dest:Path):
-        '''
+    def copy_file(src: Path, dest: Path):
+        """
         copy a single file with logging
-        '''
+        """
         if dest.exists():
-            logger.info(f'Skipping existing {str(dest)}')
+            logger.info(f"Skipping existing {str(dest)}")
             return
-        logger.info(f'Copying {str(src)} to {str(dest)}')
+        logger.info(f"Copying {str(src)} to {str(dest)}")
         try:
             shutil.copy(src, dest)
         except Exception as e:
-            logger.error(f'COPY FAILED: {str(e)}')
+            logger.error(f"COPY FAILED: {str(e)}")
 
     @staticmethod
-    def copy_dir(src:Path,dest:Path):
-        '''
+    def copy_dir(src: Path, dest: Path):
+        """
         Recursively copy a directory with logging
-        '''
+        """
         if dest.exists():
-            logger.info(f'Skipping existing {str(dest)}')
+            logger.info(f"Skipping existing {str(dest)}")
             return
-        
-        logger.info(f'Copying {str(src)} to {str(dest)}')
+
+        logger.info(f"Copying {str(src)} to {str(dest)}")
         try:
             shutil.copytree(src, dest)
         except Exception as e:
-            logger.error(f'COPY FAILED: {str(e)}')
+            logger.error(f"COPY FAILED: {str(e)}")
 
     def migrate_models(self, src_dir: Path):
-        '''
+        """
         Recursively walk through src directory, probe anything
         that looks like a model, and copy the model into the
         appropriate location within the destination models directory.
-        '''
+        """
         directories_scanned = set()
         for root, dirs, files in os.walk(src_dir):
             for d in dirs:
                 try:
-                    model = Path(root,d)
+                    model = Path(root, d)
                     info = ModelProbe().heuristic_probe(model)
                     if not info:
                         continue
                     dest = self._model_probe_to_path(info) / model.name
                     self.copy_dir(model, dest)
                     directories_scanned.add(model)
                 except Exception as e:
@@ -132,17 +132,17 @@
                     raise
                 except Exception as e:
                     logger.error(str(e))
             for f in files:
                 # don't copy raw learned_embeds.bin or pytorch_lora_weights.bin
                 # let them be copied as part of a tree copy operation
                 try:
-                    if f in {'learned_embeds.bin','pytorch_lora_weights.bin'}:
+                    if f in {"learned_embeds.bin", "pytorch_lora_weights.bin"}:
                         continue
-                    model = Path(root,f)
+                    model = Path(root, f)
                     if model.parent in directories_scanned:
                         continue
                     info = ModelProbe().heuristic_probe(model)
                     if not info:
                         continue
                     dest = self._model_probe_to_path(info) / f
                     self.copy_file(model, dest)
@@ -150,460 +150,454 @@
                     logger.error(str(e))
                 except KeyboardInterrupt:
                     raise
                 except Exception as e:
                     logger.error(str(e))
 
     def migrate_support_models(self):
-        '''
+        """
         Copy the clipseg, upscaler, and restoration models to their new
         locations.
-        '''
+        """
         dest_directory = self.dest_models
-        if (self.root_directory / 'models/clipseg').exists():
-            self.copy_dir(self.root_directory / 'models/clipseg', dest_directory / 'core/misc/clipseg')
-        if (self.root_directory / 'models/realesrgan').exists():
-            self.copy_dir(self.root_directory / 'models/realesrgan', dest_directory / 'core/upscaling/realesrgan')
-        for d in ['codeformer','gfpgan']:
-            path = self.root_directory / 'models' / d
+        if (self.root_directory / "models/clipseg").exists():
+            self.copy_dir(self.root_directory / "models/clipseg", dest_directory / "core/misc/clipseg")
+        if (self.root_directory / "models/realesrgan").exists():
+            self.copy_dir(self.root_directory / "models/realesrgan", dest_directory / "core/upscaling/realesrgan")
+        for d in ["codeformer", "gfpgan"]:
+            path = self.root_directory / "models" / d
             if path.exists():
-                self.copy_dir(path,dest_directory / f'core/face_restoration/{d}')
+                self.copy_dir(path, dest_directory / f"core/face_restoration/{d}")
 
     def migrate_tuning_models(self):
-        '''
+        """
         Migrate the embeddings, loras and controlnets directories to their new homes.
-        '''
+        """
         for src in [self.src_paths.embeddings, self.src_paths.loras, self.src_paths.controlnets]:
             if not src:
                 continue
             if src.is_dir():
-                logger.info(f'Scanning {src}')
+                logger.info(f"Scanning {src}")
                 self.migrate_models(src)
             else:
-                logger.info(f'{src} directory not found; skipping')
+                logger.info(f"{src} directory not found; skipping")
                 continue
 
     def migrate_conversion_models(self):
-        '''
+        """
         Migrate all the models that are needed by the ckpt_to_diffusers conversion
         script.
-        '''
+        """
 
         dest_directory = self.dest_models
         kwargs = dict(
-            cache_dir = self.root_directory / 'models/hub',
-            #local_files_only = True
+            cache_dir=self.root_directory / "models/hub",
+            # local_files_only = True
         )
         try:
-            logger.info('Migrating core tokenizers and text encoders')
-            target_dir = dest_directory / 'core' / 'convert'
+            logger.info("Migrating core tokenizers and text encoders")
+            target_dir = dest_directory / "core" / "convert"
 
-            self._migrate_pretrained(BertTokenizerFast,
-                                     repo_id='bert-base-uncased',
-                                     dest = target_dir / 'bert-base-uncased',
-                                     **kwargs)
+            self._migrate_pretrained(
+                BertTokenizerFast, repo_id="bert-base-uncased", dest=target_dir / "bert-base-uncased", **kwargs
+            )
 
             # sd-1
-            repo_id = 'openai/clip-vit-large-patch14'
-            self._migrate_pretrained(CLIPTokenizer,
-                                     repo_id= repo_id,
-                                     dest= target_dir / 'clip-vit-large-patch14',
-                                     **kwargs)
-            self._migrate_pretrained(CLIPTextModel,
-                                     repo_id = repo_id,
-                                     dest = target_dir / 'clip-vit-large-patch14',
-                                     force = True,
-                                     **kwargs)
+            repo_id = "openai/clip-vit-large-patch14"
+            self._migrate_pretrained(
+                CLIPTokenizer, repo_id=repo_id, dest=target_dir / "clip-vit-large-patch14", **kwargs
+            )
+            self._migrate_pretrained(
+                CLIPTextModel, repo_id=repo_id, dest=target_dir / "clip-vit-large-patch14", force=True, **kwargs
+            )
 
             # sd-2
             repo_id = "stabilityai/stable-diffusion-2"
-            self._migrate_pretrained(CLIPTokenizer,
-                                     repo_id = repo_id,
-                                     dest = target_dir / 'stable-diffusion-2-clip' / 'tokenizer',
-                                     **{'subfolder':'tokenizer',**kwargs}
-                                     )
-            self._migrate_pretrained(CLIPTextModel,
-                                     repo_id = repo_id,
-                                     dest = target_dir / 'stable-diffusion-2-clip' / 'text_encoder',
-                                     **{'subfolder':'text_encoder',**kwargs}
-                                     )
+            self._migrate_pretrained(
+                CLIPTokenizer,
+                repo_id=repo_id,
+                dest=target_dir / "stable-diffusion-2-clip" / "tokenizer",
+                **{"subfolder": "tokenizer", **kwargs},
+            )
+            self._migrate_pretrained(
+                CLIPTextModel,
+                repo_id=repo_id,
+                dest=target_dir / "stable-diffusion-2-clip" / "text_encoder",
+                **{"subfolder": "text_encoder", **kwargs},
+            )
 
             # VAE
-            logger.info('Migrating stable diffusion VAE')
-            self._migrate_pretrained(AutoencoderKL,
-                                     repo_id = 'stabilityai/sd-vae-ft-mse',
-                                     dest = target_dir / 'sd-vae-ft-mse',
-                                     **kwargs)
-            
+            logger.info("Migrating stable diffusion VAE")
+            self._migrate_pretrained(
+                AutoencoderKL, repo_id="stabilityai/sd-vae-ft-mse", dest=target_dir / "sd-vae-ft-mse", **kwargs
+            )
+
             # safety checking
-            logger.info('Migrating safety checker')
+            logger.info("Migrating safety checker")
             repo_id = "CompVis/stable-diffusion-safety-checker"
-            self._migrate_pretrained(AutoFeatureExtractor,
-                                     repo_id = repo_id,
-                                     dest = target_dir / 'stable-diffusion-safety-checker',
-                                     **kwargs)
-            self._migrate_pretrained(StableDiffusionSafetyChecker,
-                                     repo_id = repo_id,
-                                     dest = target_dir / 'stable-diffusion-safety-checker',
-                                     **kwargs)
+            self._migrate_pretrained(
+                AutoFeatureExtractor, repo_id=repo_id, dest=target_dir / "stable-diffusion-safety-checker", **kwargs
+            )
+            self._migrate_pretrained(
+                StableDiffusionSafetyChecker,
+                repo_id=repo_id,
+                dest=target_dir / "stable-diffusion-safety-checker",
+                **kwargs,
+            )
         except KeyboardInterrupt:
             raise
         except Exception as e:
             logger.error(str(e))
 
-    def _model_probe_to_path(self, info: ModelProbeInfo)->Path:
+    def _model_probe_to_path(self, info: ModelProbeInfo) -> Path:
         return Path(self.dest_models, info.base_type.value, info.model_type.value)
 
-    def _migrate_pretrained(self, model_class, repo_id: str, dest: Path, force:bool=False, **kwargs):
+    def _migrate_pretrained(self, model_class, repo_id: str, dest: Path, force: bool = False, **kwargs):
         if dest.exists() and not force:
-            logger.info(f'Skipping existing {dest}')
+            logger.info(f"Skipping existing {dest}")
             return
         model = model_class.from_pretrained(repo_id, **kwargs)
         self._save_pretrained(model, dest, overwrite=force)
 
-    def _save_pretrained(self, model, dest: Path, overwrite: bool=False):
+    def _save_pretrained(self, model, dest: Path, overwrite: bool = False):
         model_name = dest.name
         if overwrite:
             model.save_pretrained(dest, safe_serialization=True)
         else:
-            download_path = dest.with_name(f'{model_name}.downloading')
+            download_path = dest.with_name(f"{model_name}.downloading")
             model.save_pretrained(download_path, safe_serialization=True)
             download_path.replace(dest)
 
-    def _download_vae(self, repo_id: str, subfolder:str=None)->Path:
-        vae = AutoencoderKL.from_pretrained(repo_id, cache_dir=self.root_directory / 'models/hub', subfolder=subfolder)
+    def _download_vae(self, repo_id: str, subfolder: str = None) -> Path:
+        vae = AutoencoderKL.from_pretrained(repo_id, cache_dir=self.root_directory / "models/hub", subfolder=subfolder)
         info = ModelProbe().heuristic_probe(vae)
-        _, model_name = repo_id.split('/')
+        _, model_name = repo_id.split("/")
         dest = self._model_probe_to_path(info) / self.unique_name(model_name, info)
         vae.save_pretrained(dest, safe_serialization=True)
         return dest
 
-    def _vae_path(self, vae: Union[str,dict])->Path:
-        '''
+    def _vae_path(self, vae: Union[str, dict]) -> Path:
+        """
         Convert 2.3 VAE stanza to a straight path.
-        '''
+        """
         vae_path = None
-        
+
         # First get a path
-        if isinstance(vae,str):
+        if isinstance(vae, str):
             vae_path = vae
 
-        elif isinstance(vae,DictConfig):
-            if p := vae.get('path'):
+        elif isinstance(vae, DictConfig):
+            if p := vae.get("path"):
                 vae_path = p
-            elif repo_id := vae.get('repo_id'):
-                if repo_id=='stabilityai/sd-vae-ft-mse':  # this guy is already downloaded
-                    vae_path = 'models/core/convert/sd-vae-ft-mse'
+            elif repo_id := vae.get("repo_id"):
+                if repo_id == "stabilityai/sd-vae-ft-mse":  # this guy is already downloaded
+                    vae_path = "models/core/convert/sd-vae-ft-mse"
                     return vae_path
                 else:
-                    vae_path = self._download_vae(repo_id, vae.get('subfolder'))
+                    vae_path = self._download_vae(repo_id, vae.get("subfolder"))
 
         assert vae_path is not None, "Couldn't find VAE for this model"
 
         # if the VAE is in the old models directory, then we must move it into the new
         # one. VAEs outside of this directory can stay where they are.
         vae_path = Path(vae_path)
         if vae_path.is_relative_to(self.src_paths.models):
             info = ModelProbe().heuristic_probe(vae_path)
             dest = self._model_probe_to_path(info) / vae_path.name
             if not dest.exists():
                 if vae_path.is_dir():
-                    self.copy_dir(vae_path,dest)
+                    self.copy_dir(vae_path, dest)
                 else:
-                    self.copy_file(vae_path,dest)
+                    self.copy_file(vae_path, dest)
             vae_path = dest
 
         if vae_path.is_relative_to(self.dest_models):
             rel_path = vae_path.relative_to(self.dest_models)
-            return Path('models',rel_path)
+            return Path("models", rel_path)
         else:
             return vae_path
 
-    def migrate_repo_id(self, repo_id: str, model_name: str=None, **extra_config):
-        '''
+    def migrate_repo_id(self, repo_id: str, model_name: str = None, **extra_config):
+        """
         Migrate a locally-cached diffusers pipeline identified with a repo_id
-        '''
+        """
         dest_dir = self.dest_models
-        
-        cache = self.root_directory / 'models/hub'
+
+        cache = self.root_directory / "models/hub"
         kwargs = dict(
-            cache_dir = cache,
-            safety_checker = None,
+            cache_dir=cache,
+            safety_checker=None,
             # local_files_only = True,
         )
 
-        owner,repo_name = repo_id.split('/')
+        owner, repo_name = repo_id.split("/")
         model_name = model_name or repo_name
-        model = cache / '--'.join(['models',owner,repo_name])
-        
-        if len(list(model.glob('snapshots/**/model_index.json')))==0:
+        model = cache / "--".join(["models", owner, repo_name])
+
+        if len(list(model.glob("snapshots/**/model_index.json"))) == 0:
             return
-        revisions = [x.name for x in model.glob('refs/*')]
+        revisions = [x.name for x in model.glob("refs/*")]
 
         # if an fp16 is available we use that
-        revision = 'fp16' if len(revisions) > 1 and 'fp16' in revisions else revisions[0]
-        pipeline = StableDiffusionPipeline.from_pretrained(
-            repo_id,
-            revision=revision,
-            **kwargs)
+        revision = "fp16" if len(revisions) > 1 and "fp16" in revisions else revisions[0]
+        pipeline = StableDiffusionPipeline.from_pretrained(repo_id, revision=revision, **kwargs)
 
         info = ModelProbe().heuristic_probe(pipeline)
         if not info:
             return
 
         if self.mgr.model_exists(model_name, info.base_type, info.model_type):
-            logger.warning(f'A model named {model_name} already exists at the destination. Skipping migration.')
+            logger.warning(f"A model named {model_name} already exists at the destination. Skipping migration.")
             return
 
         dest = self._model_probe_to_path(info) / model_name
         self._save_pretrained(pipeline, dest)
-            
-        rel_path = Path('models',dest.relative_to(dest_dir))
+
+        rel_path = Path("models", dest.relative_to(dest_dir))
         self._add_model(model_name, info, rel_path, **extra_config)
 
-    def migrate_path(self, location: Path, model_name: str=None, **extra_config):
-        '''
+    def migrate_path(self, location: Path, model_name: str = None, **extra_config):
+        """
         Migrate a model referred to using 'weights' or 'path'
-        '''
+        """
 
         # handle relative paths
         dest_dir = self.dest_models
         location = self.root_directory / location
         model_name = model_name or location.stem
-        
+
         info = ModelProbe().heuristic_probe(location)
         if not info:
             return
-        
+
         if self.mgr.model_exists(model_name, info.base_type, info.model_type):
-            logger.warning(f'A model named {model_name} already exists at the destination. Skipping migration.')
+            logger.warning(f"A model named {model_name} already exists at the destination. Skipping migration.")
             return
 
         # uh oh, weights is in the old models directory - move it into the new one
         if Path(location).is_relative_to(self.src_paths.models):
             dest = Path(dest_dir, info.base_type.value, info.model_type.value, location.name)
             if location.is_dir():
-                self.copy_dir(location,dest)
+                self.copy_dir(location, dest)
             else:
-                self.copy_file(location,dest)
-            location = Path('models', info.base_type.value, info.model_type.value, location.name)
+                self.copy_file(location, dest)
+            location = Path("models", info.base_type.value, info.model_type.value, location.name)
 
         self._add_model(model_name, info, location, **extra_config)
 
-    def _add_model(self,
-                   model_name: str,
-                   info: ModelProbeInfo,
-                   location: Path,
-                   **extra_config):
+    def _add_model(self, model_name: str, info: ModelProbeInfo, location: Path, **extra_config):
         if info.model_type != ModelType.Main:
             return
-        
+
         self.mgr.add_model(
-            model_name = model_name,
-            base_model = info.base_type,
-            model_type = info.model_type,
-            clobber = True,
-            model_attributes = {
-                'path': str(location),
-                'description': f'A {info.base_type.value} {info.model_type.value} model',
-                'model_format': info.format,
-                'variant': info.variant_type.value,
+            model_name=model_name,
+            base_model=info.base_type,
+            model_type=info.model_type,
+            clobber=True,
+            model_attributes={
+                "path": str(location),
+                "description": f"A {info.base_type.value} {info.model_type.value} model",
+                "model_format": info.format,
+                "variant": info.variant_type.value,
                 **extra_config,
-            }
+            },
         )
-        
+
     def migrate_defined_models(self):
-        '''
+        """
         Migrate models defined in models.yaml
-        '''
+        """
         # find any models referred to in old models.yaml
-        conf = OmegaConf.load(self.root_directory / 'configs/models.yaml')
-        
-        for model_name, stanza in conf.items():
+        conf = OmegaConf.load(self.root_directory / "configs/models.yaml")
 
+        for model_name, stanza in conf.items():
             try:
                 passthru_args = {}
-                
-                if vae := stanza.get('vae'):
+
+                if vae := stanza.get("vae"):
                     try:
-                        passthru_args['vae'] = str(self._vae_path(vae))
+                        passthru_args["vae"] = str(self._vae_path(vae))
                     except Exception as e:
                         logger.warning(f'Could not find a VAE matching "{vae}" for model "{model_name}"')
                         logger.warning(str(e))
 
-                if config := stanza.get('config'):
-                    passthru_args['config'] = config
+                if config := stanza.get("config"):
+                    passthru_args["config"] = config
 
-                if description:= stanza.get('description'):
-                    passthru_args['description'] = description
-                
-                if repo_id := stanza.get('repo_id'):
-                    logger.info(f'Migrating diffusers model {model_name}')
+                if description := stanza.get("description"):
+                    passthru_args["description"] = description
+
+                if repo_id := stanza.get("repo_id"):
+                    logger.info(f"Migrating diffusers model {model_name}")
                     self.migrate_repo_id(repo_id, model_name, **passthru_args)
 
-                elif location := stanza.get('weights'):
-                    logger.info(f'Migrating checkpoint model {model_name}')
+                elif location := stanza.get("weights"):
+                    logger.info(f"Migrating checkpoint model {model_name}")
                     self.migrate_path(Path(location), model_name, **passthru_args)
-                    
-                elif location := stanza.get('path'):
-                    logger.info(f'Migrating diffusers model {model_name}')
+
+                elif location := stanza.get("path"):
+                    logger.info(f"Migrating diffusers model {model_name}")
                     self.migrate_path(Path(location), model_name, **passthru_args)
-                    
+
             except KeyboardInterrupt:
                 raise
             except Exception as e:
                 logger.error(str(e))
-                    
+
     def migrate(self):
         self.create_directory_structure()
         # the configure script is doing this
         self.migrate_support_models()
         self.migrate_conversion_models()
         self.migrate_tuning_models()
         self.migrate_defined_models()
 
-def _parse_legacy_initfile(root: Path, initfile: Path)->ModelPaths:
-    '''
+
+def _parse_legacy_initfile(root: Path, initfile: Path) -> ModelPaths:
+    """
     Returns tuple of (embedding_path, lora_path, controlnet_path)
-    '''
-    parser = argparse.ArgumentParser(fromfile_prefix_chars='@')
+    """
+    parser = argparse.ArgumentParser(fromfile_prefix_chars="@")
     parser.add_argument(
-        '--embedding_directory',
-        '--embedding_path',
+        "--embedding_directory",
+        "--embedding_path",
         type=Path,
-        dest='embedding_path',
-        default=Path('embeddings'),
+        dest="embedding_path",
+        default=Path("embeddings"),
     )
     parser.add_argument(
-        '--lora_directory',
-        dest='lora_path',
+        "--lora_directory",
+        dest="lora_path",
         type=Path,
-        default=Path('loras'),
+        default=Path("loras"),
     )
-    opt,_ = parser.parse_known_args([f'@{str(initfile)}'])
+    opt, _ = parser.parse_known_args([f"@{str(initfile)}"])
     return ModelPaths(
-        models = root / 'models',
-        embeddings = root / str(opt.embedding_path).strip('"'),
-        loras = root / str(opt.lora_path).strip('"'),
-        controlnets = root / 'controlnets',
+        models=root / "models",
+        embeddings=root / str(opt.embedding_path).strip('"'),
+        loras=root / str(opt.lora_path).strip('"'),
+        controlnets=root / "controlnets",
     )
 
-def _parse_legacy_yamlfile(root: Path, initfile: Path)->ModelPaths:
-    '''
+
+def _parse_legacy_yamlfile(root: Path, initfile: Path) -> ModelPaths:
+    """
     Returns tuple of (embedding_path, lora_path, controlnet_path)
-    '''
+    """
     # Don't use the config object because it is unforgiving of version updates
     # Just use omegaconf directly
     opt = OmegaConf.load(initfile)
     paths = opt.InvokeAI.Paths
-    models = paths.get('models_dir','models')
-    embeddings = paths.get('embedding_dir','embeddings')
-    loras = paths.get('lora_dir','loras')
-    controlnets = paths.get('controlnet_dir','controlnets')
+    models = paths.get("models_dir", "models")
+    embeddings = paths.get("embedding_dir", "embeddings")
+    loras = paths.get("lora_dir", "loras")
+    controlnets = paths.get("controlnet_dir", "controlnets")
     return ModelPaths(
-        models = root / models,
-        embeddings = root / embeddings,
-        loras = root /loras,
-        controlnets = root / controlnets,
+        models=root / models,
+        embeddings=root / embeddings,
+        loras=root / loras,
+        controlnets=root / controlnets,
     )
-    
+
+
 def get_legacy_embeddings(root: Path) -> ModelPaths:
-    path = root / 'invokeai.init'
+    path = root / "invokeai.init"
     if path.exists():
         return _parse_legacy_initfile(root, path)
-    path = root / 'invokeai.yaml'
+    path = root / "invokeai.yaml"
     if path.exists():
         return _parse_legacy_yamlfile(root, path)
 
+
 def do_migrate(src_directory: Path, dest_directory: Path):
     """
     Migrate models from src to dest InvokeAI root directories
     """
-    config_file = dest_directory / 'configs' / 'models.yaml.3'
-    dest_models = dest_directory / 'models.3'
-    
-    version_3 = (dest_directory / 'models' / 'core').exists()
+    config_file = dest_directory / "configs" / "models.yaml.3"
+    dest_models = dest_directory / "models.3"
+
+    version_3 = (dest_directory / "models" / "core").exists()
 
     # Here we create the destination models.yaml file.
     # If we are writing into a version 3 directory and the
     # file already exists, then we write into a copy of it to
     # avoid deleting its previous customizations. Otherwise we
     # create a new empty one.
     if version_3:  # write into the dest directory
         try:
-            shutil.copy(dest_directory / 'configs' / 'models.yaml', config_file)
+            shutil.copy(dest_directory / "configs" / "models.yaml", config_file)
         except:
             MigrateTo3.initialize_yaml(config_file)
-        mgr = ModelManager(config_file) # important to initialize BEFORE moving the models directory
-        (dest_directory / 'models').replace(dest_models)
+        mgr = ModelManager(config_file)  # important to initialize BEFORE moving the models directory
+        (dest_directory / "models").replace(dest_models)
     else:
         MigrateTo3.initialize_yaml(config_file)
         mgr = ModelManager(config_file)
-    
+
     paths = get_legacy_embeddings(src_directory)
-    migrator = MigrateTo3(
-        from_root = src_directory,
-        to_models = dest_models,
-        model_manager = mgr,
-        src_paths = paths
-    )
+    migrator = MigrateTo3(from_root=src_directory, to_models=dest_models, model_manager=mgr, src_paths=paths)
     migrator.migrate()
     print("Migration successful.")
 
     if not version_3:
-        (dest_directory / 'models').replace(src_directory / 'models.orig')
-        print(f'Original models directory moved to {dest_directory}/models.orig')
-        
-    (dest_directory / 'configs' / 'models.yaml').replace(src_directory / 'configs' / 'models.yaml.orig')
-    print(f'Original models.yaml file moved to {dest_directory}/configs/models.yaml.orig')
-    
-    config_file.replace(config_file.with_suffix(''))
-    dest_models.replace(dest_models.with_suffix(''))
-    
+        (dest_directory / "models").replace(src_directory / "models.orig")
+        print(f"Original models directory moved to {dest_directory}/models.orig")
+
+    (dest_directory / "configs" / "models.yaml").replace(src_directory / "configs" / "models.yaml.orig")
+    print(f"Original models.yaml file moved to {dest_directory}/configs/models.yaml.orig")
+
+    config_file.replace(config_file.with_suffix(""))
+    dest_models.replace(dest_models.with_suffix(""))
+
+
 def main():
-    parser = argparse.ArgumentParser(prog="invokeai-migrate3",
-                                     description="""
+    parser = argparse.ArgumentParser(
+        prog="invokeai-migrate3",
+        description="""
 This will copy and convert the models directory and the configs/models.yaml from the InvokeAI 2.3 format 
 '--from-directory' root to the InvokeAI 3.0 '--to-directory' root. These may be abbreviated '--from' and '--to'.a
 
 The old models directory and config file will be renamed 'models.orig' and 'models.yaml.orig' respectively.
 It is safe to provide the same directory for both arguments, but it is better to use the invokeai_configure
-script, which will perform a full upgrade in place."""
-                                     )
-    parser.add_argument('--from-directory',
-                        dest='src_root',
-                        type=Path,
-                        required=True,
-                        help='Source InvokeAI 2.3 root directory (containing "invokeai.init" or "invokeai.yaml")'
-                        )
-    parser.add_argument('--to-directory',
-                        dest='dest_root',
-                        type=Path,
-                        required=True,
-                        help='Destination InvokeAI 3.0 directory (containing "invokeai.yaml")'
-                        )
+script, which will perform a full upgrade in place.""",
+    )
+    parser.add_argument(
+        "--from-directory",
+        dest="src_root",
+        type=Path,
+        required=True,
+        help='Source InvokeAI 2.3 root directory (containing "invokeai.init" or "invokeai.yaml")',
+    )
+    parser.add_argument(
+        "--to-directory",
+        dest="dest_root",
+        type=Path,
+        required=True,
+        help='Destination InvokeAI 3.0 directory (containing "invokeai.yaml")',
+    )
     args = parser.parse_args()
     src_root = args.src_root
     assert src_root.is_dir(), f"{src_root} is not a valid directory"
-    assert (src_root / 'models').is_dir(), f"{src_root} does not contain a 'models' subdirectory"
-    assert (src_root / 'models' / 'hub').exists(), f"{src_root} does not contain a version 2.3 models directory"
-    assert (src_root / 'invokeai.init').exists() or (src_root / 'invokeai.yaml').exists(), f"{src_root} does not contain an InvokeAI init file."
+    assert (src_root / "models").is_dir(), f"{src_root} does not contain a 'models' subdirectory"
+    assert (src_root / "models" / "hub").exists(), f"{src_root} does not contain a version 2.3 models directory"
+    assert (src_root / "invokeai.init").exists() or (
+        src_root / "invokeai.yaml"
+    ).exists(), f"{src_root} does not contain an InvokeAI init file."
 
     dest_root = args.dest_root
     assert dest_root.is_dir(), f"{dest_root} is not a valid directory"
     config = InvokeAIAppConfig.get_config()
-    config.parse_args(['--root',str(dest_root)])
+    config.parse_args(["--root", str(dest_root)])
 
     # TODO: revisit - don't rely on invokeai.yaml to exist yet!
-    dest_is_setup = (dest_root / 'models/core').exists() and (dest_root / 'databases').exists()
+    dest_is_setup = (dest_root / "models/core").exists() and (dest_root / "databases").exists()
     if not dest_is_setup:
         import invokeai.frontend.install.invokeai_configure
         from invokeai.backend.install.invokeai_configure import initialize_rootdir
-        initialize_rootdir(dest_root, True)
-
-    do_migrate(src_root,dest_root)
 
-if __name__ == '__main__':
-    main()
+        initialize_rootdir(dest_root, True)
 
+    do_migrate(src_root, dest_root)
 
 
+if __name__ == "__main__":
+    main()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/install/model_install_backend.py` & `InvokeAI-3.0.1rc2/invokeai/backend/install/model_install_backend.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """
 Utility (backend) functions used by model_install.py
 """
 import os
 import shutil
 import warnings
-from dataclasses import dataclass,field
+from dataclasses import dataclass, field
 from pathlib import Path
 from tempfile import TemporaryDirectory
 from typing import List, Dict, Callable, Union, Set
 
 import requests
 from diffusers import DiffusionPipeline
 from diffusers import logging as dlogging
@@ -24,15 +24,15 @@
 from invokeai.backend.util import download_with_resume
 from ..util.logging import InvokeAILogger
 
 warnings.filterwarnings("ignore")
 
 # --------------------------globals-----------------------
 config = InvokeAIAppConfig.get_config()
-logger = InvokeAILogger.getLogger(name='InvokeAI')
+logger = InvokeAILogger.getLogger(name="InvokeAI")
 
 # the initial "configs" dir is now bundled in the `invokeai.configs` package
 Dataset_path = Path(configs.__path__[0]) / "INITIAL_MODELS.yaml"
 
 Config_preamble = """
 # This file describes the alternative machine learning models
 # available to InvokeAI script.
@@ -41,418 +41,437 @@
 # model requires a model config file, a weights file,
 # and the width and height of the images it
 # was trained on.
 """
 
 LEGACY_CONFIGS = {
     BaseModelType.StableDiffusion1: {
-        ModelVariantType.Normal: 'v1-inference.yaml',
-        ModelVariantType.Inpaint: 'v1-inpainting-inference.yaml',
+        ModelVariantType.Normal: "v1-inference.yaml",
+        ModelVariantType.Inpaint: "v1-inpainting-inference.yaml",
     },
-
     BaseModelType.StableDiffusion2: {
         ModelVariantType.Normal: {
-            SchedulerPredictionType.Epsilon: 'v2-inference.yaml',
-            SchedulerPredictionType.VPrediction: 'v2-inference-v.yaml',
+            SchedulerPredictionType.Epsilon: "v2-inference.yaml",
+            SchedulerPredictionType.VPrediction: "v2-inference-v.yaml",
         },
         ModelVariantType.Inpaint: {
-            SchedulerPredictionType.Epsilon: 'v2-inpainting-inference.yaml',
-            SchedulerPredictionType.VPrediction: 'v2-inpainting-inference-v.yaml',
-        }
+            SchedulerPredictionType.Epsilon: "v2-inpainting-inference.yaml",
+            SchedulerPredictionType.VPrediction: "v2-inpainting-inference-v.yaml",
+        },
     },
-    
     BaseModelType.StableDiffusionXL: {
-        ModelVariantType.Normal: 'sd_xl_base.yaml',
+        ModelVariantType.Normal: "sd_xl_base.yaml",
     },
-    
     BaseModelType.StableDiffusionXLRefiner: {
-        ModelVariantType.Normal: 'sd_xl_refiner.yaml',
+        ModelVariantType.Normal: "sd_xl_refiner.yaml",
     },
 }
 
+
 @dataclass
 class ModelInstallList:
-    '''Class for listing models to be installed/removed'''
+    """Class for listing models to be installed/removed"""
+
     install_models: List[str] = field(default_factory=list)
     remove_models: List[str] = field(default_factory=list)
 
+
 @dataclass
-class InstallSelections():
-    install_models: List[str]= field(default_factory=list)
-    remove_models: List[str]=field(default_factory=list)
+class InstallSelections:
+    install_models: List[str] = field(default_factory=list)
+    remove_models: List[str] = field(default_factory=list)
+
 
 @dataclass
-class ModelLoadInfo():
+class ModelLoadInfo:
     name: str
     model_type: ModelType
     base_type: BaseModelType
     path: Path = None
     repo_id: str = None
-    description: str = ''
+    description: str = ""
     installed: bool = False
     recommended: bool = False
     default: bool = False
 
+
 class ModelInstall(object):
-    def __init__(self,
-                 config:InvokeAIAppConfig,
-                 prediction_type_helper: Callable[[Path],SchedulerPredictionType]=None,
-                 model_manager: ModelManager = None,
-                 access_token:str = None):
+    def __init__(
+        self,
+        config: InvokeAIAppConfig,
+        prediction_type_helper: Callable[[Path], SchedulerPredictionType] = None,
+        model_manager: ModelManager = None,
+        access_token: str = None,
+    ):
         self.config = config
         self.mgr = model_manager or ModelManager(config.model_conf_path)
         self.datasets = OmegaConf.load(Dataset_path)
         self.prediction_helper = prediction_type_helper
         self.access_token = access_token or HfFolder.get_token()
         self.reverse_paths = self._reverse_paths(self.datasets)
 
-    def all_models(self)->Dict[str,ModelLoadInfo]:
-        '''
+    def all_models(self) -> Dict[str, ModelLoadInfo]:
+        """
         Return dict of model_key=>ModelLoadInfo objects.
         This method consolidates and simplifies the entries in both
         models.yaml and INITIAL_MODELS.yaml so that they can
         be treated uniformly. It also sorts the models alphabetically
         by their name, to improve the display somewhat.
-        '''
+        """
         model_dict = dict()
-        
+
         # first populate with the entries in INITIAL_MODELS.yaml
         for key, value in self.datasets.items():
-            name,base,model_type = ModelManager.parse_key(key)
-            value['name'] = name
-            value['base_type'] = base
-            value['model_type'] = model_type
+            name, base, model_type = ModelManager.parse_key(key)
+            value["name"] = name
+            value["base_type"] = base
+            value["model_type"] = model_type
             model_dict[key] = ModelLoadInfo(**value)
 
         # supplement with entries in models.yaml
         installed_models = self.mgr.list_models()
-        
+
         for md in installed_models:
-            base = md['base_model']
-            model_type = md['model_type']
-            name = md['model_name']
+            base = md["base_model"]
+            model_type = md["model_type"]
+            name = md["model_name"]
             key = ModelManager.create_key(name, base, model_type)
             if key in model_dict:
                 model_dict[key].installed = True
             else:
                 model_dict[key] = ModelLoadInfo(
-                    name = name,
-                    base_type = base,
-                    model_type = model_type,
-                    path = value.get('path'),
-                    installed = True,
+                    name=name,
+                    base_type=base,
+                    model_type=model_type,
+                    path=value.get("path"),
+                    installed=True,
                 )
-        return {x : model_dict[x] for x in sorted(model_dict.keys(),key=lambda y: model_dict[y].name.lower())}
+        return {x: model_dict[x] for x in sorted(model_dict.keys(), key=lambda y: model_dict[y].name.lower())}
 
     def list_models(self, model_type):
         installed = self.mgr.list_models(model_type=model_type)
-        print(f'Installed models of type `{model_type}`:')
+        print(f"Installed models of type `{model_type}`:")
         for i in installed:
             print(f"{i['model_name']}\t{i['base_model']}\t{i['path']}")
 
     # logic here a little reversed to maintain backward compatibility
-    def starter_models(self, all_models: bool=False)->Set[str]:
+    def starter_models(self, all_models: bool = False) -> Set[str]:
         models = set()
         for key, value in self.datasets.items():
-            name,base,model_type = ModelManager.parse_key(key)
+            name, base, model_type = ModelManager.parse_key(key)
             if all_models or model_type in [ModelType.Main, ModelType.Vae]:
                 models.add(key)
         return models
 
-    def recommended_models(self)->Set[str]:
+    def recommended_models(self) -> Set[str]:
         starters = self.starter_models(all_models=True)
-        return set([x for x in starters if self.datasets[x].get('recommended',False)])
-    
-    def default_model(self)->str:
+        return set([x for x in starters if self.datasets[x].get("recommended", False)])
+
+    def default_model(self) -> str:
         starters = self.starter_models()
-        defaults = [x for x in starters if self.datasets[x].get('default',False)]
+        defaults = [x for x in starters if self.datasets[x].get("default", False)]
         return defaults[0]
 
     def install(self, selections: InstallSelections):
         verbosity = dlogging.get_verbosity()  # quench NSFW nags
         dlogging.set_verbosity_error()
 
         job = 1
         jobs = len(selections.remove_models) + len(selections.install_models)
-        
+
         # remove requested models
         for key in selections.remove_models:
-            name,base,mtype = self.mgr.parse_key(key)
-            logger.info(f'Deleting {mtype} model {name} [{job}/{jobs}]')
+            name, base, mtype = self.mgr.parse_key(key)
+            logger.info(f"Deleting {mtype} model {name} [{job}/{jobs}]")
             try:
-                self.mgr.del_model(name,base,mtype)
+                self.mgr.del_model(name, base, mtype)
             except FileNotFoundError as e:
                 logger.warning(e)
             job += 1
-            
+
         # add requested models
         for path in selections.install_models:
-            logger.info(f'Installing {path} [{job}/{jobs}]')
+            logger.info(f"Installing {path} [{job}/{jobs}]")
             try:
                 self.heuristic_import(path)
             except (ValueError, KeyError) as e:
                 logger.error(str(e))
             job += 1
-            
+
         dlogging.set_verbosity(verbosity)
         self.mgr.commit()
 
-    def heuristic_import(self,
-                         model_path_id_or_url: Union[str,Path],
-                         models_installed: Set[Path]=None,
-                         )->Dict[str, AddModelResult]:
-        '''
+    def heuristic_import(
+        self,
+        model_path_id_or_url: Union[str, Path],
+        models_installed: Set[Path] = None,
+    ) -> Dict[str, AddModelResult]:
+        """
         :param model_path_id_or_url: A Path to a local model to import, or a string representing its repo_id or URL
         :param models_installed: Set of installed models, used for recursive invocation
         Returns a set of dict objects corresponding to newly-created stanzas in models.yaml.
-        '''
+        """
 
         if not models_installed:
             models_installed = dict()
-            
+
         # A little hack to allow nested routines to retrieve info on the requested ID
         self.current_id = model_path_id_or_url
         path = Path(model_path_id_or_url)
         # checkpoint file, or similar
         if path.is_file():
-            models_installed.update({str(path):self._install_path(path)})
+            models_installed.update({str(path): self._install_path(path)})
 
         # folders style or similar
-        elif path.is_dir() and any([(path/x).exists() for x in \
-                                    {'config.json','model_index.json','learned_embeds.bin','pytorch_lora_weights.bin'}
-                                    ]
-                                   ):
+        elif path.is_dir() and any(
+            [
+                (path / x).exists()
+                for x in {"config.json", "model_index.json", "learned_embeds.bin", "pytorch_lora_weights.bin"}
+            ]
+        ):
             models_installed.update({str(model_path_id_or_url): self._install_path(path)})
 
         # recursive scan
         elif path.is_dir():
             for child in path.iterdir():
                 self.heuristic_import(child, models_installed=models_installed)
 
         # huggingface repo
-        elif len(str(model_path_id_or_url).split('/')) == 2:
+        elif len(str(model_path_id_or_url).split("/")) == 2:
             models_installed.update({str(model_path_id_or_url): self._install_repo(str(model_path_id_or_url))})
 
         # a URL
         elif str(model_path_id_or_url).startswith(("http:", "https:", "ftp:")):
             models_installed.update({str(model_path_id_or_url): self._install_url(model_path_id_or_url)})
 
         else:
-            raise KeyError(f'{str(model_path_id_or_url)} is not recognized as a local path, repo ID or URL. Skipping')
+            raise KeyError(f"{str(model_path_id_or_url)} is not recognized as a local path, repo ID or URL. Skipping")
 
         return models_installed
 
     # install a model from a local path. The optional info parameter is there to prevent
     # the model from being probed twice in the event that it has already been probed.
-    def _install_path(self, path: Path, info: ModelProbeInfo=None)->AddModelResult:
-        info = info or ModelProbe().heuristic_probe(path,self.prediction_helper)
+    def _install_path(self, path: Path, info: ModelProbeInfo = None) -> AddModelResult:
+        info = info or ModelProbe().heuristic_probe(path, self.prediction_helper)
         if not info:
-            logger.warning(f'Unable to parse format of {path}')
+            logger.warning(f"Unable to parse format of {path}")
             return None
         model_name = path.stem if path.is_file() else path.name
         if self.mgr.model_exists(model_name, info.base_type, info.model_type):
             raise ValueError(f'A model named "{model_name}" is already installed.')
-        attributes = self._make_attributes(path,info)
-        return self.mgr.add_model(model_name = model_name,
-                                  base_model = info.base_type,
-                                  model_type = info.model_type,
-                                  model_attributes = attributes,
-                                  )
+        attributes = self._make_attributes(path, info)
+        return self.mgr.add_model(
+            model_name=model_name,
+            base_model=info.base_type,
+            model_type=info.model_type,
+            model_attributes=attributes,
+        )
 
-    def _install_url(self, url: str)->AddModelResult:
+    def _install_url(self, url: str) -> AddModelResult:
         with TemporaryDirectory(dir=self.config.models_path) as staging:
-            location = download_with_resume(url,Path(staging))
+            location = download_with_resume(url, Path(staging))
             if not location:
-                logger.error(f'Unable to download {url}. Skipping.')
+                logger.error(f"Unable to download {url}. Skipping.")
             info = ModelProbe().heuristic_probe(location)
             dest = self.config.models_path / info.base_type.value / info.model_type.value / location.name
-            models_path = shutil.move(location,dest)
+            models_path = shutil.move(location, dest)
 
         # staged version will be garbage-collected at this time
         return self._install_path(Path(models_path), info)
 
-    def _install_repo(self, repo_id: str)->AddModelResult:
+    def _install_repo(self, repo_id: str) -> AddModelResult:
         hinfo = HfApi().model_info(repo_id)
-        
+
         # we try to figure out how to download this most economically
         # list all the files in the repo
         files = [x.rfilename for x in hinfo.siblings]
         location = None
 
         with TemporaryDirectory(dir=self.config.models_path) as staging:
             staging = Path(staging)
-            if 'model_index.json' in files:
-                location = self._download_hf_pipeline(repo_id, staging)   # pipeline
+            if "model_index.json" in files:
+                location = self._download_hf_pipeline(repo_id, staging)  # pipeline
             else:
-                for suffix in ['safetensors','bin']:
-                    if f'pytorch_lora_weights.{suffix}' in files:
-                        location = self._download_hf_model(repo_id, ['pytorch_lora_weights.bin'], staging)  # LoRA
+                for suffix in ["safetensors", "bin"]:
+                    if f"pytorch_lora_weights.{suffix}" in files:
+                        location = self._download_hf_model(repo_id, ["pytorch_lora_weights.bin"], staging)  # LoRA
                         break
-                    elif self.config.precision=='float16' and f'diffusion_pytorch_model.fp16.{suffix}' in files: # vae, controlnet or some other standalone
-                        files = ['config.json', f'diffusion_pytorch_model.fp16.{suffix}']
+                    elif (
+                        self.config.precision == "float16" and f"diffusion_pytorch_model.fp16.{suffix}" in files
+                    ):  # vae, controlnet or some other standalone
+                        files = ["config.json", f"diffusion_pytorch_model.fp16.{suffix}"]
                         location = self._download_hf_model(repo_id, files, staging)
                         break
-                    elif f'diffusion_pytorch_model.{suffix}' in files:
-                        files = ['config.json', f'diffusion_pytorch_model.{suffix}']
+                    elif f"diffusion_pytorch_model.{suffix}" in files:
+                        files = ["config.json", f"diffusion_pytorch_model.{suffix}"]
                         location = self._download_hf_model(repo_id, files, staging)
                         break
-                    elif f'learned_embeds.{suffix}' in files:
-                        location = self._download_hf_model(repo_id, [f'learned_embeds.{suffix}'], staging)
+                    elif f"learned_embeds.{suffix}" in files:
+                        location = self._download_hf_model(repo_id, [f"learned_embeds.{suffix}"], staging)
                         break
             if not location:
-                logger.warning(f'Could not determine type of repo {repo_id}. Skipping install.')
+                logger.warning(f"Could not determine type of repo {repo_id}. Skipping install.")
                 return {}
 
             info = ModelProbe().heuristic_probe(location, self.prediction_helper)
             if not info:
-                logger.warning(f'Could not probe {location}. Skipping install.')
+                logger.warning(f"Could not probe {location}. Skipping install.")
                 return {}
-            dest = self.config.models_path / info.base_type.value / info.model_type.value / self._get_model_name(repo_id,location)
+            dest = (
+                self.config.models_path
+                / info.base_type.value
+                / info.model_type.value
+                / self._get_model_name(repo_id, location)
+            )
             if dest.exists():
                 shutil.rmtree(dest)
-            shutil.copytree(location,dest)
+            shutil.copytree(location, dest)
             return self._install_path(dest, info)
 
-    def _get_model_name(self,path_name: str, location: Path)->str:
-        '''
+    def _get_model_name(self, path_name: str, location: Path) -> str:
+        """
         Calculate a name for the model - primitive implementation.
-        '''
+        """
         if key := self.reverse_paths.get(path_name):
             (name, base, mtype) = ModelManager.parse_key(key)
             return name
         elif location.is_dir():
             return location.name
         else:
             return location.stem
 
-    def _make_attributes(self, path: Path, info: ModelProbeInfo)->dict:
+    def _make_attributes(self, path: Path, info: ModelProbeInfo) -> dict:
         model_name = path.name if path.is_dir() else path.stem
-        description = f'{info.base_type.value} {info.model_type.value} model {model_name}'
+        description = f"{info.base_type.value} {info.model_type.value} model {model_name}"
         if key := self.reverse_paths.get(self.current_id):
             if key in self.datasets:
-                description = self.datasets[key].get('description') or description
+                description = self.datasets[key].get("description") or description
 
         rel_path = self.relative_to_root(path)
 
         attributes = dict(
-            path = str(rel_path),
-            description = str(description),
-            model_format = info.format,
-            )
+            path=str(rel_path),
+            description=str(description),
+            model_format=info.format,
+        )
         legacy_conf = None
         if info.model_type == ModelType.Main:
-            attributes.update(dict(variant = info.variant_type,))
-            if info.format=="checkpoint":
+            attributes.update(
+                dict(
+                    variant=info.variant_type,
+                )
+            )
+            if info.format == "checkpoint":
                 try:
-                    possible_conf = path.with_suffix('.yaml')
+                    possible_conf = path.with_suffix(".yaml")
                     if possible_conf.exists():
                         legacy_conf = str(self.relative_to_root(possible_conf))
                     elif info.base_type == BaseModelType.StableDiffusion2:
-                        legacy_conf = Path(self.config.legacy_conf_dir, LEGACY_CONFIGS[info.base_type][info.variant_type][info.prediction_type])
+                        legacy_conf = Path(
+                            self.config.legacy_conf_dir,
+                            LEGACY_CONFIGS[info.base_type][info.variant_type][info.prediction_type],
+                        )
                     else:
-                        legacy_conf = Path(self.config.legacy_conf_dir, LEGACY_CONFIGS[info.base_type][info.variant_type])
+                        legacy_conf = Path(
+                            self.config.legacy_conf_dir, LEGACY_CONFIGS[info.base_type][info.variant_type]
+                        )
                 except KeyError:
-                    legacy_conf = Path(self.config.legacy_conf_dir, 'v1-inference.yaml')  # best guess
-                    
-        if info.model_type == ModelType.ControlNet and info.format=="checkpoint":
-            possible_conf = path.with_suffix('.yaml')
+                    legacy_conf = Path(self.config.legacy_conf_dir, "v1-inference.yaml")  # best guess
+
+        if info.model_type == ModelType.ControlNet and info.format == "checkpoint":
+            possible_conf = path.with_suffix(".yaml")
             if possible_conf.exists():
                 legacy_conf = str(self.relative_to_root(possible_conf))
 
         if legacy_conf:
-            attributes.update(
-                dict(
-                    config = str(legacy_conf)
-                )
-            )
+            attributes.update(dict(config=str(legacy_conf)))
         return attributes
 
-    def relative_to_root(self, path: Path)->Path:
+    def relative_to_root(self, path: Path) -> Path:
         root = self.config.root_path
         if path.is_relative_to(root):
             return path.relative_to(root)
         else:
             return path
 
-    def _download_hf_pipeline(self, repo_id: str, staging: Path)->Path:
-        '''
+    def _download_hf_pipeline(self, repo_id: str, staging: Path) -> Path:
+        """
         This retrieves a StableDiffusion model from cache or remote and then
         does a save_pretrained() to the indicated staging area.
-        '''
-        _,name = repo_id.split("/")
-        revisions = ['fp16','main'] if self.config.precision=='float16' else ['main']
+        """
+        _, name = repo_id.split("/")
+        revisions = ["fp16", "main"] if self.config.precision == "float16" else ["main"]
         model = None
         for revision in revisions:
             try:
-                model = DiffusionPipeline.from_pretrained(repo_id,revision=revision,safety_checker=None)
+                model = DiffusionPipeline.from_pretrained(repo_id, revision=revision, safety_checker=None)
             except:  # most errors are due to fp16 not being present. Fix this to catch other errors
                 pass
             if model:
                 break
         if not model:
-            logger.error(f'Diffusers model {repo_id} could not be downloaded. Skipping.')
+            logger.error(f"Diffusers model {repo_id} could not be downloaded. Skipping.")
             return None
         model.save_pretrained(staging / name, safe_serialization=True)
         return staging / name
 
-    def _download_hf_model(self, repo_id: str, files: List[str], staging: Path)->Path:
-        _,name = repo_id.split("/")
+    def _download_hf_model(self, repo_id: str, files: List[str], staging: Path) -> Path:
+        _, name = repo_id.split("/")
         location = staging / name
         paths = list()
         for filename in files:
-            p = hf_download_with_resume(repo_id,
-                                        model_dir=location,
-                                        model_name=filename,
-                                        access_token = self.access_token
-                                        )
+            p = hf_download_with_resume(
+                repo_id, model_dir=location, model_name=filename, access_token=self.access_token
+            )
             if p:
                 paths.append(p)
             else:
-                logger.warning(f'Could not download {filename} from {repo_id}.')
-            
-        return location if len(paths)>0 else None
+                logger.warning(f"Could not download {filename} from {repo_id}.")
+
+        return location if len(paths) > 0 else None
 
     @classmethod
-    def _reverse_paths(cls,datasets)->dict:
-        '''
+    def _reverse_paths(cls, datasets) -> dict:
+        """
         Reverse mapping from repo_id/path to destination name.
-        '''
-        return {v.get('path') or v.get('repo_id') : k for k, v in datasets.items()}
+        """
+        return {v.get("path") or v.get("repo_id"): k for k, v in datasets.items()}
+
 
 # -------------------------------------
 def yes_or_no(prompt: str, default_yes=True):
     default = "y" if default_yes else "n"
     response = input(f"{prompt} [{default}] ") or default
     if default_yes:
         return response[0] not in ("n", "N")
     else:
         return response[0] in ("y", "Y")
 
+
 # ---------------------------------------------
-def hf_download_from_pretrained(
-        model_class: object, model_name: str, destination: Path, **kwargs
-):
-    logger = InvokeAILogger.getLogger('InvokeAI')
-    logger.addFilter(lambda x: 'fp16 is not a valid' not in x.getMessage())
-    
+def hf_download_from_pretrained(model_class: object, model_name: str, destination: Path, **kwargs):
+    logger = InvokeAILogger.getLogger("InvokeAI")
+    logger.addFilter(lambda x: "fp16 is not a valid" not in x.getMessage())
+
     model = model_class.from_pretrained(
         model_name,
         resume_download=True,
         **kwargs,
     )
     model.save_pretrained(destination, safe_serialization=True)
     return destination
 
+
 # ---------------------------------------------
 def hf_download_with_resume(
-        repo_id: str,
-        model_dir: str,
-        model_name: str,
-        model_dest: Path = None,
-        access_token: str = None,
+    repo_id: str,
+    model_dir: str,
+    model_name: str,
+    model_dest: Path = None,
+    access_token: str = None,
 ) -> Path:
     model_dest = model_dest or Path(os.path.join(model_dir, model_name))
     os.makedirs(model_dir, exist_ok=True)
 
     url = hf_hub_url(repo_id, model_name)
 
     header = {"Authorization": f"Bearer {access_token}"} if access_token else {}
@@ -463,17 +482,15 @@
         exist_size = os.path.getsize(model_dest)
         header["Range"] = f"bytes={exist_size}-"
         open_mode = "ab"
 
     resp = requests.get(url, headers=header, stream=True)
     total = int(resp.headers.get("content-length", 0))
 
-    if (
-        resp.status_code == 416
-    ):  # "range not satisfiable", which means nothing to return
+    if resp.status_code == 416:  # "range not satisfiable", which means nothing to return
         logger.info(f"{model_name}: complete file found. Skipping.")
         return model_dest
     elif resp.status_code == 404:
         logger.warning("File not found")
         return None
     elif resp.status_code != 200:
         logger.warning(f"{model_name}: {resp.reason}")
@@ -494,9 +511,7 @@
             for data in resp.iter_content(chunk_size=1024):
                 size = file.write(data)
                 bar.update(size)
     except Exception as e:
         logger.error(f"An error occurred while downloading {model_name}: {str(e)}")
         return None
     return model_dest
-
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/convert_ckpt_to_diffusers.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/convert_ckpt_to_diffusers.py`

 * *Files 0% similar despite different names*

```diff
@@ -52,17 +52,15 @@
     HeunDiscreteScheduler,
     LMSDiscreteScheduler,
     PNDMScheduler,
     UnCLIPScheduler,
 )
 from diffusers.utils import is_accelerate_available, is_omegaconf_available, is_safetensors_available
 from diffusers.utils.import_utils import BACKENDS_MAPPING
-from diffusers.pipelines.latent_diffusion.pipeline_latent_diffusion import (
-    LDMBertConfig, LDMBertModel
-)
+from diffusers.pipelines.latent_diffusion.pipeline_latent_diffusion import LDMBertConfig, LDMBertModel
 from diffusers.pipelines.paint_by_example import PaintByExampleImageEncoder
 from diffusers.pipelines.pipeline_utils import DiffusionPipeline
 from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker
 from diffusers.pipelines.stable_diffusion.stable_unclip_image_normalizer import StableUnCLIPImageNormalizer
 
 from invokeai.backend.util.logging import InvokeAILogger
 from invokeai.app.services.config import InvokeAIAppConfig, MODEL_CORE
@@ -81,14 +79,15 @@
 if is_accelerate_available():
     from accelerate import init_empty_weights
     from accelerate.utils import set_module_tensor_to_device
 
 logger = InvokeAILogger.getLogger(__name__)
 CONVERT_MODEL_ROOT = InvokeAIAppConfig.get_config().root_path / MODEL_CORE / "convert"
 
+
 def shave_segments(path, n_shave_prefix_segments=1):
     """
     Removes segments. Positive values shave the first segments, negative shave the last segments.
     """
     if n_shave_prefix_segments >= 0:
         return ".".join(path.split(".")[n_shave_prefix_segments:])
     else:
@@ -505,17 +504,15 @@
             )
             new_checkpoint[f"down_blocks.{block_id}.downsamplers.0.conv.bias"] = unet_state_dict.pop(
                 f"input_blocks.{i}.0.op.bias"
             )
 
         paths = renew_resnet_paths(resnets)
         meta_path = {"old": f"input_blocks.{i}.0", "new": f"down_blocks.{block_id}.resnets.{layer_in_block_id}"}
-        assign_to_checkpoint(
-            paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config
-        )
+        assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)
 
         if len(attentions):
             paths = renew_attention_paths(attentions)
             meta_path = {"old": f"input_blocks.{i}.1", "new": f"down_blocks.{block_id}.attentions.{layer_in_block_id}"}
             assign_to_checkpoint(
                 paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config
             )
@@ -792,15 +789,15 @@
     _copy_linear(hf_model.to_logits, checkpoint.transformer.to_logits)
 
     return hf_model
 
 
 def convert_ldm_clip_checkpoint(checkpoint, local_files_only=False, text_encoder=None):
     if text_encoder is None:
-        config = CLIPTextConfig.from_pretrained(CONVERT_MODEL_ROOT / 'clip-vit-large-patch14')
+        config = CLIPTextConfig.from_pretrained(CONVERT_MODEL_ROOT / "clip-vit-large-patch14")
 
         ctx = init_empty_weights if is_accelerate_available() else nullcontext
         with ctx():
             text_model = CLIPTextModel(config)
 
     keys = list(checkpoint.keys())
 
@@ -1004,15 +1001,17 @@
             image_encoder = CLIPVisionModelWithProjection.from_pretrained(CONVERT_MODEL_ROOT / "clip-vit-large-patch14")
         else:
             raise NotImplementedError(f"Unknown CLIP checkpoint name in stable diffusion checkpoint {clip_model_name}")
 
     elif sd_clip_image_embedder_class == "FrozenOpenCLIPImageEmbedder":
         feature_extractor = CLIPImageProcessor()
         # InvokeAI doesn't use CLIPVisionModelWithProjection so it isn't in the core - if this code is hit a download will occur
-        image_encoder = CLIPVisionModelWithProjection.from_pretrained(CONVERT_MODEL_ROOT / "CLIP-ViT-H-14-laion2B-s32B-b79K")
+        image_encoder = CLIPVisionModelWithProjection.from_pretrained(
+            CONVERT_MODEL_ROOT / "CLIP-ViT-H-14-laion2B-s32B-b79K"
+        )
     else:
         raise NotImplementedError(
             f"Unknown CLIP image embedder class in stable diffusion checkpoint {sd_clip_image_embedder_class}"
         )
 
     return feature_extractor, image_encoder
 
@@ -1067,25 +1066,25 @@
     original_config,
     checkpoint_path,
     image_size,
     upcast_attention,
     extract_ema,
     use_linear_projection=None,
     cross_attention_dim=None,
-    precision: torch.dtype=torch.float32,
+    precision: torch.dtype = torch.float32,
 ):
     ctrlnet_config = create_unet_diffusers_config(original_config, image_size=image_size, controlnet=True)
     ctrlnet_config["upcast_attention"] = upcast_attention
 
     ctrlnet_config.pop("sample_size")
     original_config = ctrlnet_config.copy()
-    
-    ctrlnet_config.pop('addition_embed_type')
-    ctrlnet_config.pop('addition_time_embed_dim')
-    ctrlnet_config.pop('transformer_layers_per_block')
+
+    ctrlnet_config.pop("addition_embed_type")
+    ctrlnet_config.pop("addition_time_embed_dim")
+    ctrlnet_config.pop("transformer_layers_per_block")
 
     if use_linear_projection is not None:
         ctrlnet_config["use_linear_projection"] = use_linear_projection
 
     if cross_attention_dim is not None:
         ctrlnet_config["cross_attention_dim"] = cross_attention_dim
 
@@ -1107,14 +1106,15 @@
         skip_extract_state_dict=skip_extract_state_dict,
     )
 
     controlnet.load_state_dict(converted_ctrl_checkpoint)
 
     return controlnet.to(precision)
 
+
 # TO DO - PASS PRECISION
 def download_from_original_stable_diffusion_ckpt(
     checkpoint_path: str,
     model_version: BaseModelType,
     model_variant: ModelVariantType,
     original_config_file: str = None,
     image_size: Optional[int] = None,
@@ -1245,24 +1245,26 @@
         logger.debug("global_step key not found in model")
         global_step = None
 
     # NOTE: this while loop isn't great but this controlnet checkpoint has one additional
     # "state_dict" key https://huggingface.co/thibaud/controlnet-canny-sd21
     while "state_dict" in checkpoint:
         checkpoint = checkpoint["state_dict"]
-        
-    logger.debug(f'model_type = {model_type}; original_config_file = {original_config_file}')
+
+    logger.debug(f"model_type = {model_type}; original_config_file = {original_config_file}")
 
     if original_config_file is None:
         key_name_v2_1 = "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight"
         key_name_sd_xl_base = "conditioner.embedders.1.model.transformer.resblocks.9.mlp.c_proj.bias"
         key_name_sd_xl_refiner = "conditioner.embedders.0.model.transformer.resblocks.9.mlp.c_proj.bias"
 
         # model_type = "v1"
-        config_url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml"
+        config_url = (
+            "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml"
+        )
 
         if key_name_v2_1 in checkpoint and checkpoint[key_name_v2_1].shape[-1] == 1024:
             # model_type = "v2"
             config_url = "https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/v2-inference-v.yaml"
 
             if global_step == 110000:
                 # v2.1 needs to upcast attention
@@ -1273,15 +1275,18 @@
         elif key_name_sd_xl_refiner in checkpoint:
             # only refiner xl has embedder and one text embedders
             config_url = "https://raw.githubusercontent.com/Stability-AI/generative-models/main/configs/inference/sd_xl_refiner.yaml"
 
         original_config_file = BytesIO(requests.get(config_url).content)
 
     original_config = OmegaConf.load(original_config_file)
-    if model_version == BaseModelType.StableDiffusion2 and original_config["model"]["params"]["parameterization"] == "v":
+    if (
+        model_version == BaseModelType.StableDiffusion2
+        and original_config["model"]["params"]["parameterization"] == "v"
+    ):
         prediction_type = "v_prediction"
         upcast_attention = True
         image_size = 768
     else:
         prediction_type = "epsilon"
         upcast_attention = False
         image_size = 512
@@ -1432,15 +1437,15 @@
         vae = AutoencoderKL.from_pretrained(vae_path)
 
     if model_type == "FrozenOpenCLIPEmbedder":
         config_name = "stabilityai/stable-diffusion-2"
         config_kwargs = {"subfolder": "text_encoder"}
 
         text_model = convert_open_clip_checkpoint(checkpoint, config_name, **config_kwargs)
-        tokenizer = CLIPTokenizer.from_pretrained(CONVERT_MODEL_ROOT / 'stable-diffusion-2-clip', subfolder="tokenizer")
+        tokenizer = CLIPTokenizer.from_pretrained(CONVERT_MODEL_ROOT / "stable-diffusion-2-clip", subfolder="tokenizer")
 
         if stable_unclip is None:
             if controlnet:
                 pipe = pipeline_class(
                     vae=vae.to(precision),
                     text_encoder=text_model,
                     tokenizer=tokenizer,
@@ -1487,15 +1492,17 @@
                 )
             elif stable_unclip == "txt2img":
                 if stable_unclip_prior is None or stable_unclip_prior == "karlo":
                     karlo_model = "kakaobrain/karlo-v1-alpha"
                     prior = PriorTransformer.from_pretrained(karlo_model, subfolder="prior")
 
                     prior_tokenizer = CLIPTokenizer.from_pretrained(CONVERT_MODEL_ROOT / "clip-vit-large-patch14")
-                    prior_text_model = CLIPTextModelWithProjection.from_pretrained(CONVERT_MODEL_ROOT / "clip-vit-large-patch14")
+                    prior_text_model = CLIPTextModelWithProjection.from_pretrained(
+                        CONVERT_MODEL_ROOT / "clip-vit-large-patch14"
+                    )
 
                     prior_scheduler = UnCLIPScheduler.from_pretrained(karlo_model, subfolder="prior_scheduler")
                     prior_scheduler = DDPMScheduler.from_config(prior_scheduler.config)
                 else:
                     raise NotImplementedError(f"unknown prior for stable unclip model: {stable_unclip_prior}")
 
                 pipe = StableUnCLIPPipeline(
@@ -1529,19 +1536,27 @@
             safety_checker=None,
             feature_extractor=feature_extractor,
         )
     elif model_type == "FrozenCLIPEmbedder":
         text_model = convert_ldm_clip_checkpoint(
             checkpoint, local_files_only=local_files_only, text_encoder=text_encoder
         )
-        tokenizer = CLIPTokenizer.from_pretrained(CONVERT_MODEL_ROOT / "clip-vit-large-patch14") if tokenizer is None else tokenizer
+        tokenizer = (
+            CLIPTokenizer.from_pretrained(CONVERT_MODEL_ROOT / "clip-vit-large-patch14")
+            if tokenizer is None
+            else tokenizer
+        )
 
         if load_safety_checker:
-            safety_checker = StableDiffusionSafetyChecker.from_pretrained(CONVERT_MODEL_ROOT / "stable-diffusion-safety-checker")
-            feature_extractor = AutoFeatureExtractor.from_pretrained(CONVERT_MODEL_ROOT / "stable-diffusion-safety-checker")
+            safety_checker = StableDiffusionSafetyChecker.from_pretrained(
+                CONVERT_MODEL_ROOT / "stable-diffusion-safety-checker"
+            )
+            feature_extractor = AutoFeatureExtractor.from_pretrained(
+                CONVERT_MODEL_ROOT / "stable-diffusion-safety-checker"
+            )
         else:
             safety_checker = None
             feature_extractor = None
 
         if controlnet:
             pipe = pipeline_class(
                 vae=vae.to(precision),
@@ -1563,25 +1578,25 @@
                 safety_checker=safety_checker,
                 feature_extractor=feature_extractor,
             )
     elif model_type in ["SDXL", "SDXL-Refiner"]:
         if model_type == "SDXL":
             tokenizer = CLIPTokenizer.from_pretrained(CONVERT_MODEL_ROOT / "clip-vit-large-patch14")
             text_encoder = convert_ldm_clip_checkpoint(checkpoint, local_files_only=local_files_only)
-            
+
             tokenizer_name = CONVERT_MODEL_ROOT / "CLIP-ViT-bigG-14-laion2B-39B-b160k"
             tokenizer_2 = CLIPTokenizer.from_pretrained(tokenizer_name, pad_token="!")
 
             config_name = tokenizer_name
             config_kwargs = {"projection_dim": 1280}
             text_encoder_2 = convert_open_clip_checkpoint(
                 checkpoint, config_name, prefix="conditioner.embedders.1.model.", has_projection=True, **config_kwargs
             )
 
-            pipe = StableDiffusionXLPipeline (
+            pipe = StableDiffusionXLPipeline(
                 vae=vae.to(precision),
                 text_encoder=text_encoder,
                 tokenizer=tokenizer,
                 text_encoder_2=text_encoder_2,
                 tokenizer_2=tokenizer_2,
                 unet=unet.to(precision),
                 scheduler=scheduler,
@@ -1682,49 +1697,48 @@
         extract_ema,
         use_linear_projection=use_linear_projection,
         cross_attention_dim=cross_attention_dim,
     )
 
     return controlnet
 
+
 def convert_ldm_vae_to_diffusers(checkpoint, vae_config: DictConfig, image_size: int) -> AutoencoderKL:
-    vae_config = create_vae_diffusers_config(
-        vae_config, image_size=image_size
-    )
+    vae_config = create_vae_diffusers_config(vae_config, image_size=image_size)
 
-    converted_vae_checkpoint = convert_ldm_vae_checkpoint(
-        checkpoint, vae_config
-    )
+    converted_vae_checkpoint = convert_ldm_vae_checkpoint(checkpoint, vae_config)
 
     vae = AutoencoderKL(**vae_config)
     vae.load_state_dict(converted_vae_checkpoint)
     return vae
 
+
 def convert_ckpt_to_diffusers(
-        checkpoint_path: Union[str, Path],
-        dump_path: Union[str, Path],
-        use_safetensors: bool=True,
-        **kwargs,
+    checkpoint_path: Union[str, Path],
+    dump_path: Union[str, Path],
+    use_safetensors: bool = True,
+    **kwargs,
 ):
     """
     Takes all the arguments of download_from_original_stable_diffusion_ckpt(),
     and in addition a path-like object indicating the location of the desired diffusers
     model to be written.
     """
     pipe = download_from_original_stable_diffusion_ckpt(checkpoint_path, **kwargs)
 
     pipe.save_pretrained(
         dump_path,
         safe_serialization=use_safetensors and is_safetensors_available(),
     )
 
+
 def convert_controlnet_to_diffusers(
-        checkpoint_path: Union[str, Path],
-        dump_path: Union[str, Path],
-        **kwargs,
+    checkpoint_path: Union[str, Path],
+    dump_path: Union[str, Path],
+    **kwargs,
 ):
     """
     Takes all the arguments of download_controlnet_from_original_ckpt(),
     and in addition a path-like object indicating the location of the desired diffusers
     model to be written.
     """
     pipe = download_controlnet_from_original_ckpt(checkpoint_path, **kwargs)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/lora.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/lora.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,55 +7,52 @@
 
 import torch
 from compel.embeddings_provider import BaseTextualInversionManager
 from diffusers.models import UNet2DConditionModel
 from safetensors.torch import load_file
 from transformers import CLIPTextModel, CLIPTokenizer
 
+
 class LoRALayerBase:
-    #rank: Optional[int]
-    #alpha: Optional[float]
-    #bias: Optional[torch.Tensor]
-    #layer_key: str
+    # rank: Optional[int]
+    # alpha: Optional[float]
+    # bias: Optional[torch.Tensor]
+    # layer_key: str
 
-    #@property
-    #def scale(self):
+    # @property
+    # def scale(self):
     #    return self.alpha / self.rank if (self.alpha and self.rank) else 1.0
 
     def __init__(
         self,
         layer_key: str,
         values: dict,
     ):
         if "alpha" in values:
             self.alpha = values["alpha"].item()
         else:
             self.alpha = None
 
-        if (
-            "bias_indices" in values
-            and "bias_values" in values
-            and "bias_size" in values
-        ):
+        if "bias_indices" in values and "bias_values" in values and "bias_size" in values:
             self.bias = torch.sparse_coo_tensor(
                 values["bias_indices"],
                 values["bias_values"],
                 tuple(values["bias_size"]),
             )
 
         else:
             self.bias = None
 
-        self.rank = None # set in layer implementation
+        self.rank = None  # set in layer implementation
         self.layer_key = layer_key
 
     def forward(
         self,
         module: torch.nn.Module,
-        input_h: Any, # for real looks like Tuple[torch.nn.Tensor] but not sure
+        input_h: Any,  # for real looks like Tuple[torch.nn.Tensor] but not sure
         multiplier: float,
     ):
         if type(module) == torch.nn.Conv2d:
             op = torch.nn.functional.conv2d
             extra_args = dict(
                 stride=module.stride,
                 padding=module.padding,
@@ -67,20 +64,24 @@
             op = torch.nn.functional.linear
             extra_args = {}
 
         weight = self.get_weight()
 
         bias = self.bias if self.bias is not None else 0
         scale = self.alpha / self.rank if (self.alpha and self.rank) else 1.0
-        return op(
-            *input_h,
-            (weight + bias).view(module.weight.shape),
-            None,
-            **extra_args,
-        ) * multiplier * scale
+        return (
+            op(
+                *input_h,
+                (weight + bias).view(module.weight.shape),
+                None,
+                **extra_args,
+            )
+            * multiplier
+            * scale
+        )
 
     def get_weight(self):
         raise NotImplementedError()
 
     def calc_size(self) -> int:
         model_size = 0
         for val in [self.bias]:
@@ -95,17 +96,17 @@
     ):
         if self.bias is not None:
             self.bias = self.bias.to(device=device, dtype=dtype)
 
 
 # TODO: find and debug lora/locon with bias
 class LoRALayer(LoRALayerBase):
-    #up: torch.Tensor
-    #mid: Optional[torch.Tensor]
-    #down: torch.Tensor
+    # up: torch.Tensor
+    # mid: Optional[torch.Tensor]
+    # down: torch.Tensor
 
     def __init__(
         self,
         layer_key: str,
         values: dict,
     ):
         super().__init__(layer_key, values)
@@ -147,20 +148,20 @@
         self.down = self.down.to(device=device, dtype=dtype)
 
         if self.mid is not None:
             self.mid = self.mid.to(device=device, dtype=dtype)
 
 
 class LoHALayer(LoRALayerBase):
-    #w1_a: torch.Tensor
-    #w1_b: torch.Tensor
-    #w2_a: torch.Tensor
-    #w2_b: torch.Tensor
-    #t1: Optional[torch.Tensor] = None
-    #t2: Optional[torch.Tensor] = None
+    # w1_a: torch.Tensor
+    # w1_b: torch.Tensor
+    # w2_a: torch.Tensor
+    # w2_b: torch.Tensor
+    # t1: Optional[torch.Tensor] = None
+    # t2: Optional[torch.Tensor] = None
 
     def __init__(
         self,
         layer_key: str,
         values: dict,
     ):
         super().__init__(layer_key, values)
@@ -183,20 +184,16 @@
         self.rank = self.w1_b.shape[0]
 
     def get_weight(self):
         if self.t1 is None:
             weight = (self.w1_a @ self.w1_b) * (self.w2_a @ self.w2_b)
 
         else:
-            rebuild1 = torch.einsum(
-                "i j k l, j r, i p -> p r k l", self.t1, self.w1_b, self.w1_a
-            )
-            rebuild2 = torch.einsum(
-                "i j k l, j r, i p -> p r k l", self.t2, self.w2_b, self.w2_a
-            )
+            rebuild1 = torch.einsum("i j k l, j r, i p -> p r k l", self.t1, self.w1_b, self.w1_a)
+            rebuild2 = torch.einsum("i j k l, j r, i p -> p r k l", self.t2, self.w2_b, self.w2_a)
             weight = rebuild1 * rebuild2
 
         return weight
 
     def calc_size(self) -> int:
         model_size = super().calc_size()
         for val in [self.w1_a, self.w1_b, self.w2_a, self.w2_b, self.t1, self.t2]:
@@ -219,28 +216,28 @@
         self.w2_a = self.w2_a.to(device=device, dtype=dtype)
         self.w2_b = self.w2_b.to(device=device, dtype=dtype)
         if self.t2 is not None:
             self.t2 = self.t2.to(device=device, dtype=dtype)
 
 
 class LoKRLayer(LoRALayerBase):
-    #w1: Optional[torch.Tensor] = None
-    #w1_a: Optional[torch.Tensor] = None
-    #w1_b: Optional[torch.Tensor] = None
-    #w2: Optional[torch.Tensor] = None
-    #w2_a: Optional[torch.Tensor] = None
-    #w2_b: Optional[torch.Tensor] = None
-    #t2: Optional[torch.Tensor] = None
+    # w1: Optional[torch.Tensor] = None
+    # w1_a: Optional[torch.Tensor] = None
+    # w1_b: Optional[torch.Tensor] = None
+    # w2: Optional[torch.Tensor] = None
+    # w2_a: Optional[torch.Tensor] = None
+    # w2_b: Optional[torch.Tensor] = None
+    # t2: Optional[torch.Tensor] = None
 
     def __init__(
         self,
         layer_key: str,
         values: dict,
     ):
-        super().__init__(layer_key, values)        
+        super().__init__(layer_key, values)
 
         if "lokr_w1" in values:
             self.w1 = values["lokr_w1"]
             self.w1_a = None
             self.w1_b = None
         else:
             self.w1 = None
@@ -262,27 +259,27 @@
             self.t2 = None
 
         if "lokr_w1_b" in values:
             self.rank = values["lokr_w1_b"].shape[0]
         elif "lokr_w2_b" in values:
             self.rank = values["lokr_w2_b"].shape[0]
         else:
-            self.rank = None # unscaled
+            self.rank = None  # unscaled
 
     def get_weight(self):
         w1 = self.w1
         if w1 is None:
             w1 = self.w1_a @ self.w1_b
 
         w2 = self.w2
         if w2 is None:
             if self.t2 is None:
                 w2 = self.w2_a @ self.w2_b
             else:
-                w2 = torch.einsum('i j k l, i p, j r -> p r k l', self.t2, self.w2_a, self.w2_b)
+                w2 = torch.einsum("i j k l, i p, j r -> p r k l", self.t2, self.w2_a, self.w2_b)
 
         if len(w2.shape) == 4:
             w1 = w1.unsqueeze(2).unsqueeze(2)
         w2 = w2.contiguous()
         weight = torch.kron(w1, w2)
 
         return weight
@@ -313,15 +310,15 @@
             self.w2_a = self.w2_a.to(device=device, dtype=dtype)
             self.w2_b = self.w2_b.to(device=device, dtype=dtype)
 
         if self.t2 is not None:
             self.t2 = self.t2.to(device=device, dtype=dtype)
 
 
-class LoRAModel: #(torch.nn.Module):
+class LoRAModel:  # (torch.nn.Module):
     _name: str
     layers: Dict[str, LoRALayer]
     _device: torch.device
     _dtype: torch.dtype
 
     def __init__(
         self,
@@ -341,15 +338,15 @@
 
     @property
     def device(self):
         return self._device
 
     @property
     def dtype(self):
-        return self._dtype    
+        return self._dtype
 
     def to(
         self,
         device: Optional[torch.device] = None,
         dtype: Optional[torch.dtype] = None,
     ) -> LoRAModel:
         # TODO: try revert if exception?
@@ -376,44 +373,41 @@
 
         if isinstance(file_path, str):
             file_path = Path(file_path)
 
         model = cls(
             device=device,
             dtype=dtype,
-            name=file_path.stem, # TODO:
+            name=file_path.stem,  # TODO:
             layers=dict(),
         )
 
         if file_path.suffix == ".safetensors":
             state_dict = load_file(file_path.absolute().as_posix(), device="cpu")
         else:
             state_dict = torch.load(file_path, map_location="cpu")
 
         state_dict = cls._group_state(state_dict)
 
         for layer_key, values in state_dict.items():
-
             # lora and locon
             if "lora_down.weight" in values:
                 layer = LoRALayer(layer_key, values)
 
             # loha
             elif "hada_w1_b" in values:
                 layer = LoHALayer(layer_key, values)
 
             # lokr
             elif "lokr_w1_b" in values or "lokr_w1" in values:
                 layer = LoKRLayer(layer_key, values)
 
             else:
                 # TODO: diff/ia3/... format
-                print(
-                    f">> Encountered unknown lora layer module in {model.name}: {layer_key}"
-                )
+                print(f">> Encountered unknown lora layer module in {model.name}: {layer_key}")
                 return
 
             # lower memory consumption by removing already parsed layer values
             state_dict[layer_key].clear()
 
             layer.to(device=device, dtype=dtype)
             model.layers[layer_key] = layer
@@ -439,30 +433,31 @@
     (lora_model2, 0.4),
 ]
 with LoRAHelper.apply_lora_unet(unet, loras):
     # unet with applied loras
 # unmodified unet
 
 """
+
+
 # TODO: rename smth like ModelPatcher and add TI method?
 class ModelPatcher:
-
     @staticmethod
     def _resolve_lora_key(model: torch.nn.Module, lora_key: str, prefix: str) -> Tuple[str, torch.nn.Module]:
         assert "." not in lora_key
 
         if not lora_key.startswith(prefix):
             raise Exception(f"lora_key with invalid prefix: {lora_key}, {prefix}")
 
         module = model
         module_key = ""
-        key_parts = lora_key[len(prefix):].split('_')
+        key_parts = lora_key[len(prefix) :].split("_")
 
         submodule_name = key_parts.pop(0)
-        
+
         while len(key_parts) > 0:
             try:
                 module = module.get_submodule(submodule_name)
                 module_key += "." + submodule_name
                 submodule_name = key_parts.pop(0)
             except:
                 submodule_name += "_" + key_parts.pop(0)
@@ -473,92 +468,87 @@
         return (module_key, module)
 
     @staticmethod
     def _lora_forward_hook(
         applied_loras: List[Tuple[LoRAModel, float]],
         layer_name: str,
     ):
-
         def lora_forward(module, input_h, output):
             if len(applied_loras) == 0:
                 return output
 
             for lora, weight in applied_loras:
                 layer = lora.layers.get(layer_name, None)
                 if layer is None:
                     continue
                 output += layer.forward(module, input_h, weight)
             return output
 
         return lora_forward
 
-
     @classmethod
     @contextmanager
     def apply_lora_unet(
         cls,
         unet: UNet2DConditionModel,
         loras: List[Tuple[LoRAModel, float]],
     ):
         with cls.apply_lora(unet, loras, "lora_unet_"):
             yield
 
-
     @classmethod
     @contextmanager
     def apply_lora_text_encoder(
         cls,
         text_encoder: CLIPTextModel,
         loras: List[Tuple[LoRAModel, float]],
     ):
         with cls.apply_lora(text_encoder, loras, "lora_te_"):
             yield
 
-
     @classmethod
     @contextmanager
     def apply_lora(
         cls,
         model: torch.nn.Module,
         loras: List[Tuple[LoRAModel, float]],
         prefix: str,
     ):
         original_weights = dict()
         try:
             with torch.no_grad():
                 for lora, lora_weight in loras:
-                    #assert lora.device.type == "cpu"
+                    # assert lora.device.type == "cpu"
                     for layer_key, layer in lora.layers.items():
                         if not layer_key.startswith(prefix):
                             continue
 
                         module_key, module = cls._resolve_lora_key(model, layer_key, prefix)
                         if module_key not in original_weights:
                             original_weights[module_key] = module.weight.detach().to(device="cpu", copy=True)
 
                         # enable autocast to calc fp16 loras on cpu
-                        #with torch.autocast(device_type="cpu"):
+                        # with torch.autocast(device_type="cpu"):
                         layer.to(dtype=torch.float32)
                         layer_scale = layer.alpha / layer.rank if (layer.alpha and layer.rank) else 1.0
                         layer_weight = layer.get_weight() * lora_weight * layer_scale
 
                         if module.weight.shape != layer_weight.shape:
                             # TODO: debug on lycoris
                             layer_weight = layer_weight.reshape(module.weight.shape)
 
                         module.weight += layer_weight.to(device=module.weight.device, dtype=module.weight.dtype)
 
-            yield # wait for context manager exit
+            yield  # wait for context manager exit
 
         finally:
             with torch.no_grad():
                 for module_key, weight in original_weights.items():
                     model.get_submodule(module_key).weight.copy_(weight)
 
-
     @classmethod
     @contextmanager
     def apply_ti(
         cls,
         tokenizer: CLIPTokenizer,
         text_encoder: CLIPTextModel,
         ti_list: List[Any],
@@ -598,27 +588,28 @@
                         raise RuntimeError(f"Unable to find token id for token '{trigger}'")
 
                     if model_embeddings.weight.data[token_id].shape != embedding.shape:
                         raise ValueError(
                             f"Cannot load embedding for {trigger}. It was trained on a model with token dimension {embedding.shape[0]}, but the current model has token dimension {model_embeddings.weight.data[token_id].shape[0]}."
                         )
 
-                    model_embeddings.weight.data[token_id] = embedding.to(device=text_encoder.device, dtype=text_encoder.dtype)
+                    model_embeddings.weight.data[token_id] = embedding.to(
+                        device=text_encoder.device, dtype=text_encoder.dtype
+                    )
                     ti_tokens.append(token_id)
 
                 if len(ti_tokens) > 1:
                     ti_manager.pad_tokens[ti_tokens[0]] = ti_tokens[1:]
 
             yield ti_tokenizer, ti_manager
 
         finally:
             if init_tokens_count and new_tokens_added:
                 text_encoder.resize_token_embeddings(init_tokens_count)
 
-
     @classmethod
     @contextmanager
     def apply_clip_skip(
         cls,
         text_encoder: CLIPTextModel,
         clip_skip: int,
     ):
@@ -629,41 +620,44 @@
 
             yield
 
         finally:
             while len(skipped_layers) > 0:
                 text_encoder.text_model.encoder.layers.append(skipped_layers.pop())
 
+
 class TextualInversionModel:
     name: str
-    embedding: torch.Tensor # [n, 768]|[n, 1280]
+    embedding: torch.Tensor  # [n, 768]|[n, 1280]
 
     @classmethod
     def from_checkpoint(
         cls,
         file_path: Union[str, Path],
         device: Optional[torch.device] = None,
         dtype: Optional[torch.dtype] = None,
     ):
         if not isinstance(file_path, Path):
             file_path = Path(file_path)
 
-        result = cls() # TODO:
-        result.name = file_path.stem # TODO:
+        result = cls()  # TODO:
+        result.name = file_path.stem  # TODO:
 
         if file_path.suffix == ".safetensors":
             state_dict = load_file(file_path.absolute().as_posix(), device="cpu")
         else:
             state_dict = torch.load(file_path, map_location="cpu")
 
         # both v1 and v2 format embeddings
         # difference mostly in metadata
         if "string_to_param" in state_dict:
             if len(state_dict["string_to_param"]) > 1:
-                print(f"Warn: Embedding \"{file_path.name}\" contains multiple tokens, which is not supported. The first token will be used.")
+                print(
+                    f'Warn: Embedding "{file_path.name}" contains multiple tokens, which is not supported. The first token will be used.'
+                )
 
             result.embedding = next(iter(state_dict["string_to_param"].values()))
 
         # v3 (easynegative)
         elif "emb_params" in state_dict:
             result.embedding = state_dict["emb_params"]
 
@@ -684,18 +678,15 @@
     pad_tokens: Dict[int, List[int]]
     tokenizer: CLIPTokenizer
 
     def __init__(self, tokenizer: CLIPTokenizer):
         self.pad_tokens = dict()
         self.tokenizer = tokenizer
 
-    def expand_textual_inversion_token_ids_if_necessary(
-        self, token_ids: list[int]
-    ) -> list[int]:
-
+    def expand_textual_inversion_token_ids_if_necessary(self, token_ids: list[int]) -> list[int]:
         if len(self.pad_tokens) == 0:
             return token_ids
 
         if token_ids[0] == self.tokenizer.bos_token_id:
             raise ValueError("token_ids must not start with bos_token_id")
         if token_ids[-1] == self.tokenizer.eos_token_id:
             raise ValueError("token_ids must not end with eos_token_id")
@@ -703,8 +694,7 @@
         new_token_ids = []
         for token_id in token_ids:
             new_token_ids.append(token_id)
             if token_id in self.pad_tokens:
                 new_token_ids.extend(self.pad_tokens[token_id])
 
         return new_token_ids
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_cache.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_cache.py`

 * *Files 3% similar despite different names*

```diff
@@ -33,27 +33,30 @@
 from .models import BaseModelType, ModelType, SubModelType, ModelBase
 
 # Maximum size of the cache, in gigs
 # Default is roughly enough to hold three fp16 diffusers models in RAM simultaneously
 DEFAULT_MAX_CACHE_SIZE = 6.0
 
 # amount of GPU memory to hold in reserve for use by generations (GB)
-DEFAULT_MAX_VRAM_CACHE_SIZE= 2.75
+DEFAULT_MAX_VRAM_CACHE_SIZE = 2.75
 
 # actual size of a gig
 GIG = 1073741824
 
+
 class ModelLocker(object):
     "Forward declaration"
     pass
 
+
 class ModelCache(object):
     "Forward declaration"
     pass
 
+
 class _CacheRecord:
     size: int
     model: Any
     cache: ModelCache
     _locks: int
 
     def __init__(self, cache, model: Any, size: int):
@@ -75,60 +78,59 @@
 
     @property
     def loaded(self):
         if self.model is not None and hasattr(self.model, "device"):
             return self.model.device != self.cache.storage_device
         else:
             return False
-    
+
 
 class ModelCache(object):
     def __init__(
         self,
-        max_cache_size: float=DEFAULT_MAX_CACHE_SIZE,
-        max_vram_cache_size: float=DEFAULT_MAX_VRAM_CACHE_SIZE,
-        execution_device: torch.device=torch.device('cuda'),
-        storage_device: torch.device=torch.device('cpu'),
-        precision: torch.dtype=torch.float16,
-        sequential_offload: bool=False,
-        lazy_offloading: bool=True,
+        max_cache_size: float = DEFAULT_MAX_CACHE_SIZE,
+        max_vram_cache_size: float = DEFAULT_MAX_VRAM_CACHE_SIZE,
+        execution_device: torch.device = torch.device("cuda"),
+        storage_device: torch.device = torch.device("cpu"),
+        precision: torch.dtype = torch.float16,
+        sequential_offload: bool = False,
+        lazy_offloading: bool = True,
         sha_chunksize: int = 16777216,
-        logger: types.ModuleType = logger
+        logger: types.ModuleType = logger,
     ):
-        '''
+        """
         :param max_cache_size: Maximum size of the RAM cache [6.0 GB]
         :param execution_device: Torch device to load active model into [torch.device('cuda')]
         :param storage_device: Torch device to save inactive model in [torch.device('cpu')]
         :param precision: Precision for loaded models [torch.float16]
         :param lazy_offloading: Keep model in VRAM until another model needs to be loaded
         :param sequential_offload: Conserve VRAM by loading and unloading each stage of the pipeline sequentially
         :param sha_chunksize: Chunksize to use when calculating sha256 model hash
-        '''
+        """
         self.model_infos: Dict[str, ModelBase] = dict()
         # allow lazy offloading only when vram cache enabled
         self.lazy_offloading = lazy_offloading and max_vram_cache_size > 0
-        self.precision: torch.dtype=precision
-        self.max_cache_size: float=max_cache_size
-        self.max_vram_cache_size: float=max_vram_cache_size
-        self.execution_device: torch.device=execution_device
-        self.storage_device: torch.device=storage_device
-        self.sha_chunksize=sha_chunksize
+        self.precision: torch.dtype = precision
+        self.max_cache_size: float = max_cache_size
+        self.max_vram_cache_size: float = max_vram_cache_size
+        self.execution_device: torch.device = execution_device
+        self.storage_device: torch.device = storage_device
+        self.sha_chunksize = sha_chunksize
         self.logger = logger
 
         self._cached_models = dict()
         self._cache_stack = list()
 
     def get_key(
         self,
         model_path: str,
         base_model: BaseModelType,
         model_type: ModelType,
         submodel_type: Optional[SubModelType] = None,
     ):
-
         key = f"{model_path}:{base_model}:{model_type}"
         if submodel_type:
             key += f":{submodel_type}"
         return key
 
     def _get_model_info(
         self,
@@ -159,15 +161,14 @@
         model_path: Union[str, Path],
         model_class: Type[ModelBase],
         base_model: BaseModelType,
         model_type: ModelType,
         submodel: Optional[SubModelType] = None,
         gpu_load: bool = True,
     ) -> Any:
-
         if not isinstance(model_path, Path):
             model_path = Path(model_path)
 
         if not os.path.exists(model_path):
             raise Exception(f"Model not found: {model_path}")
 
         model_info = self._get_model_info(
@@ -182,88 +183,87 @@
             model_type=model_type,
             submodel_type=submodel,
         )
 
         # TODO: lock for no copies on simultaneous calls?
         cache_entry = self._cached_models.get(key, None)
         if cache_entry is None:
-            self.logger.info(f'Loading model {model_path}, type {base_model}:{model_type}:{submodel}')
+            self.logger.info(f"Loading model {model_path}, type {base_model}:{model_type}:{submodel}")
 
             # this will remove older cached models until
             # there is sufficient room to load the requested model
             self._make_cache_room(model_info.get_size(submodel))
 
             # clean memory to make MemoryUsage() more accurate
             gc.collect()
             model = model_info.get_model(child_type=submodel, torch_dtype=self.precision)
             if mem_used := model_info.get_size(submodel):
-                self.logger.debug(f'CPU RAM used for load: {(mem_used/GIG):.2f} GB')
+                self.logger.debug(f"CPU RAM used for load: {(mem_used/GIG):.2f} GB")
 
             cache_entry = _CacheRecord(self, model, mem_used)
             self._cached_models[key] = cache_entry
 
         with suppress(Exception):
             self._cache_stack.remove(key)
         self._cache_stack.append(key)
 
         return self.ModelLocker(self, key, cache_entry.model, gpu_load, cache_entry.size)
 
     class ModelLocker(object):
         def __init__(self, cache, key, model, gpu_load, size_needed):
-            '''
+            """
             :param cache: The model_cache object
             :param key: The key of the model to lock in GPU
             :param model: The model to lock
             :param gpu_load: True if load into gpu
             :param size_needed: Size of the model to load
-            '''
+            """
             self.gpu_load = gpu_load
             self.cache = cache
             self.key = key
             self.model = model
             self.size_needed = size_needed
             self.cache_entry = self.cache._cached_models[self.key]
 
         def __enter__(self) -> Any:
-            if not hasattr(self.model, 'to'):
+            if not hasattr(self.model, "to"):
                 return self.model
 
             # NOTE that the model has to have the to() method in order for this
             # code to move it into GPU!
             if self.gpu_load:
                 self.cache_entry.lock()
 
                 try:
                     if self.cache.lazy_offloading:
-                       self.cache._offload_unlocked_models(self.size_needed)
-                       
+                        self.cache._offload_unlocked_models(self.size_needed)
+
                     if self.model.device != self.cache.execution_device:
-                        self.cache.logger.debug(f'Moving {self.key} into {self.cache.execution_device}')
+                        self.cache.logger.debug(f"Moving {self.key} into {self.cache.execution_device}")
                         with VRAMUsage() as mem:
                             self.model.to(self.cache.execution_device)  # move into GPU
-                        self.cache.logger.debug(f'GPU VRAM used for load: {(mem.vram_used/GIG):.2f} GB')
-                        
-                    self.cache.logger.debug(f'Locking {self.key} in {self.cache.execution_device}')                
+                        self.cache.logger.debug(f"GPU VRAM used for load: {(mem.vram_used/GIG):.2f} GB")
+
+                    self.cache.logger.debug(f"Locking {self.key} in {self.cache.execution_device}")
                     self.cache._print_cuda_stats()
 
                 except:
                     self.cache_entry.unlock()
                     raise
 
-            
             # TODO: not fully understand
             # in the event that the caller wants the model in RAM, we
             # move it into CPU if it is in GPU and not locked
             elif self.cache_entry.loaded and not self.cache_entry.locked:
                 self.model.to(self.cache.storage_device)
 
             return self.model
 
         def __exit__(self, type, value, traceback):
-            if not hasattr(self.model, 'to'):
+            if not hasattr(self.model, "to"):
                 return
 
             self.cache_entry.unlock()
             if not self.cache.lazy_offloading:
                 self.cache._offload_unlocked_models()
                 self.cache._print_cuda_stats()
 
@@ -273,28 +273,28 @@
             self._cache_stack.remove(cache_id)
         self._cached_models.pop(cache_id, None)
 
     def model_hash(
         self,
         model_path: Union[str, Path],
     ) -> str:
-        '''
+        """
         Given the HF repo id or path to a model on disk, returns a unique
         hash. Works for legacy checkpoint files, HF models on disk, and HF repo IDs
         :param model_path: Path to model file/directory on disk.
-        '''
+        """
         return self._local_model_hash(model_path)
 
     def cache_size(self) -> float:
         "Return the current size of the cache, in GB"
         current_cache_size = sum([m.size for m in self._cached_models.values()])
         return current_cache_size / GIG
 
     def _has_cuda(self) -> bool:
-        return self.execution_device.type == 'cuda'
+        return self.execution_device.type == "cuda"
 
     def _print_cuda_stats(self):
         vram = "%4.2fG" % (torch.cuda.memory_allocated() / GIG)
         ram = "%4.2fG" % self.cache_size()
 
         cached_models = 0
         loaded_models = 0
@@ -302,26 +302,29 @@
         for model_info in self._cached_models.values():
             cached_models += 1
             if model_info.loaded:
                 loaded_models += 1
             if model_info.locked:
                 locked_models += 1
 
-        self.logger.debug(f"Current VRAM/RAM usage: {vram}/{ram}; cached_models/loaded_models/locked_models/ = {cached_models}/{loaded_models}/{locked_models}")
-
+        self.logger.debug(
+            f"Current VRAM/RAM usage: {vram}/{ram}; cached_models/loaded_models/locked_models/ = {cached_models}/{loaded_models}/{locked_models}"
+        )
 
     def _make_cache_room(self, model_size):
         # calculate how much memory this model will require
-        #multiplier = 2 if self.precision==torch.float32 else 1
+        # multiplier = 2 if self.precision==torch.float32 else 1
         bytes_needed = model_size
         maximum_size = self.max_cache_size * GIG  # stored in GB, convert to bytes
         current_size = sum([m.size for m in self._cached_models.values()])
 
         if current_size + bytes_needed > maximum_size:
-            self.logger.debug(f'Max cache size exceeded: {(current_size/GIG):.2f}/{self.max_cache_size:.2f} GB, need an additional {(bytes_needed/GIG):.2f} GB')
+            self.logger.debug(
+                f"Max cache size exceeded: {(current_size/GIG):.2f}/{self.max_cache_size:.2f} GB, need an additional {(bytes_needed/GIG):.2f} GB"
+            )
 
         self.logger.debug(f"Before unloading: cached_models={len(self._cached_models)}")
 
         pos = 0
         while current_size + bytes_needed > maximum_size and pos < len(self._cache_stack):
             model_key = self._cache_stack[pos]
             cache_entry = self._cached_models[model_key]
@@ -335,87 +338,90 @@
                     cleared = False
                     for referrer in gc.get_referrers(cache_entry.model):
                         if type(referrer).__name__ == "frame":
                             # RuntimeError: cannot clear an executing frame
                             with suppress(RuntimeError):
                                 referrer.clear()
                                 cleared = True
-                                #break
+                                # break
 
                     # repeat if referrers changes(due to frame clear), else exit loop
                     if cleared:
                         gc.collect()
                     else:
                         break
 
             device = cache_entry.model.device if hasattr(cache_entry.model, "device") else None
-            self.logger.debug(f"Model: {model_key}, locks: {cache_entry._locks}, device: {device}, loaded: {cache_entry.loaded}, refs: {refs}")
+            self.logger.debug(
+                f"Model: {model_key}, locks: {cache_entry._locks}, device: {device}, loaded: {cache_entry.loaded}, refs: {refs}"
+            )
 
             # 2 refs:
             # 1 from cache_entry
             # 1 from getrefcount function
             if not cache_entry.locked and refs <= 2:
-                self.logger.debug(f'Unloading model {model_key} to free {(model_size/GIG):.2f} GB (-{(cache_entry.size/GIG):.2f} GB)')
+                self.logger.debug(
+                    f"Unloading model {model_key} to free {(model_size/GIG):.2f} GB (-{(cache_entry.size/GIG):.2f} GB)"
+                )
                 current_size -= cache_entry.size
                 del self._cache_stack[pos]
                 del self._cached_models[model_key]
                 del cache_entry
 
             else:
                 pos += 1
 
         gc.collect()
         torch.cuda.empty_cache()
 
         self.logger.debug(f"After unloading: cached_models={len(self._cached_models)}")
 
-    def _offload_unlocked_models(self, size_needed: int=0):
+    def _offload_unlocked_models(self, size_needed: int = 0):
         reserved = self.max_vram_cache_size * GIG
         vram_in_use = torch.cuda.memory_allocated()
-        self.logger.debug(f'{(vram_in_use/GIG):.2f}GB VRAM used for models; max allowed={(reserved/GIG):.2f}GB')
-        for model_key, cache_entry in sorted(self._cached_models.items(), key=lambda x:x[1].size):
+        self.logger.debug(f"{(vram_in_use/GIG):.2f}GB VRAM used for models; max allowed={(reserved/GIG):.2f}GB")
+        for model_key, cache_entry in sorted(self._cached_models.items(), key=lambda x: x[1].size):
             if vram_in_use <= reserved:
                 break
             if not cache_entry.locked and cache_entry.loaded:
-                self.logger.debug(f'Offloading {model_key} from {self.execution_device} into {self.storage_device}')
+                self.logger.debug(f"Offloading {model_key} from {self.execution_device} into {self.storage_device}")
                 with VRAMUsage() as mem:
                     cache_entry.model.to(self.storage_device)
-                self.logger.debug(f'GPU VRAM freed: {(mem.vram_used/GIG):.2f} GB')
+                self.logger.debug(f"GPU VRAM freed: {(mem.vram_used/GIG):.2f} GB")
                 vram_in_use += mem.vram_used  # note vram_used is negative
-                self.logger.debug(f'{(vram_in_use/GIG):.2f}GB VRAM used for models; max allowed={(reserved/GIG):.2f}GB')
+                self.logger.debug(f"{(vram_in_use/GIG):.2f}GB VRAM used for models; max allowed={(reserved/GIG):.2f}GB")
 
         gc.collect()
         torch.cuda.empty_cache()
-        
+
     def _local_model_hash(self, model_path: Union[str, Path]) -> str:
         sha = hashlib.sha256()
         path = Path(model_path)
-        
+
         hashpath = path / "checksum.sha256"
         if hashpath.exists() and path.stat().st_mtime <= hashpath.stat().st_mtime:
             with open(hashpath) as f:
                 hash = f.read()
             return hash
-        
-        self.logger.debug(f'computing hash of model {path.name}')
-        for file in list(path.rglob("*.ckpt")) \
-            + list(path.rglob("*.safetensors")) \
-            + list(path.rglob("*.pth")):
+
+        self.logger.debug(f"computing hash of model {path.name}")
+        for file in list(path.rglob("*.ckpt")) + list(path.rglob("*.safetensors")) + list(path.rglob("*.pth")):
             with open(file, "rb") as f:
                 while chunk := f.read(self.sha_chunksize):
                     sha.update(chunk)
         hash = sha.hexdigest()
         with open(hashpath, "w") as f:
             f.write(hash)
         return hash
 
+
 class VRAMUsage(object):
     def __init__(self):
         self.vram = None
         self.vram_used = 0
-        
+
     def __enter__(self):
         self.vram = torch.cuda.memory_allocated()
         return self
 
     def __exit__(self, *args):
         self.vram_used = torch.cuda.memory_allocated() - self.vram
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_manager.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -245,54 +245,64 @@
 
 import invokeai.backend.util.logging as logger
 from invokeai.app.services.config import InvokeAIAppConfig
 from invokeai.backend.util import CUDA_DEVICE, Chdir
 from .model_cache import ModelCache, ModelLocker
 from .model_search import ModelSearch
 from .models import (
-    BaseModelType, ModelType, SubModelType,
-    ModelError, SchedulerPredictionType, MODEL_CLASSES,
+    BaseModelType,
+    ModelType,
+    SubModelType,
+    ModelError,
+    SchedulerPredictionType,
+    MODEL_CLASSES,
     ModelConfigBase,
-    ModelNotFoundException, InvalidModelException,
+    ModelNotFoundException,
+    InvalidModelException,
     DuplicateModelException,
 )
 
 # We are only starting to number the config file with release 3.
 # The config file version doesn't have to start at release version, but it will help
 # reduce confusion.
-CONFIG_FILE_VERSION='3.0.0'
+CONFIG_FILE_VERSION = "3.0.0"
+
 
 @dataclass
-class ModelInfo():
+class ModelInfo:
     context: ModelLocker
     name: str
     base_model: BaseModelType
     type: ModelType
     hash: str
     location: Union[Path, str]
     precision: torch.dtype
     _cache: ModelCache = None
 
     def __enter__(self):
         return self.context.__enter__()
 
-    def __exit__(self,*args, **kwargs):
+    def __exit__(self, *args, **kwargs):
         self.context.__exit__(*args, **kwargs)
 
+
 class AddModelResult(BaseModel):
     name: str = Field(description="The name of the model after installation")
     model_type: ModelType = Field(description="The type of model")
     base_model: BaseModelType = Field(description="The base model")
     config: ModelConfigBase = Field(description="The configuration of the model")
 
+
 MAX_CACHE_SIZE = 6.0  # GB
 
+
 class ConfigMeta(BaseModel):
     version: str
 
+
 class ModelManager(object):
     """
     High-level interface to model management.
     """
 
     logger: types.ModuleType = logger
 
@@ -311,48 +321,48 @@
         and sequential_offload boolean. Note that the default device
         type and precision are set up for a CUDA system running at half precision.
         """
         self.config_path = None
         if isinstance(config, (str, Path)):
             self.config_path = Path(config)
             if not self.config_path.exists():
-                logger.warning(f'The file {self.config_path} was not found. Initializing a new file')
+                logger.warning(f"The file {self.config_path} was not found. Initializing a new file")
                 self.initialize_model_config(self.config_path)
             config = OmegaConf.load(self.config_path)
 
         elif not isinstance(config, DictConfig):
-            raise ValueError('config argument must be an OmegaConf object, a Path or a string')
+            raise ValueError("config argument must be an OmegaConf object, a Path or a string")
 
         self.config_meta = ConfigMeta(**config.pop("__metadata__"))
         # TODO: metadata not found
         # TODO: version check
 
         self.app_config = InvokeAIAppConfig.get_config()
         self.logger = logger
         self.cache = ModelCache(
             max_cache_size=max_cache_size,
-            max_vram_cache_size = self.app_config.max_vram_cache_size,
-            execution_device = device_type,
-            precision = precision,
-            sequential_offload = sequential_offload,
-            logger = logger,
+            max_vram_cache_size=self.app_config.max_vram_cache_size,
+            execution_device=device_type,
+            precision=precision,
+            sequential_offload=sequential_offload,
+            logger=logger,
         )
 
         self._read_models(config)
 
     def _read_models(self, config: Optional[DictConfig] = None):
         if not config:
             if self.config_path:
                 config = OmegaConf.load(self.config_path)
             else:
                 return
 
         self.models = dict()
         for model_key, model_config in config.items():
-            if model_key.startswith('_'):
+            if model_key.startswith("_"):
                 continue
             model_name, base_model, model_type = self.parse_key(model_key)
             model_class = MODEL_CLASSES[base_model][model_type]
             # alias for config file
             model_config["model_format"] = model_config.pop("format")
             self.models[model_key] = model_class.create_config(**model_config)
 
@@ -387,19 +397,23 @@
     @classmethod
     def create_key(
         cls,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
     ) -> str:
-        return f"{base_model}/{model_type}/{model_name}"
+        # In 3.11, the behavior of (str,enum) when interpolated into a
+        # string has changed. The next two lines are defensive.
+        base_model = BaseModelType(base_model)
+        model_type = ModelType(model_type)
+        return f"{base_model.value}/{model_type.value}/{model_name}"
 
     @classmethod
     def parse_key(cls, model_key: str) -> Tuple[str, BaseModelType, ModelType]:
-        base_model_str, model_type_str, model_name = model_key.split('/', 2)
+        base_model_str, model_type_str, model_name = model_key.split("/", 2)
         try:
             model_type = ModelType(model_type_str)
         except:
             raise Exception(f"Unknown model type: {model_type_str}")
 
         try:
             base_model = BaseModelType(base_model_str)
@@ -410,28 +424,24 @@
 
     def _get_model_cache_path(self, model_path):
         return self.app_config.models_path / ".cache" / hashlib.md5(str(model_path).encode()).hexdigest()
 
     @classmethod
     def initialize_model_config(cls, config_path: Path):
         """Create empty config file"""
-        with open(config_path,'w') as yaml_file:
-            yaml_file.write(yaml.dump({'__metadata__':
-                                       {'version':'3.0.0'}
-                                       }
-                                      )
-                            )
+        with open(config_path, "w") as yaml_file:
+            yaml_file.write(yaml.dump({"__metadata__": {"version": "3.0.0"}}))
 
     def get_model(
         self,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
-        submodel_type: Optional[SubModelType] = None
-    )->ModelInfo:
+        submodel_type: Optional[SubModelType] = None,
+    ) -> ModelInfo:
         """Given a model named identified in models.yaml, return
         an ModelInfo object describing it.
         :param model_name: symbolic name of the model in models.yaml
         :param model_type: ModelType enum indicating the type of model to return
         :param base_model: BaseModelType enum indicating the base model used by this model
         :param submode_typel: an ModelType enum indicating the portion of
                the model to retrieve (e.g. ModelType.Vae)
@@ -447,15 +457,15 @@
 
         model_config = self.models[model_key]
         model_path = self.app_config.root_path / model_config.path
 
         if not model_path.exists():
             if model_class.save_to_config:
                 self.models[model_key].error = ModelError.NotFound
-                raise Exception(f"Files for model \"{model_key}\" not found")
+                raise Exception(f'Files for model "{model_key}" not found')
 
             else:
                 self.models.pop(model_key, None)
                 raise ModelNotFoundException(f"Model not found - {model_key}")
 
         # vae/movq override
         # TODO:
@@ -469,15 +479,15 @@
 
         # TODO: path
         # TODO: is it accurate to use path as id
         dst_convert_path = self._get_model_cache_path(model_path)
 
         model_path = model_class.convert_if_required(
             base_model=base_model,
-            model_path=str(model_path), # TODO: refactor str/Path types logic
+            model_path=str(model_path),  # TODO: refactor str/Path types logic
             output_path=dst_convert_path,
             config=model_config,
         )
 
         model_context = self.cache.get_model(
             model_path=model_path,
             model_class=model_class,
@@ -486,25 +496,25 @@
             submodel=submodel_type,
         )
 
         if model_key not in self.cache_keys:
             self.cache_keys[model_key] = set()
         self.cache_keys[model_key].add(model_context.key)
 
-        model_hash = "<NO_HASH>" # TODO:
+        model_hash = "<NO_HASH>"  # TODO:
 
         return ModelInfo(
-            context = model_context,
-            name = model_name,
-            base_model = base_model,
-            type = submodel_type or model_type,
-            hash = model_hash,
-            location = model_path, # TODO:
-            precision = self.cache.precision,
-            _cache = self.cache,
+            context=model_context,
+            name=model_name,
+            base_model=base_model,
+            type=submodel_type or model_type,
+            hash=model_hash,
+            location=model_path,  # TODO:
+            precision=self.cache.precision,
+            _cache=self.cache,
         )
 
     def model_info(
         self,
         model_name: str,
         base_model: BaseModelType,
         model_type: ModelType,
@@ -512,53 +522,57 @@
         """
         Given a model name returns the OmegaConf (dict-like) object describing it.
         """
         model_key = self.create_key(model_name, base_model, model_type)
         if model_key in self.models:
             return self.models[model_key].dict(exclude_defaults=True)
         else:
-            return None # TODO: None or empty dict on not found
+            return None  # TODO: None or empty dict on not found
 
     def model_names(self) -> List[Tuple[str, BaseModelType, ModelType]]:
         """
         Return a list of (str, BaseModelType, ModelType) corresponding to all models
         known to the configuration.
         """
         return [(self.parse_key(x)) for x in self.models.keys()]
 
     def list_model(
-            self,
-            model_name: str,
-            base_model: BaseModelType,
-            model_type: ModelType,
+        self,
+        model_name: str,
+        base_model: BaseModelType,
+        model_type: ModelType,
     ) -> dict:
         """
         Returns a dict describing one installed model, using
         the combined format of the list_models() method.
         """
-        models = self.list_models(base_model,model_type,model_name)
+        models = self.list_models(base_model, model_type, model_name)
         return models[0] if models else None
 
     def list_models(
         self,
         base_model: Optional[BaseModelType] = None,
         model_type: Optional[ModelType] = None,
         model_name: Optional[str] = None,
     ) -> list[dict]:
         """
         Return a list of models.
         """
 
-        model_keys = [self.create_key(model_name, base_model, model_type)] if model_name else sorted(self.models, key=str.casefold)
+        model_keys = (
+            [self.create_key(model_name, base_model, model_type)]
+            if model_name
+            else sorted(self.models, key=str.casefold)
+        )
         models = []
         for model_key in model_keys:
             model_config = self.models.get(model_key)
             if not model_config:
-                self.logger.error(f'Unknown model {model_name}')
-                raise ModelNotFoundException(f'Unknown model {model_name}')
+                self.logger.error(f"Unknown model {model_name}")
+                raise ModelNotFoundException(f"Unknown model {model_name}")
 
             cur_model_name, cur_base_model, cur_model_type = self.parse_key(model_key)
             if base_model is not None and cur_base_model != base_model:
                 continue
             if model_type is not None and cur_model_type != model_type:
                 continue
 
@@ -567,16 +581,16 @@
                 # OpenAPIModelInfoBase
                 model_name=cur_model_name,
                 base_model=cur_base_model,
                 model_type=cur_model_type,
             )
 
             # expose paths as absolute to help web UI
-            if path := model_dict.get('path'):
-                model_dict['path'] = str(self.app_config.root_path / path)
+            if path := model_dict.get("path"):
+                model_dict["path"] = str(self.app_config.root_path / path)
             models.append(model_dict)
 
         return models
 
     def print_models(self) -> None:
         """
         Print a table of models and their descriptions. This needs to be redone
@@ -637,23 +651,23 @@
         method will return True. Will fail with an assertion error if provided
         attributes are incorrect or the model name is missing.
 
         The returned dict has the same format as the dict returned by
         model_info().
         """
         # relativize paths as they go in - this makes it easier to move the root directory around
-        if path := model_attributes.get('path'):
+        if path := model_attributes.get("path"):
             if Path(path).is_relative_to(self.app_config.root_path):
-                model_attributes['path'] = str(Path(path).relative_to(self.app_config.root_path))
+                model_attributes["path"] = str(Path(path).relative_to(self.app_config.root_path))
 
         model_class = MODEL_CLASSES[base_model][model_type]
         model_config = model_class.create_config(**model_attributes)
         model_key = self.create_key(model_name, base_model, model_type)
 
-        if  model_key in self.models and not clobber:
+        if model_key in self.models and not clobber:
             raise Exception(f'Attempt to overwrite existing model definition "{model_key}"')
 
         old_model = self.models.pop(model_key, None)
         if old_model is not None:
             # TODO: if path changed and old_model.path inside models folder should we delete this too?
 
             # remove conversion cache as config changed
@@ -671,31 +685,31 @@
             for cache_id in cache_ids:
                 self.cache.uncache_model(cache_id)
 
         self.models[model_key] = model_config
         self.commit()
 
         return AddModelResult(
-            name = model_name,
-            model_type = model_type,
-            base_model = base_model,
-            config = model_config,
+            name=model_name,
+            model_type=model_type,
+            base_model=base_model,
+            config=model_config,
         )
 
     def rename_model(
-            self,
-            model_name: str,
-            base_model: BaseModelType,
-            model_type: ModelType,
-            new_name: str = None,
-            new_base: BaseModelType = None,
+        self,
+        model_name: str,
+        base_model: BaseModelType,
+        model_type: ModelType,
+        new_name: str = None,
+        new_base: BaseModelType = None,
     ):
-        '''
+        """
         Rename or rebase a model.
-        '''
+        """
         if new_name is None and new_base is None:
             self.logger.error("rename_model() called with neither a new_name nor a new_base. {model_name} unchanged.")
             return
 
         model_key = self.create_key(model_name, base_model, model_type)
         model_cfg = self.models.get(model_key, None)
         if not model_cfg:
@@ -706,15 +720,21 @@
         new_base = new_base or base_model
         new_key = self.create_key(new_name, new_base, model_type)
         if new_key in self.models:
             raise ValueError(f'Attempt to overwrite existing model definition "{new_key}"')
 
         # if this is a model file/directory that we manage ourselves, we need to move it
         if old_path.is_relative_to(self.app_config.models_path):
-            new_path = self.app_config.root_path / 'models' / BaseModelType(new_base).value / ModelType(model_type).value / new_name
+            new_path = (
+                self.app_config.root_path
+                / "models"
+                / BaseModelType(new_base).value
+                / ModelType(model_type).value
+                / new_name
+            )
             move(old_path, new_path)
             model_cfg.path = str(new_path.relative_to(self.app_config.root_path))
 
         # clean up caches
         old_model_cache = self._get_model_cache_path(old_path)
         if old_model_cache.exists():
             if old_model_cache.is_dir():
@@ -722,63 +742,68 @@
             else:
                 old_model_cache.unlink()
 
         cache_ids = self.cache_keys.pop(model_key, [])
         for cache_id in cache_ids:
             self.cache.uncache_model(cache_id)
 
-        self.models.pop(model_key, None) # delete
+        self.models.pop(model_key, None)  # delete
         self.models[new_key] = model_cfg
         self.commit()
 
-    def convert_model (
-            self,
-            model_name: str,
-            base_model: BaseModelType,
-            model_type: Union[ModelType.Main,ModelType.Vae],
-            dest_directory: Optional[Path]=None,
+    def convert_model(
+        self,
+        model_name: str,
+        base_model: BaseModelType,
+        model_type: Union[ModelType.Main, ModelType.Vae],
+        dest_directory: Optional[Path] = None,
     ) -> AddModelResult:
-        '''
+        """
         Convert a checkpoint file into a diffusers folder, deleting the cached
         version and deleting the original checkpoint file if it is in the models
         directory.
         :param model_name: Name of the model to convert
         :param base_model: Base model type
         :param model_type: Type of model ['vae' or 'main']
 
         This will raise a ValueError unless the model is a checkpoint.
-        '''
+        """
         info = self.model_info(model_name, base_model, model_type)
         if info["model_format"] != "checkpoint":
             raise ValueError(f"not a checkpoint format model: {model_name}")
 
         # We are taking advantage of a side effect of get_model() that converts check points
         # into cached diffusers directories stored at `location`. It doesn't matter
         # what submodeltype we request here, so we get the smallest.
-        submodel = {"submodel_type": SubModelType.Scheduler} if model_type==ModelType.Main else {}
-        model = self.get_model(model_name,
-                               base_model,
-                               model_type,
-                               **submodel,
-                               )
+        submodel = {"submodel_type": SubModelType.Scheduler} if model_type == ModelType.Main else {}
+        model = self.get_model(
+            model_name,
+            base_model,
+            model_type,
+            **submodel,
+        )
         checkpoint_path = self.app_config.root_path / info["path"]
         old_diffusers_path = self.app_config.models_path / model.location
-        new_diffusers_path = (dest_directory or self.app_config.models_path / base_model.value / model_type.value) / model_name
+        new_diffusers_path = (
+            dest_directory or self.app_config.models_path / base_model.value / model_type.value
+        ) / model_name
         if new_diffusers_path.exists():
             raise ValueError(f"A diffusers model already exists at {new_diffusers_path}")
 
         try:
-            move(old_diffusers_path,new_diffusers_path)
+            move(old_diffusers_path, new_diffusers_path)
             info["model_format"] = "diffusers"
-            info["path"] = str(new_diffusers_path) if dest_directory else str(new_diffusers_path.relative_to(self.app_config.root_path))
-            info.pop('config')
+            info["path"] = (
+                str(new_diffusers_path)
+                if dest_directory
+                else str(new_diffusers_path.relative_to(self.app_config.root_path))
+            )
+            info.pop("config")
 
-            result = self.add_model(model_name, base_model, model_type,
-                                    model_attributes = info,
-                                    clobber=True)
+            result = self.add_model(model_name, base_model, model_type, model_attributes=info, clobber=True)
         except:
             # something went wrong, so don't leave dangling diffusers model in directory or it will cause a duplicate model error!
             rmtree(new_diffusers_path)
             raise
 
         if checkpoint_path.exists() and checkpoint_path.is_relative_to(self.app_config.models_path):
             checkpoint_path.unlink()
@@ -794,23 +819,20 @@
         safetensor_files = [x for x in models_folder_safetensors if x.is_file()]
 
         files = ckpt_files + safetensor_files
 
         found_models = []
         for file in files:
             location = str(file.resolve()).replace("\\", "/")
-            if (
-                "model.safetensors" not in location
-                and "diffusion_pytorch_model.safetensors" not in location
-            ):
+            if "model.safetensors" not in location and "diffusion_pytorch_model.safetensors" not in location:
                 found_models.append({"name": file.stem, "location": location})
 
         return search_folder, found_models
 
-    def commit(self, conf_file: Path=None) -> None:
+    def commit(self, conf_file: Path = None) -> None:
         """
         Write current configuration out to the indicated file.
         """
         data_to_save = dict()
         data_to_save["__metadata__"] = self.config_meta.dict()
 
         for model_key, model_config in self.models.items():
@@ -820,15 +842,15 @@
                 # TODO: or exclude_unset better fits here?
                 data_to_save[model_key] = model_config.dict(exclude_defaults=True, exclude={"error"})
                 # alias for config file
                 data_to_save[model_key]["format"] = data_to_save[model_key].pop("model_format")
 
         yaml_str = OmegaConf.to_yaml(data_to_save)
         config_file_path = conf_file or self.config_path
-        assert config_file_path is not None,'no config file path to write to'
+        assert config_file_path is not None, "no config file path to write to"
         config_file_path = self.app_config.root_path / config_file_path
         tmpfile = os.path.join(os.path.dirname(config_file_path), "new_config.tmp")
         try:
             with open(tmpfile, "w", encoding="utf-8") as outfile:
                 outfile.write(self.preamble())
                 outfile.write(yaml_str)
             os.replace(tmpfile, config_file_path)
@@ -853,19 +875,18 @@
         )
 
     def scan_models_directory(
         self,
         base_model: Optional[BaseModelType] = None,
         model_type: Optional[ModelType] = None,
     ):
-
         loaded_files = set()
         new_models_found = False
 
-        self.logger.info(f'Scanning {self.app_config.models_path} for new models')
+        self.logger.info(f"Scanning {self.app_config.models_path} for new models")
         with Chdir(self.app_config.root_path):
             for model_key, model_config in list(self.models.items()):
                 model_name, cur_base_model, cur_model_type = self.parse_key(model_key)
                 model_path = self.app_config.root_path.absolute() / model_config.path
                 if not model_path.exists():
                     model_class = MODEL_CLASSES[cur_base_model][cur_model_type]
                     if model_class.save_to_config:
@@ -883,28 +904,28 @@
                 for cur_model_type in ModelType:
                     if model_type is not None and cur_model_type != model_type:
                         continue
                     model_class = MODEL_CLASSES[cur_base_model][cur_model_type]
                     models_dir = self.app_config.models_path / cur_base_model.value / cur_model_type.value
 
                     if not models_dir.exists():
-                        continue # TODO: or create all folders?
+                        continue  # TODO: or create all folders?
 
                     for model_path in models_dir.iterdir():
-                        if model_path not in loaded_files: # TODO: check
+                        if model_path not in loaded_files:  # TODO: check
                             model_name = model_path.name if model_path.is_dir() else model_path.stem
                             model_key = self.create_key(model_name, cur_base_model, cur_model_type)
 
                             try:
                                 if model_key in self.models:
                                     raise DuplicateModelException(f"Model with key {model_key} added twice")
 
                                 if model_path.is_relative_to(self.app_config.root_path):
                                     model_path = model_path.relative_to(self.app_config.root_path)
-                                    
+
                                 model_config: ModelConfigBase = model_class.probe_config(str(model_path))
                                 self.models[model_key] = model_config
                                 new_models_found = True
                             except DuplicateModelException as e:
                                 self.logger.warning(e)
                             except InvalidModelException:
                                 self.logger.warning(f"Not a valid model: {model_path}")
@@ -912,19 +933,18 @@
                                 self.logger.warning(e)
 
         imported_models = self.autoimport()
 
         if (new_models_found or imported_models) and self.config_path:
             self.commit()
 
-
-    def autoimport(self)->Dict[str, AddModelResult]:
-        '''
+    def autoimport(self) -> Dict[str, AddModelResult]:
+        """
         Scan the autoimport directory (if defined) and import new models, delete defunct models.
-        '''
+        """
         # avoid circular import
         from invokeai.backend.install.model_install_backend import ModelInstall
         from invokeai.frontend.install.model_install import ask_user_for_prediction_type
 
         class ScanAndImport(ModelSearch):
             def __init__(self, directories, logger, ignore: Set[Path], installer: ModelInstall):
                 super().__init__(directories, logger)
@@ -935,49 +955,57 @@
                 self.new_models_found = dict()
 
             def on_model_found(self, model: Path):
                 if model not in self.ignore:
                     self.new_models_found.update(self.installer.heuristic_import(model))
 
             def on_search_completed(self):
-                self.logger.info(f'Scanned {self._items_scanned} files and directories, imported {len(self.new_models_found)} models')
+                self.logger.info(
+                    f"Scanned {self._items_scanned} files and directories, imported {len(self.new_models_found)} models"
+                )
 
             def models_found(self):
                 return self.new_models_found
 
         config = self.app_config
 
         # LS: hacky
         # Patch in the SD VAE from core so that it is available for use by the UI
         try:
-            self.heuristic_import({config.root_path / 'models/core/convert/sd-vae-ft-mse'})
+            self.heuristic_import({config.root_path / "models/core/convert/sd-vae-ft-mse"})
         except:
             pass
 
-        installer = ModelInstall(config = self.app_config,
-                                 model_manager = self,
-                                 prediction_type_helper = ask_user_for_prediction_type,
-                                 )
-        known_paths = {config.root_path / x['path'] for x in self.list_models()}
-        directories = {config.root_path / x for x in [config.autoimport_dir,
-                                                      config.lora_dir,
-                                                      config.embedding_dir,
-                                                      config.controlnet_dir,
-                                                      ] if x
-                       }
+        installer = ModelInstall(
+            config=self.app_config,
+            model_manager=self,
+            prediction_type_helper=ask_user_for_prediction_type,
+        )
+        known_paths = {config.root_path / x["path"] for x in self.list_models()}
+        directories = {
+            config.root_path / x
+            for x in [
+                config.autoimport_dir,
+                config.lora_dir,
+                config.embedding_dir,
+                config.controlnet_dir,
+            ]
+            if x
+        }
         scanner = ScanAndImport(directories, self.logger, ignore=known_paths, installer=installer)
         scanner.search()
-        
+
         return scanner.models_found()
 
-    def heuristic_import(self,
-                         items_to_import: Set[str],
-                         prediction_type_helper: Callable[[Path],SchedulerPredictionType]=None,
-                         )->Dict[str, AddModelResult]:
-        '''Import a list of paths, repo_ids or URLs. Returns the set of
+    def heuristic_import(
+        self,
+        items_to_import: Set[str],
+        prediction_type_helper: Callable[[Path], SchedulerPredictionType] = None,
+    ) -> Dict[str, AddModelResult]:
+        """Import a list of paths, repo_ids or URLs. Returns the set of
         successfully imported items.
         :param items_to_import: Set of strings corresponding to models to be imported.
         :param prediction_type_helper: A callback that receives the Path of a Stable Diffusion 2 checkpoint model and returns a SchedulerPredictionType.
 
         The prediction type helper is necessary to distinguish between
         models based on Stable Diffusion 2 Base (requiring
         SchedulerPredictionType.Epsilson) and Stable Diffusion 768
@@ -988,20 +1016,21 @@
         The result is a set of successfully installed models. Each element
         of the set is a dict corresponding to the newly-created OmegaConf stanza for
         that model.
 
         May return the following exceptions:
         - ModelNotFoundException   - one or more of the items to import is not a valid path, repo_id or URL
         - ValueError - a corresponding model already exists
-        '''
+        """
         # avoid circular import here
         from invokeai.backend.install.model_install_backend import ModelInstall
+
         successfully_installed = dict()
 
-        installer = ModelInstall(config = self.app_config,
-                                 prediction_type_helper = prediction_type_helper,
-                                 model_manager = self)
+        installer = ModelInstall(
+            config=self.app_config, prediction_type_helper=prediction_type_helper, model_manager=self
+        )
         for thing in items_to_import:
             installed = installer.heuristic_import(thing)
             successfully_installed.update(installed)
         self.commit()
         return successfully_installed
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_merge.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_merge.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,31 +13,33 @@
 from diffusers import logging as dlogging
 from typing import List, Union, Optional
 
 import invokeai.backend.util.logging as logger
 
 from ...backend.model_management import ModelManager, ModelType, BaseModelType, ModelVariantType, AddModelResult
 
+
 class MergeInterpolationMethod(str, Enum):
     WeightedSum = "weighted_sum"
     Sigmoid = "sigmoid"
     InvSigmoid = "inv_sigmoid"
     AddDifference = "add_difference"
 
+
 class ModelMerger(object):
     def __init__(self, manager: ModelManager):
         self.manager = manager
 
     def merge_diffusion_models(
-            self,
-            model_paths: List[Path],
-            alpha: float = 0.5,
-            interp: MergeInterpolationMethod = None,
-            force: bool = False,
-            **kwargs,
+        self,
+        model_paths: List[Path],
+        alpha: float = 0.5,
+        interp: MergeInterpolationMethod = None,
+        force: bool = False,
+        **kwargs,
     ) -> DiffusionPipeline:
         """
         :param model_paths:  up to three models, designated by their local paths or HuggingFace repo_ids
         :param alpha: The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha
                    would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2
         :param interp: The interpolation method to use for the merging. Supports "sigmoid", "inv_sigmoid", "add_difference" and None.
                    Passing None uses the default interpolation which is weighted sum interpolation. For merging three checkpoints, only "add_difference" is supported.
@@ -54,32 +56,31 @@
             pipe = DiffusionPipeline.from_pretrained(
                 model_paths[0],
                 custom_pipeline="checkpoint_merger",
             )
             merged_pipe = pipe.merge(
                 pretrained_model_name_or_path_list=model_paths,
                 alpha=alpha,
-                interp=interp.value if interp else None, #diffusers API treats None as "weighted sum"
+                interp=interp.value if interp else None,  # diffusers API treats None as "weighted sum"
                 force=force,
                 **kwargs,
             )
             dlogging.set_verbosity(verbosity)
         return merged_pipe
 
-
-    def merge_diffusion_models_and_save (
-            self,
-            model_names: List[str],
-            base_model: Union[BaseModelType,str],
-            merged_model_name: str,
-            alpha: float = 0.5,
-            interp: MergeInterpolationMethod = None,
-            force: bool = False,
-            merge_dest_directory: Optional[Path] = None,
-            **kwargs,
+    def merge_diffusion_models_and_save(
+        self,
+        model_names: List[str],
+        base_model: Union[BaseModelType, str],
+        merged_model_name: str,
+        alpha: float = 0.5,
+        interp: MergeInterpolationMethod = None,
+        force: bool = False,
+        merge_dest_directory: Optional[Path] = None,
+        **kwargs,
     ) -> AddModelResult:
         """
         :param models: up to three models, designated by their InvokeAI models.yaml model name
         :param base_model: base model (must be the same for all merged models!)
         :param merged_model_name: name for new model
         :param alpha: The interpolation parameter. Ranges from 0 to 1.  It affects the ratio in which the checkpoints are merged. A 0.8 alpha
                    would mean that the first model checkpoints would affect the final result far less than an alpha of 0.2
@@ -90,43 +91,49 @@
         **kwargs - the default DiffusionPipeline.get_config_dict kwargs:
              cache_dir, resume_download, force_download, proxies, local_files_only, use_auth_token, revision, torch_dtype, device_map
         """
         model_paths = list()
         config = self.manager.app_config
         base_model = BaseModelType(base_model)
         vae = None
-        
+
         for mod in model_names:
             info = self.manager.list_model(mod, base_model=base_model, model_type=ModelType.Main)
-            assert info,                                f"model {mod}, base_model {base_model}, is unknown"
-            assert info["model_format"] == "diffusers", f"{mod} is not a diffusers model. It must be optimized before merging"
-            assert info["variant"] == "normal",         f"{mod} is a {info['variant']} model, which cannot currently be merged"
-            assert len(model_names) <= 2 or \
-                interp==MergeInterpolationMethod.AddDifference, "When merging three models, only the 'add_difference' merge method is supported"
+            assert info, f"model {mod}, base_model {base_model}, is unknown"
+            assert (
+                info["model_format"] == "diffusers"
+            ), f"{mod} is not a diffusers model. It must be optimized before merging"
+            assert info["variant"] == "normal", f"{mod} is a {info['variant']} model, which cannot currently be merged"
+            assert (
+                len(model_names) <= 2 or interp == MergeInterpolationMethod.AddDifference
+            ), "When merging three models, only the 'add_difference' merge method is supported"
             # pick up the first model's vae
             if mod == model_names[0]:
                 vae = info.get("vae")
             model_paths.extend([config.root_path / info["path"]])
 
-        merge_method = None if interp == 'weighted_sum' else MergeInterpolationMethod(interp)
-        logger.debug(f'interp = {interp}, merge_method={merge_method}')
-        merged_pipe = self.merge_diffusion_models(
-           model_paths, alpha, merge_method, force, **kwargs
+        merge_method = None if interp == "weighted_sum" else MergeInterpolationMethod(interp)
+        logger.debug(f"interp = {interp}, merge_method={merge_method}")
+        merged_pipe = self.merge_diffusion_models(model_paths, alpha, merge_method, force, **kwargs)
+        dump_path = (
+            Path(merge_dest_directory)
+            if merge_dest_directory
+            else config.models_path / base_model.value / ModelType.Main.value
         )
-        dump_path = Path(merge_dest_directory) if merge_dest_directory else config.models_path / base_model.value / ModelType.Main.value
         dump_path.mkdir(parents=True, exist_ok=True)
         dump_path = dump_path / merged_model_name
 
         merged_pipe.save_pretrained(dump_path, safe_serialization=1)
         attributes = dict(
-            path = str(dump_path),
-            description = f"Merge of models {', '.join(model_names)}",
-            model_format = "diffusers",
-            variant = ModelVariantType.Normal.value,
-            vae = vae,
+            path=str(dump_path),
+            description=f"Merge of models {', '.join(model_names)}",
+            model_format="diffusers",
+            variant=ModelVariantType.Normal.value,
+            vae=vae,
+        )
+        return self.manager.add_model(
+            merged_model_name,
+            base_model=base_model,
+            model_type=ModelType.Main,
+            model_attributes=attributes,
+            clobber=True,
         )
-        return self.manager.add_model(merged_model_name,
-                                      base_model = base_model,
-                                      model_type = ModelType.Main,
-                                      model_attributes = attributes,
-                                      clobber = True
-                                      )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_probe.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_probe.py`

 * *Files 5% similar despite different names*

```diff
@@ -6,120 +6,134 @@
 
 from diffusers import ModelMixin, ConfigMixin
 from pathlib import Path
 from typing import Callable, Literal, Union, Dict, Optional
 from picklescan.scanner import scan_file_path
 
 from .models import (
-    BaseModelType, ModelType, ModelVariantType,
-    SchedulerPredictionType, SilenceWarnings,
-    InvalidModelException
+    BaseModelType,
+    ModelType,
+    ModelVariantType,
+    SchedulerPredictionType,
+    SilenceWarnings,
+    InvalidModelException,
 )
 from .models.base import read_checkpoint_meta
 
+
 @dataclass
 class ModelProbeInfo(object):
     model_type: ModelType
     base_type: BaseModelType
     variant_type: ModelVariantType
     prediction_type: SchedulerPredictionType
     upcast_attention: bool
-    format: Literal['diffusers','checkpoint', 'lycoris']
+    format: Literal["diffusers", "checkpoint", "lycoris"]
     image_size: int
 
+
 class ProbeBase(object):
-    '''forward declaration'''
+    """forward declaration"""
+
     pass
 
+
 class ModelProbe(object):
-    
     PROBES = {
-        'diffusers': { },
-        'checkpoint': { },
+        "diffusers": {},
+        "checkpoint": {},
     }
 
     CLASS2TYPE = {
-        'StableDiffusionPipeline' : ModelType.Main,
-        'StableDiffusionInpaintPipeline' : ModelType.Main,
-        'StableDiffusionXLPipeline' : ModelType.Main,
-        'StableDiffusionXLImg2ImgPipeline' : ModelType.Main,
-        'AutoencoderKL' : ModelType.Vae,
-        'ControlNetModel' : ModelType.ControlNet,
+        "StableDiffusionPipeline": ModelType.Main,
+        "StableDiffusionInpaintPipeline": ModelType.Main,
+        "StableDiffusionXLPipeline": ModelType.Main,
+        "StableDiffusionXLImg2ImgPipeline": ModelType.Main,
+        "AutoencoderKL": ModelType.Vae,
+        "ControlNetModel": ModelType.ControlNet,
     }
-    
+
     @classmethod
-    def register_probe(cls,
-                       format: Literal['diffusers','checkpoint'],
-                       model_type: ModelType,
-                       probe_class: ProbeBase):
+    def register_probe(cls, format: Literal["diffusers", "checkpoint"], model_type: ModelType, probe_class: ProbeBase):
         cls.PROBES[format][model_type] = probe_class
 
     @classmethod
-    def heuristic_probe(cls,
-                        model: Union[Dict, ModelMixin, Path],
-                        prediction_type_helper: Callable[[Path],SchedulerPredictionType]=None,
-                        )->ModelProbeInfo:
-        if isinstance(model,Path):
-            return cls.probe(model_path=model,prediction_type_helper=prediction_type_helper)
-        elif isinstance(model,(dict,ModelMixin,ConfigMixin)):
+    def heuristic_probe(
+        cls,
+        model: Union[Dict, ModelMixin, Path],
+        prediction_type_helper: Callable[[Path], SchedulerPredictionType] = None,
+    ) -> ModelProbeInfo:
+        if isinstance(model, Path):
+            return cls.probe(model_path=model, prediction_type_helper=prediction_type_helper)
+        elif isinstance(model, (dict, ModelMixin, ConfigMixin)):
             return cls.probe(model_path=None, model=model, prediction_type_helper=prediction_type_helper)
         else:
             raise InvalidModelException("model parameter {model} is neither a Path, nor a model")
 
     @classmethod
-    def probe(cls,
-              model_path: Path,
-              model: Optional[Union[Dict, ModelMixin]] = None,
-              prediction_type_helper: Optional[Callable[[Path],SchedulerPredictionType]] = None)->ModelProbeInfo:
-        '''
+    def probe(
+        cls,
+        model_path: Path,
+        model: Optional[Union[Dict, ModelMixin]] = None,
+        prediction_type_helper: Optional[Callable[[Path], SchedulerPredictionType]] = None,
+    ) -> ModelProbeInfo:
+        """
         Probe the model at model_path and return sufficient information about it
         to place it somewhere in the models directory hierarchy. If the model is
         already loaded into memory, you may provide it as model in order to avoid
         opening it a second time. The prediction_type_helper callable is a function that receives
         the path to the model and returns the BaseModelType. It is called to distinguish
         between V2-Base and V2-768 SD models.
-        '''
+        """
         if model_path:
-            format_type = 'diffusers' if model_path.is_dir() else 'checkpoint'
+            format_type = "diffusers" if model_path.is_dir() else "checkpoint"
         else:
-            format_type = 'diffusers' if isinstance(model,(ConfigMixin,ModelMixin)) else 'checkpoint'
+            format_type = "diffusers" if isinstance(model, (ConfigMixin, ModelMixin)) else "checkpoint"
         model_info = None
         try:
-            model_type = cls.get_model_type_from_folder(model_path, model) \
-                if format_type == 'diffusers' \
-                   else cls.get_model_type_from_checkpoint(model_path, model)
+            model_type = (
+                cls.get_model_type_from_folder(model_path, model)
+                if format_type == "diffusers"
+                else cls.get_model_type_from_checkpoint(model_path, model)
+            )
             probe_class = cls.PROBES[format_type].get(model_type)
             if not probe_class:
                 return None
             probe = probe_class(model_path, model, prediction_type_helper)
             base_type = probe.get_base_type()
             variant_type = probe.get_variant_type()
             prediction_type = probe.get_scheduler_prediction_type()
             format = probe.get_format()
             model_info = ModelProbeInfo(
-                model_type = model_type,
-                base_type = base_type,
-                variant_type = variant_type,
-                prediction_type = prediction_type,
-                upcast_attention = (base_type==BaseModelType.StableDiffusion2 \
-                                     and prediction_type==SchedulerPredictionType.VPrediction),
-                format = format,
-                image_size = 1024 if (base_type in {BaseModelType.StableDiffusionXL,BaseModelType.StableDiffusionXLRefiner}) else \
-                              768 if (base_type==BaseModelType.StableDiffusion2 \
-                                     and prediction_type==SchedulerPredictionType.VPrediction ) else \
-                              512
+                model_type=model_type,
+                base_type=base_type,
+                variant_type=variant_type,
+                prediction_type=prediction_type,
+                upcast_attention=(
+                    base_type == BaseModelType.StableDiffusion2
+                    and prediction_type == SchedulerPredictionType.VPrediction
+                ),
+                format=format,
+                image_size=1024
+                if (base_type in {BaseModelType.StableDiffusionXL, BaseModelType.StableDiffusionXLRefiner})
+                else 768
+                if (
+                    base_type == BaseModelType.StableDiffusion2
+                    and prediction_type == SchedulerPredictionType.VPrediction
+                )
+                else 512,
             )
         except Exception:
             raise
 
         return model_info
 
     @classmethod
     def get_model_type_from_checkpoint(cls, model_path: Path, checkpoint: dict) -> ModelType:
-        if model_path.suffix not in ('.bin','.pt','.ckpt','.safetensors','.pth'):
+        if model_path.suffix not in (".bin", ".pt", ".ckpt", ".safetensors", ".pth"):
             return None
 
         if model_path.name == "learned_embeds.bin":
             return ModelType.TextualInversion
 
         ckpt = checkpoint if checkpoint else read_checkpoint_meta(model_path, scan=True)
         ckpt = ckpt.get("state_dict", ckpt)
@@ -138,162 +152,166 @@
             elif key in {"emb_params", "string_to_param"}:
                 return ModelType.TextualInversion
 
         else:
             # diffusers-ti
             if len(ckpt) < 10 and all(isinstance(v, torch.Tensor) for v in ckpt.values()):
                 return ModelType.TextualInversion
-        
+
         raise InvalidModelException(f"Unable to determine model type for {model_path}")
 
     @classmethod
-    def get_model_type_from_folder(cls, folder_path: Path, model: ModelMixin)->ModelType:
-        '''
+    def get_model_type_from_folder(cls, folder_path: Path, model: ModelMixin) -> ModelType:
+        """
         Get the model type of a hugging-face style folder.
-        '''
+        """
         class_name = None
         if model:
             class_name = model.__class__.__name__
         else:
-            if (folder_path / 'learned_embeds.bin').exists():
+            if (folder_path / "learned_embeds.bin").exists():
                 return ModelType.TextualInversion
 
-            if (folder_path / 'pytorch_lora_weights.bin').exists():
+            if (folder_path / "pytorch_lora_weights.bin").exists():
                 return ModelType.Lora
 
-            i  = folder_path / 'model_index.json'
-            c = folder_path / 'config.json'
+            i = folder_path / "model_index.json"
+            c = folder_path / "config.json"
             config_path = i if i.exists() else c if c.exists() else None
 
             if config_path:
-                with open(config_path,'r') as file:
+                with open(config_path, "r") as file:
                     conf = json.load(file)
-                class_name = conf['_class_name']
+                class_name = conf["_class_name"]
 
         if class_name and (type := cls.CLASS2TYPE.get(class_name)):
             return type
 
         # give up
         raise InvalidModelException(f"Unable to determine model type for {folder_path}")
 
     @classmethod
-    def _scan_and_load_checkpoint(cls,model_path: Path)->dict:
+    def _scan_and_load_checkpoint(cls, model_path: Path) -> dict:
         with SilenceWarnings():
             if model_path.suffix.endswith((".ckpt", ".pt", ".bin")):
                 cls._scan_model(model_path, model_path)
                 return torch.load(model_path)
             else:
                 return safetensors.torch.load_file(model_path)
 
     @classmethod
     def _scan_model(cls, model_name, checkpoint):
-            """
-            Apply picklescanner to the indicated checkpoint and issue a warning
-            and option to exit if an infected file is identified.
-            """
-            # scan model
-            scan_result = scan_file_path(checkpoint)
-            if scan_result.infected_files != 0:
-                raise "The model {model_name} is potentially infected by malware. Aborting import."
+        """
+        Apply picklescanner to the indicated checkpoint and issue a warning
+        and option to exit if an infected file is identified.
+        """
+        # scan model
+        scan_result = scan_file_path(checkpoint)
+        if scan_result.infected_files != 0:
+            raise "The model {model_name} is potentially infected by malware. Aborting import."
+
 
 ###################################################3
 # Checkpoint probing
 ###################################################3
 class ProbeBase(object):
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         pass
 
-    def get_variant_type(self)->ModelVariantType:
+    def get_variant_type(self) -> ModelVariantType:
         pass
-    
-    def get_scheduler_prediction_type(self)->SchedulerPredictionType:
+
+    def get_scheduler_prediction_type(self) -> SchedulerPredictionType:
         pass
 
-    def get_format(self)->str:
+    def get_format(self) -> str:
         pass
 
+
 class CheckpointProbeBase(ProbeBase):
-    def __init__(self,
-                 checkpoint_path: Path,
-                 checkpoint: dict,
-                 helper: Callable[[Path],SchedulerPredictionType] = None
-                 )->BaseModelType:
+    def __init__(
+        self, checkpoint_path: Path, checkpoint: dict, helper: Callable[[Path], SchedulerPredictionType] = None
+    ) -> BaseModelType:
         self.checkpoint = checkpoint or ModelProbe._scan_and_load_checkpoint(checkpoint_path)
         self.checkpoint_path = checkpoint_path
         self.helper = helper
 
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         pass
 
-    def get_format(self)->str:
-        return 'checkpoint'
+    def get_format(self) -> str:
+        return "checkpoint"
 
-    def get_variant_type(self)-> ModelVariantType:
-        model_type = ModelProbe.get_model_type_from_checkpoint(self.checkpoint_path,self.checkpoint)
+    def get_variant_type(self) -> ModelVariantType:
+        model_type = ModelProbe.get_model_type_from_checkpoint(self.checkpoint_path, self.checkpoint)
         if model_type != ModelType.Main:
             return ModelVariantType.Normal
-        state_dict = self.checkpoint.get('state_dict') or self.checkpoint
-        in_channels = state_dict[
-            "model.diffusion_model.input_blocks.0.0.weight"
-        ].shape[1]
+        state_dict = self.checkpoint.get("state_dict") or self.checkpoint
+        in_channels = state_dict["model.diffusion_model.input_blocks.0.0.weight"].shape[1]
         if in_channels == 9:
             return ModelVariantType.Inpaint
         elif in_channels == 5:
             return ModelVariantType.Depth
         elif in_channels == 4:
             return ModelVariantType.Normal
         else:
-            raise InvalidModelException(f"Cannot determine variant type (in_channels={in_channels}) at {self.checkpoint_path}")
+            raise InvalidModelException(
+                f"Cannot determine variant type (in_channels={in_channels}) at {self.checkpoint_path}"
+            )
+
 
 class PipelineCheckpointProbe(CheckpointProbeBase):
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         checkpoint = self.checkpoint
-        state_dict = self.checkpoint.get('state_dict') or checkpoint
+        state_dict = self.checkpoint.get("state_dict") or checkpoint
         key_name = "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight"
         if key_name in state_dict and state_dict[key_name].shape[-1] == 768:
             return BaseModelType.StableDiffusion1
         if key_name in state_dict and state_dict[key_name].shape[-1] == 1024:
             return BaseModelType.StableDiffusion2
-        key_name = 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight'
+        key_name = "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight"
         if key_name in state_dict and state_dict[key_name].shape[-1] == 2048:
             return BaseModelType.StableDiffusionXL
         elif key_name in state_dict and state_dict[key_name].shape[-1] == 1280:
             return BaseModelType.StableDiffusionXLRefiner
         else:
             raise InvalidModelException("Cannot determine base type")
 
-    def get_scheduler_prediction_type(self)->SchedulerPredictionType:
+    def get_scheduler_prediction_type(self) -> SchedulerPredictionType:
         type = self.get_base_type()
         if type == BaseModelType.StableDiffusion1:
             return SchedulerPredictionType.Epsilon
         checkpoint = self.checkpoint
-        state_dict = self.checkpoint.get('state_dict') or checkpoint
+        state_dict = self.checkpoint.get("state_dict") or checkpoint
         key_name = "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight"
         if key_name in state_dict and state_dict[key_name].shape[-1] == 1024:
-            if 'global_step' in checkpoint:
-                if checkpoint['global_step'] == 220000:
+            if "global_step" in checkpoint:
+                if checkpoint["global_step"] == 220000:
                     return SchedulerPredictionType.Epsilon
                 elif checkpoint["global_step"] == 110000:
                     return SchedulerPredictionType.VPrediction
-            if self.checkpoint_path and self.helper \
-               and not self.checkpoint_path.with_suffix('.yaml').exists():  # if a .yaml config file exists, then this step not needed
+            if (
+                self.checkpoint_path and self.helper and not self.checkpoint_path.with_suffix(".yaml").exists()
+            ):  # if a .yaml config file exists, then this step not needed
                 return self.helper(self.checkpoint_path)
             else:
                 return None
 
+
 class VaeCheckpointProbe(CheckpointProbeBase):
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         # I can't find any standalone 2.X VAEs to test with!
         return BaseModelType.StableDiffusion1
 
+
 class LoRACheckpointProbe(CheckpointProbeBase):
-    def get_format(self)->str:
-        return 'lycoris'
+    def get_format(self) -> str:
+        return "lycoris"
 
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         checkpoint = self.checkpoint
         key1 = "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight"
         key2 = "lora_te_text_model_encoder_layers_0_self_attn_k_proj.hada_w1_a"
         lora_token_vector_length = (
             checkpoint[key1].shape[1]
             if key1 in checkpoint
             else checkpoint[key2].shape[0]
@@ -303,171 +321,179 @@
         if lora_token_vector_length == 768:
             return BaseModelType.StableDiffusion1
         elif lora_token_vector_length == 1024:
             return BaseModelType.StableDiffusion2
         else:
             return None
 
+
 class TextualInversionCheckpointProbe(CheckpointProbeBase):
-    def get_format(self)->str:
+    def get_format(self) -> str:
         return None
 
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         checkpoint = self.checkpoint
-        if 'string_to_token' in checkpoint:
-            token_dim = list(checkpoint['string_to_param'].values())[0].shape[-1]
-        elif 'emb_params' in checkpoint:
-            token_dim = checkpoint['emb_params'].shape[-1]
+        if "string_to_token" in checkpoint:
+            token_dim = list(checkpoint["string_to_param"].values())[0].shape[-1]
+        elif "emb_params" in checkpoint:
+            token_dim = checkpoint["emb_params"].shape[-1]
         else:
             token_dim = list(checkpoint.values())[0].shape[0]
         if token_dim == 768:
             return BaseModelType.StableDiffusion1
         elif token_dim == 1024:
             return BaseModelType.StableDiffusion2
         else:
             return None
 
+
 class ControlNetCheckpointProbe(CheckpointProbeBase):
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         checkpoint = self.checkpoint
-        for key_name in ('control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight',
-                         'input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight'
-                         ):
+        for key_name in (
+            "control_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight",
+            "input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight",
+        ):
             if key_name not in checkpoint:
                 continue
             if checkpoint[key_name].shape[-1] == 768:
                 return BaseModelType.StableDiffusion1
             elif checkpoint[key_name].shape[-1] == 1024:
                 return BaseModelType.StableDiffusion2
             elif self.checkpoint_path and self.helper:
                 return self.helper(self.checkpoint_path)
         raise InvalidModelException("Unable to determine base type for {self.checkpoint_path}")
 
+
 ########################################################
 # classes for probing folders
 #######################################################
 class FolderProbeBase(ProbeBase):
-    def __init__(self,
-                 folder_path: Path,
-                 model: ModelMixin = None,
-                 helper: Callable=None  # not used
-                 ):
+    def __init__(self, folder_path: Path, model: ModelMixin = None, helper: Callable = None):  # not used
         self.model = model
         self.folder_path = folder_path
 
-    def get_variant_type(self)->ModelVariantType:
+    def get_variant_type(self) -> ModelVariantType:
         return ModelVariantType.Normal
 
-    def get_format(self)->str:
-        return 'diffusers'
-    
+    def get_format(self) -> str:
+        return "diffusers"
+
+
 class PipelineFolderProbe(FolderProbeBase):
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         if self.model:
             unet_conf = self.model.unet.config
         else:
-            with open(self.folder_path / 'unet' / 'config.json','r') as file:
+            with open(self.folder_path / "unet" / "config.json", "r") as file:
                 unet_conf = json.load(file)
-        if unet_conf['cross_attention_dim'] == 768:
-            return BaseModelType.StableDiffusion1  
-        elif unet_conf['cross_attention_dim'] == 1024:
+        if unet_conf["cross_attention_dim"] == 768:
+            return BaseModelType.StableDiffusion1
+        elif unet_conf["cross_attention_dim"] == 1024:
             return BaseModelType.StableDiffusion2
-        elif unet_conf['cross_attention_dim'] == 1280:
+        elif unet_conf["cross_attention_dim"] == 1280:
             return BaseModelType.StableDiffusionXLRefiner
-        elif unet_conf['cross_attention_dim'] == 2048:
+        elif unet_conf["cross_attention_dim"] == 2048:
             return BaseModelType.StableDiffusionXL
         else:
-            raise InvalidModelException(f'Unknown base model for {self.folder_path}')
+            raise InvalidModelException(f"Unknown base model for {self.folder_path}")
 
-    def get_scheduler_prediction_type(self)->SchedulerPredictionType:
+    def get_scheduler_prediction_type(self) -> SchedulerPredictionType:
         if self.model:
             scheduler_conf = self.model.scheduler.config
         else:
-            with open(self.folder_path / 'scheduler' / 'scheduler_config.json','r') as file:
+            with open(self.folder_path / "scheduler" / "scheduler_config.json", "r") as file:
                 scheduler_conf = json.load(file)
-        if scheduler_conf['prediction_type'] == "v_prediction":
+        if scheduler_conf["prediction_type"] == "v_prediction":
             return SchedulerPredictionType.VPrediction
-        elif scheduler_conf['prediction_type'] == 'epsilon':
+        elif scheduler_conf["prediction_type"] == "epsilon":
             return SchedulerPredictionType.Epsilon
         else:
             return None
-        
-    def get_variant_type(self)->ModelVariantType:
+
+    def get_variant_type(self) -> ModelVariantType:
         # This only works for pipelines! Any kind of
         # exception results in our returning the
         # "normal" variant type
         try:
             if self.model:
                 conf = self.model.unet.config
             else:
-                config_file = self.folder_path / 'unet' / 'config.json'
-                with open(config_file,'r') as file:
+                config_file = self.folder_path / "unet" / "config.json"
+                with open(config_file, "r") as file:
                     conf = json.load(file)
-                
-            in_channels = conf['in_channels']
+
+            in_channels = conf["in_channels"]
             if in_channels == 9:
                 return ModelVariantType.Inpaint
             elif in_channels == 5:
                 return ModelVariantType.Depth
             elif in_channels == 4:
                 return ModelVariantType.Normal
         except:
             pass
         return ModelVariantType.Normal
 
+
 class VaeFolderProbe(FolderProbeBase):
-    def get_base_type(self)->BaseModelType:
-        config_file = self.folder_path / 'config.json'
+    def get_base_type(self) -> BaseModelType:
+        config_file = self.folder_path / "config.json"
         if not config_file.exists():
             raise InvalidModelException(f"Cannot determine base type for {self.folder_path}")
-        with open(config_file,'r') as file:
+        with open(config_file, "r") as file:
             config = json.load(file)
-        return BaseModelType.StableDiffusionXL \
-            if config.get('scaling_factor',0)==0.13025 and config.get('sample_size') in [512, 1024] \
+        return (
+            BaseModelType.StableDiffusionXL
+            if config.get("scaling_factor", 0) == 0.13025 and config.get("sample_size") in [512, 1024]
             else BaseModelType.StableDiffusion1
+        )
+
 
 class TextualInversionFolderProbe(FolderProbeBase):
-    def get_format(self)->str:
+    def get_format(self) -> str:
         return None
-    
-    def get_base_type(self)->BaseModelType:
-        path = self.folder_path / 'learned_embeds.bin'
+
+    def get_base_type(self) -> BaseModelType:
+        path = self.folder_path / "learned_embeds.bin"
         if not path.exists():
             return None
         checkpoint = ModelProbe._scan_and_load_checkpoint(path)
-        return TextualInversionCheckpointProbe(None,checkpoint=checkpoint).get_base_type()
+        return TextualInversionCheckpointProbe(None, checkpoint=checkpoint).get_base_type()
+
 
 class ControlNetFolderProbe(FolderProbeBase):
-    def get_base_type(self)->BaseModelType:
-        config_file = self.folder_path / 'config.json'
+    def get_base_type(self) -> BaseModelType:
+        config_file = self.folder_path / "config.json"
         if not config_file.exists():
             raise InvalidModelException(f"Cannot determine base type for {self.folder_path}")
-        with open(config_file,'r') as file:
+        with open(config_file, "r") as file:
             config = json.load(file)
         # no obvious way to distinguish between sd2-base and sd2-768
-        return BaseModelType.StableDiffusion1 \
-            if config['cross_attention_dim']==768 \
-               else BaseModelType.StableDiffusion2
+        return (
+            BaseModelType.StableDiffusion1 if config["cross_attention_dim"] == 768 else BaseModelType.StableDiffusion2
+        )
+
 
 class LoRAFolderProbe(FolderProbeBase):
-    def get_base_type(self)->BaseModelType:
+    def get_base_type(self) -> BaseModelType:
         model_file = None
-        for suffix in ['safetensors','bin']:
-            base_file = self.folder_path / f'pytorch_lora_weights.{suffix}'
+        for suffix in ["safetensors", "bin"]:
+            base_file = self.folder_path / f"pytorch_lora_weights.{suffix}"
             if base_file.exists():
                 model_file = base_file
                 break
         if not model_file:
-            raise InvalidModelException('Unknown LoRA format encountered')
-        return LoRACheckpointProbe(model_file,None).get_base_type()
+            raise InvalidModelException("Unknown LoRA format encountered")
+        return LoRACheckpointProbe(model_file, None).get_base_type()
+
 
 ############## register probe classes ######
-ModelProbe.register_probe('diffusers', ModelType.Main,  PipelineFolderProbe)
-ModelProbe.register_probe('diffusers', ModelType.Vae, VaeFolderProbe)
-ModelProbe.register_probe('diffusers', ModelType.Lora, LoRAFolderProbe)
-ModelProbe.register_probe('diffusers', ModelType.TextualInversion, TextualInversionFolderProbe)
-ModelProbe.register_probe('diffusers', ModelType.ControlNet, ControlNetFolderProbe)
-ModelProbe.register_probe('checkpoint', ModelType.Main, PipelineCheckpointProbe)
-ModelProbe.register_probe('checkpoint', ModelType.Vae, VaeCheckpointProbe)
-ModelProbe.register_probe('checkpoint', ModelType.Lora, LoRACheckpointProbe)
-ModelProbe.register_probe('checkpoint', ModelType.TextualInversion, TextualInversionCheckpointProbe)
-ModelProbe.register_probe('checkpoint', ModelType.ControlNet, ControlNetCheckpointProbe)
+ModelProbe.register_probe("diffusers", ModelType.Main, PipelineFolderProbe)
+ModelProbe.register_probe("diffusers", ModelType.Vae, VaeFolderProbe)
+ModelProbe.register_probe("diffusers", ModelType.Lora, LoRAFolderProbe)
+ModelProbe.register_probe("diffusers", ModelType.TextualInversion, TextualInversionFolderProbe)
+ModelProbe.register_probe("diffusers", ModelType.ControlNet, ControlNetFolderProbe)
+ModelProbe.register_probe("checkpoint", ModelType.Main, PipelineCheckpointProbe)
+ModelProbe.register_probe("checkpoint", ModelType.Vae, VaeCheckpointProbe)
+ModelProbe.register_probe("checkpoint", ModelType.Lora, LoRACheckpointProbe)
+ModelProbe.register_probe("checkpoint", ModelType.TextualInversion, TextualInversionCheckpointProbe)
+ModelProbe.register_probe("checkpoint", ModelType.ControlNet, ControlNetCheckpointProbe)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/model_search.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/model_search.py`

 * *Files 18% similar despite different names*

```diff
@@ -6,16 +6,17 @@
 import os
 from abc import ABC, abstractmethod
 from typing import List, Set, types
 from pathlib import Path
 
 import invokeai.backend.util.logging as logger
 
+
 class ModelSearch(ABC):
-    def __init__(self, directories: List[Path], logger: types.ModuleType=logger):
+    def __init__(self, directories: List[Path], logger: types.ModuleType = logger):
         """
         Initialize a recursive model directory search.
         :param directories: List of directory Paths to recurse through
         :param logger: Logger to use
         """
         self.directories = directories
         self.logger = logger
@@ -52,52 +53,56 @@
         self.on_search_started()
         for dir in self.directories:
             self.walk_directory(dir)
         self.on_search_completed()
 
     def walk_directory(self, path: Path):
         for root, dirs, files in os.walk(path):
-            if str(Path(root).name).startswith('.'):
+            if str(Path(root).name).startswith("."):
                 self._pruned_paths.add(root)
             if any([Path(root).is_relative_to(x) for x in self._pruned_paths]):
                 continue
-            
+
             self._items_scanned += len(dirs) + len(files)
             for d in dirs:
                 path = Path(root) / d
                 if path in self._scanned_paths or path.parent in self._scanned_dirs:
                     self._scanned_dirs.add(path)
                     continue
-                if any([(path/x).exists() for x in {'config.json','model_index.json','learned_embeds.bin','pytorch_lora_weights.bin'}]):
+                if any(
+                    [
+                        (path / x).exists()
+                        for x in {"config.json", "model_index.json", "learned_embeds.bin", "pytorch_lora_weights.bin"}
+                    ]
+                ):
                     try:
                         self.on_model_found(path)
                         self._models_found += 1
                         self._scanned_dirs.add(path)
                     except Exception as e:
                         self.logger.warning(str(e))
 
             for f in files:
                 path = Path(root) / f
                 if path.parent in self._scanned_dirs:
                     continue
-                if path.suffix in {'.ckpt','.bin','.pth','.safetensors','.pt'}:
+                if path.suffix in {".ckpt", ".bin", ".pth", ".safetensors", ".pt"}:
                     try:
                         self.on_model_found(path)
                         self._models_found += 1
                     except Exception as e:
                         self.logger.warning(str(e))
 
+
 class FindModels(ModelSearch):
     def on_search_started(self):
         self.models_found: Set[Path] = set()
 
-    def on_model_found(self,model: Path):
+    def on_model_found(self, model: Path):
         self.models_found.add(model)
 
     def on_search_completed(self):
         pass
 
     def list_models(self) -> List[Path]:
         self.search()
         return list(self.models_found)
-
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/__init__.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,30 @@
 import inspect
 from enum import Enum
 from pydantic import BaseModel
 from typing import Literal, get_origin
 from .base import (
-    BaseModelType, ModelType, SubModelType, ModelBase, ModelConfigBase,
-    ModelVariantType, SchedulerPredictionType, ModelError, SilenceWarnings,
-    ModelNotFoundException, InvalidModelException, DuplicateModelException
-    )
+    BaseModelType,
+    ModelType,
+    SubModelType,
+    ModelBase,
+    ModelConfigBase,
+    ModelVariantType,
+    SchedulerPredictionType,
+    ModelError,
+    SilenceWarnings,
+    ModelNotFoundException,
+    InvalidModelException,
+    DuplicateModelException,
+)
 from .stable_diffusion import StableDiffusion1Model, StableDiffusion2Model
 from .sdxl import StableDiffusionXLModel
 from .vae import VaeModel
 from .lora import LoRAModel
-from .controlnet import ControlNetModel # TODO:
+from .controlnet import ControlNetModel  # TODO:
 from .textual_inversion import TextualInversionModel
 
 MODEL_CLASSES = {
     BaseModelType.StableDiffusion1: {
         ModelType.Main: StableDiffusion1Model,
         ModelType.Vae: VaeModel,
         ModelType.Lora: LoRAModel,
@@ -41,26 +50,27 @@
         ModelType.Main: StableDiffusionXLModel,
         ModelType.Vae: VaeModel,
         # will not work until support written
         ModelType.Lora: LoRAModel,
         ModelType.ControlNet: ControlNetModel,
         ModelType.TextualInversion: TextualInversionModel,
     },
-    #BaseModelType.Kandinsky2_1: {
+    # BaseModelType.Kandinsky2_1: {
     #    ModelType.Main: Kandinsky2_1Model,
     #    ModelType.MoVQ: MoVQModel,
     #    ModelType.Lora: LoRAModel,
     #    ModelType.ControlNet: ControlNetModel,
     #    ModelType.TextualInversion: TextualInversionModel,
-    #},
+    # },
 }
 
 MODEL_CONFIGS = list()
 OPENAPI_MODEL_CONFIGS = list()
 
+
 class OpenAPIModelInfoBase(BaseModel):
     model_name: str
     base_model: BaseModelType
     model_type: ModelType
 
 
 for base_model, models in MODEL_CLASSES.items():
@@ -68,35 +78,39 @@
         model_configs = set(model_class._get_configs().values())
         model_configs.discard(None)
         MODEL_CONFIGS.extend(model_configs)
 
         # LS: sort to get the checkpoint configs first, which makes
         # for a better template in the Swagger docs
         for cfg in sorted(model_configs, key=lambda x: str(x)):
-            model_name, cfg_name = cfg.__qualname__.split('.')[-2:]
+            model_name, cfg_name = cfg.__qualname__.split(".")[-2:]
             openapi_cfg_name = model_name + cfg_name
             if openapi_cfg_name in vars():
                 continue
 
-            api_wrapper = type(openapi_cfg_name, (cfg, OpenAPIModelInfoBase), dict(
-                __annotations__ = dict(
-                    model_type=Literal[model_type.value],
+            api_wrapper = type(
+                openapi_cfg_name,
+                (cfg, OpenAPIModelInfoBase),
+                dict(
+                    __annotations__=dict(
+                        model_type=Literal[model_type.value],
+                    ),
                 ),
-            ))
+            )
 
-            #globals()[openapi_cfg_name] = api_wrapper
+            # globals()[openapi_cfg_name] = api_wrapper
             vars()[openapi_cfg_name] = api_wrapper
             OPENAPI_MODEL_CONFIGS.append(api_wrapper)
 
+
 def get_model_config_enums():
     enums = list()
 
     for model_config in MODEL_CONFIGS:
-
-        if hasattr(inspect,'get_annotations'):
+        if hasattr(inspect, "get_annotations"):
             fields = inspect.get_annotations(model_config)
         else:
             fields = model_config.__annotations__
         try:
             field = fields["model_format"]
         except:
             raise Exception("format field not found")
@@ -105,18 +119,19 @@
         # model_format: SomeModelFormat
         # model_format: Literal[SomeModelFormat.Diffusers]
         # model_format: Literal[SomeModelFormat.Diffusers, SomeModelFormat.Checkpoint]
 
         if isinstance(field, type) and issubclass(field, str) and issubclass(field, Enum):
             enums.append(field)
 
-        elif get_origin(field) is Literal and all(isinstance(arg, str) and isinstance(arg, Enum) for arg in field.__args__):
+        elif get_origin(field) is Literal and all(
+            isinstance(arg, str) and isinstance(arg, Enum) for arg in field.__args__
+        ):
             enums.append(type(field.__args__[0]))
 
         elif field is None:
             pass
 
         else:
             raise Exception(f"Unsupported format definition in {model_configs.__qualname__}")
 
     return enums
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/base.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,91 +11,106 @@
 import safetensors.torch
 from diffusers import DiffusionPipeline, ConfigMixin
 
 from contextlib import suppress
 from pydantic import BaseModel, Field
 from typing import List, Dict, Optional, Type, Literal, TypeVar, Generic, Callable, Any, Union
 
+
 class DuplicateModelException(Exception):
     pass
 
+
 class InvalidModelException(Exception):
     pass
 
+
 class ModelNotFoundException(Exception):
     pass
 
+
 class BaseModelType(str, Enum):
     StableDiffusion1 = "sd-1"
     StableDiffusion2 = "sd-2"
     StableDiffusionXL = "sdxl"
     StableDiffusionXLRefiner = "sdxl-refiner"
-    #Kandinsky2_1 = "kandinsky-2.1"
+    # Kandinsky2_1 = "kandinsky-2.1"
+
 
 class ModelType(str, Enum):
     Main = "main"
     Vae = "vae"
     Lora = "lora"
-    ControlNet = "controlnet" # used by model_probe
+    ControlNet = "controlnet"  # used by model_probe
     TextualInversion = "embedding"
 
+
 class SubModelType(str, Enum):
     UNet = "unet"
     TextEncoder = "text_encoder"
     TextEncoder2 = "text_encoder_2"
     Tokenizer = "tokenizer"
     Tokenizer2 = "tokenizer_2"
     Vae = "vae"
     Scheduler = "scheduler"
     SafetyChecker = "safety_checker"
-    #MoVQ = "movq"
+    # MoVQ = "movq"
+
 
 class ModelVariantType(str, Enum):
     Normal = "normal"
     Inpaint = "inpaint"
     Depth = "depth"
 
+
 class SchedulerPredictionType(str, Enum):
     Epsilon = "epsilon"
     VPrediction = "v_prediction"
     Sample = "sample"
-    
+
+
 class ModelError(str, Enum):
     NotFound = "not_found"
 
+
 class ModelConfigBase(BaseModel):
-    path: str # or Path
+    path: str  # or Path
     description: Optional[str] = Field(None)
     model_format: Optional[str] = Field(None)
     error: Optional[ModelError] = Field(None)
 
     class Config:
         use_enum_values = True
 
+
 class EmptyConfigLoader(ConfigMixin):
     @classmethod
     def load_config(cls, *args, **kwargs):
         cls.config_name = kwargs.pop("config_name")
         return super().load_config(*args, **kwargs)
 
-T_co = TypeVar('T_co', covariant=True)
+
+T_co = TypeVar("T_co", covariant=True)
+
+
 class classproperty(Generic[T_co]):
     def __init__(self, fget: Callable[[Any], T_co]) -> None:
         self.fget = fget
 
     def __get__(self, instance: Optional[Any], owner: Type[Any]) -> T_co:
         return self.fget(owner)
 
     def __set__(self, instance: Optional[Any], value: Any) -> None:
-        raise AttributeError('cannot set attribute')
+        raise AttributeError("cannot set attribute")
+
 
 class ModelBase(metaclass=ABCMeta):
-    #model_path: str
-    #base_model: BaseModelType
-    #model_type: ModelType
+    # model_path: str
+    # base_model: BaseModelType
+    # model_type: ModelType
 
     def __init__(
         self,
         model_path: str,
         base_model: BaseModelType,
         model_type: ModelType,
     ):
@@ -106,56 +121,57 @@
     def _hf_definition_to_type(self, subtypes: List[str]) -> Type:
         if len(subtypes) < 2:
             raise Exception("Invalid subfolder definition!")
         if all(t is None for t in subtypes):
             return None
         elif any(t is None for t in subtypes):
             raise Exception(f"Unsupported definition: {subtypes}")
-        
+
         if subtypes[0] in ["diffusers", "transformers"]:
             res_type = sys.modules[subtypes[0]]
             subtypes = subtypes[1:]
 
         else:
             res_type = sys.modules["diffusers"]
             res_type = getattr(res_type, "pipelines")
 
-
         for subtype in subtypes:
             res_type = getattr(res_type, subtype)
         return res_type
 
     @classmethod
     def _get_configs(cls):
         with suppress(Exception):
             return cls.__configs
-        
+
         configs = dict()
         for name in dir(cls):
             if name.startswith("__"):
                 continue
 
             value = getattr(cls, name)
             if not isinstance(value, type) or not issubclass(value, ModelConfigBase):
                 continue
 
-            if hasattr(inspect,'get_annotations'):
+            if hasattr(inspect, "get_annotations"):
                 fields = inspect.get_annotations(value)
             else:
                 fields = value.__annotations__
             try:
                 field = fields["model_format"]
             except:
                 raise Exception(f"Invalid config definition - format field not found({cls.__qualname__})")
 
             if isinstance(field, type) and issubclass(field, str) and issubclass(field, Enum):
                 for model_format in field:
                     configs[model_format.value] = value
 
-            elif typing.get_origin(field) is Literal and all(isinstance(arg, str) and isinstance(arg, Enum) for arg in field.__args__):
+            elif typing.get_origin(field) is Literal and all(
+                isinstance(arg, str) and isinstance(arg, Enum) for arg in field.__args__
+            ):
                 for model_format in field.__args__:
                     configs[model_format.value] = value
 
             elif field is None:
                 configs[None] = value
 
             else:
@@ -199,57 +215,55 @@
         torch_dtype: Optional[torch.dtype],
         child_type: Optional[SubModelType] = None,
     ) -> Any:
         raise NotImplementedError()
 
 
 class DiffusersModel(ModelBase):
-    #child_types: Dict[str, Type]
-    #child_sizes: Dict[str, int]
+    # child_types: Dict[str, Type]
+    # child_sizes: Dict[str, int]
 
     def __init__(self, model_path: str, base_model: BaseModelType, model_type: ModelType):
         super().__init__(model_path, base_model, model_type)
 
         self.child_types: Dict[str, Type] = dict()
         self.child_sizes: Dict[str, int] = dict()
 
         try:
             config_data = DiffusionPipeline.load_config(self.model_path)
-            #config_data = json.loads(os.path.join(self.model_path, "model_index.json"))
+            # config_data = json.loads(os.path.join(self.model_path, "model_index.json"))
         except:
             raise Exception("Invalid diffusers model! (model_index.json not found or invalid)")
 
         config_data.pop("_ignore_files", None)
 
         # retrieve all folder_names that contain relevant files
         child_components = [k for k, v in config_data.items() if isinstance(v, list)]
 
         for child_name in child_components:
             child_type = self._hf_definition_to_type(config_data[child_name])
             self.child_types[child_name] = child_type
             self.child_sizes[child_name] = calc_model_size_by_fs(self.model_path, subfolder=child_name)
 
-
     def get_size(self, child_type: Optional[SubModelType] = None):
         if child_type is None:
             return sum(self.child_sizes.values())
         else:
             return self.child_sizes[child_type]
 
-
     def get_model(
         self,
         torch_dtype: Optional[torch.dtype],
         child_type: Optional[SubModelType] = None,
     ):
         # return pipeline in different function to pass more arguments
         if child_type is None:
             raise Exception("Child model type can't be null on diffusers model")
         if child_type not in self.child_types:
-            return None # TODO: or raise
+            return None  # TODO: or raise
 
         if torch_dtype == torch.float16:
             variants = ["fp16", None]
         else:
             variants = [None, "fp16"]
 
         # TODO: better error handling(differentiate not found from others)
@@ -261,33 +275,28 @@
                     subfolder=child_type.value,
                     torch_dtype=torch_dtype,
                     variant=variant,
                     local_files_only=True,
                 )
                 break
             except Exception as e:
-                #print("====ERR LOAD====")
-                #print(f"{variant}: {e}")
+                # print("====ERR LOAD====")
+                # print(f"{variant}: {e}")
                 pass
         else:
             raise Exception(f"Failed to load {self.base_model}:{self.model_type}:{child_type} model")
 
         # calc more accurate size
         self.child_sizes[child_type] = calc_model_size_by_data(model)
         return model
 
-    #def convert_if_required(model_path: str, cache_path: str, config: Optional[dict]) -> str:
+    # def convert_if_required(model_path: str, cache_path: str, config: Optional[dict]) -> str:
 
 
-
-def calc_model_size_by_fs(
-    model_path: str,
-    subfolder: Optional[str] = None,
-    variant: Optional[str] = None
-):
+def calc_model_size_by_fs(model_path: str, subfolder: Optional[str] = None, variant: Optional[str] = None):
     if subfolder is not None:
         model_path = os.path.join(model_path, subfolder)
 
     # this can happen when, for example, the safety checker
     # is not downloaded.
     if not os.path.exists(model_path):
         return 0
@@ -321,35 +330,35 @@
                 index_data = json.loads(f.read())
             return int(index_data["metadata"]["total_size"])
         except:
             pass
 
     # calculate files size if there is no index file
     formats = [
-        (".safetensors",), # safetensors
-        (".bin",), # torch
-        (".onnx", ".pb"), # onnx
-        (".msgpack",), # flax
-        (".ckpt",), # tf
-        (".h5",), # tf2
+        (".safetensors",),  # safetensors
+        (".bin",),  # torch
+        (".onnx", ".pb"),  # onnx
+        (".msgpack",),  # flax
+        (".ckpt",),  # tf
+        (".h5",),  # tf2
     ]
 
     for file_format in formats:
         model_files = [f for f in files if f.endswith(file_format)]
         if len(model_files) == 0:
             continue
 
         model_size = 0
         for model_file in model_files:
             file_stats = os.stat(os.path.join(model_path, model_file))
             model_size += file_stats.st_size
         return model_size
-    
-    #raise NotImplementedError(f"Unknown model structure! Files: {all_files}")
-    return 0 # scheduler/feature_extractor/tokenizer - models without loading to gpu
+
+    # raise NotImplementedError(f"Unknown model structure! Files: {all_files}")
+    return 0  # scheduler/feature_extractor/tokenizer - models without loading to gpu
 
 
 def calc_model_size_by_data(model) -> int:
     if isinstance(model, DiffusionPipeline):
         return _calc_pipeline_by_data(model)
     elif isinstance(model, torch.nn.Module):
         return _calc_model_by_data(model)
@@ -360,32 +369,36 @@
 def _calc_pipeline_by_data(pipeline) -> int:
     res = 0
     for submodel_key in pipeline.components.keys():
         submodel = getattr(pipeline, submodel_key)
         if submodel is not None and isinstance(submodel, torch.nn.Module):
             res += _calc_model_by_data(submodel)
     return res
-    
+
 
 def _calc_model_by_data(model) -> int:
-    mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])
-    mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])
-    mem = mem_params + mem_bufs # in bytes
+    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])
+    mem_bufs = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])
+    mem = mem_params + mem_bufs  # in bytes
     return mem
 
 
 def _fast_safetensors_reader(path: str):
     checkpoint = dict()
     device = torch.device("meta")
     with open(path, "rb") as f:
-        definition_len = int.from_bytes(f.read(8), 'little')
+        definition_len = int.from_bytes(f.read(8), "little")
         definition_json = f.read(definition_len)
         definition = json.loads(definition_json)
 
-        if "__metadata__" in definition and definition["__metadata__"].get("format", "pt") not in {"pt", "torch", "pytorch"}:
+        if "__metadata__" in definition and definition["__metadata__"].get("format", "pt") not in {
+            "pt",
+            "torch",
+            "pytorch",
+        }:
             raise Exception("Supported only pytorch safetensors files")
         definition.pop("__metadata__", None)
 
         for key, info in definition.items():
             dtype = {
                 "I8": torch.int8,
                 "I16": torch.int16,
@@ -396,40 +409,43 @@
                 "F64": torch.float64,
             }[info["dtype"]]
 
             checkpoint[key] = torch.empty(info["shape"], dtype=dtype, device=device)
 
     return checkpoint
 
+
 def read_checkpoint_meta(path: Union[str, Path], scan: bool = False):
     if str(path).endswith(".safetensors"):
         try:
             checkpoint = _fast_safetensors_reader(path)
         except:
             # TODO: create issue for support "meta"?
             checkpoint = safetensors.torch.load_file(path, device="cpu")
     else:
         if scan:
             scan_result = scan_file_path(path)
             if scan_result.infected_files != 0:
-                raise Exception(f"The model file \"{path}\" is potentially infected by malware. Aborting import.")
+                raise Exception(f'The model file "{path}" is potentially infected by malware. Aborting import.')
         checkpoint = torch.load(path, map_location=torch.device("meta"))
     return checkpoint
 
+
 import warnings
 from diffusers import logging as diffusers_logging
 from transformers import logging as transformers_logging
 
+
 class SilenceWarnings(object):
     def __init__(self):
         self.transformers_verbosity = transformers_logging.get_verbosity()
         self.diffusers_verbosity = diffusers_logging.get_verbosity()
-        
+
     def __enter__(self):
         transformers_logging.set_verbosity_error()
         diffusers_logging.set_verbosity_error()
-        warnings.simplefilter('ignore')
+        warnings.simplefilter("ignore")
 
     def __exit__(self, type, value, traceback):
         transformers_logging.set_verbosity(self.transformers_verbosity)
         diffusers_logging.set_verbosity(self.diffusers_verbosity)
-        warnings.simplefilter('default')
+        warnings.simplefilter("default")
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/controlnet.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/controlnet.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,36 +14,38 @@
     calc_model_size_by_data,
     classproperty,
     InvalidModelException,
     ModelNotFoundException,
 )
 from invokeai.app.services.config import InvokeAIAppConfig
 
+
 class ControlNetModelFormat(str, Enum):
     Checkpoint = "checkpoint"
     Diffusers = "diffusers"
 
+
 class ControlNetModel(ModelBase):
-    #model_class: Type
-    #model_size: int
+    # model_class: Type
+    # model_size: int
 
     class DiffusersConfig(ModelConfigBase):
         model_format: Literal[ControlNetModelFormat.Diffusers]
 
     class CheckpointConfig(ModelConfigBase):
         model_format: Literal[ControlNetModelFormat.Checkpoint]
         config: str
 
     def __init__(self, model_path: str, base_model: BaseModelType, model_type: ModelType):
         assert model_type == ModelType.ControlNet
         super().__init__(model_path, base_model, model_type)
 
         try:
             config = EmptyConfigLoader.load_config(self.model_path, config_name="config.json")
-            #config = json.loads(os.path.join(self.model_path, "config.json"))
+            # config = json.loads(os.path.join(self.model_path, "config.json"))
         except:
             raise Exception("Invalid controlnet model! (config.json not found or invalid)")
 
         model_class_name = config.get("_class_name", None)
         if model_class_name not in {"ControlNetModel"}:
             raise Exception(f"Invalid ControlNet model! Unknown _class_name: {model_class_name}")
 
@@ -63,27 +65,27 @@
         torch_dtype: Optional[torch.dtype],
         child_type: Optional[SubModelType] = None,
     ):
         if child_type is not None:
             raise Exception("There is no child models in controlnet model")
 
         model = None
-        for variant in ['fp16',None]:
+        for variant in ["fp16", None]:
             try:
                 model = self.model_class.from_pretrained(
                     self.model_path,
                     torch_dtype=torch_dtype,
                     variant=variant,
                 )
                 break
             except:
                 pass
         if not model:
             raise ModelNotFoundException()
-        
+
         # calc more accurate size
         self.model_size = calc_model_size_by_data(model)
         return model
 
     @classproperty
     def save_to_config(cls) -> bool:
         return False
@@ -101,37 +103,38 @@
             if any([path.endswith(f".{ext}") for ext in ["safetensors", "ckpt", "pt", "pth"]]):
                 return ControlNetModelFormat.Checkpoint
 
         raise InvalidModelException(f"Not a valid model: {path}")
 
     @classmethod
     def convert_if_required(
-            cls,
-            model_path: str,
-            output_path: str,
-            config: ModelConfigBase,
-            base_model: BaseModelType,
+        cls,
+        model_path: str,
+        output_path: str,
+        config: ModelConfigBase,
+        base_model: BaseModelType,
     ) -> str:
         if cls.detect_format(model_path) == ControlNetModelFormat.Checkpoint:
             return _convert_controlnet_ckpt_and_cache(
-                model_path = model_path,
-                model_config = config.config,
-                output_path = output_path,
-                base_model = base_model,
-                )
+                model_path=model_path,
+                model_config=config.config,
+                output_path=output_path,
+                base_model=base_model,
+            )
         else:
             return model_path
 
+
 @classmethod
 def _convert_controlnet_ckpt_and_cache(
-        cls,
-        model_path: str,
-        output_path: str,
-        base_model: BaseModelType,
-        model_config: ControlNetModel.CheckpointConfig,
+    cls,
+    model_path: str,
+    output_path: str,
+    base_model: BaseModelType,
+    model_config: ControlNetModel.CheckpointConfig,
 ) -> str:
     """
     Convert the controlnet from checkpoint format to diffusers format,
     cache it to disk, and return Path to converted
     file. If already on disk then just returns Path.
     """
     app_config = InvokeAIAppConfig.get_config()
@@ -140,16 +143,17 @@
 
     # return cached version if it exists
     if output_path.exists():
         return output_path
 
     # to avoid circular import errors
     from ..convert_ckpt_to_diffusers import convert_controlnet_to_diffusers
+
     convert_controlnet_to_diffusers(
         weights,
         output_path,
-        original_config_file = app_config.root_path / model_config,
-        image_size = 512,
-        scan_needed = True,
-        from_safetensors = weights.suffix == ".safetensors"
+        original_config_file=app_config.root_path / model_config,
+        image_size=512,
+        scan_needed=True,
+        from_safetensors=weights.suffix == ".safetensors",
     )
     return output_path
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/lora.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/lora.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,26 +8,29 @@
     BaseModelType,
     ModelType,
     SubModelType,
     classproperty,
     InvalidModelException,
     ModelNotFoundException,
 )
+
 # TODO: naming
 from ..lora import LoRAModel as LoRAModelRaw
 
+
 class LoRAModelFormat(str, Enum):
     LyCORIS = "lycoris"
     Diffusers = "diffusers"
 
+
 class LoRAModel(ModelBase):
-    #model_size: int
+    # model_size: int
 
     class Config(ModelConfigBase):
-        model_format: LoRAModelFormat # TODO:
+        model_format: LoRAModelFormat  # TODO:
 
     def __init__(self, model_path: str, base_model: BaseModelType, model_type: ModelType):
         assert model_type == ModelType.Lora
         super().__init__(model_path, base_model, model_type)
 
         self.model_size = os.path.getsize(self.model_path)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/sdxl.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/sdxl.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,20 +11,21 @@
     ModelVariantType,
     DiffusersModel,
     read_checkpoint_meta,
     classproperty,
 )
 from omegaconf import OmegaConf
 
+
 class StableDiffusionXLModelFormat(str, Enum):
     Checkpoint = "checkpoint"
     Diffusers = "diffusers"
-    
-class StableDiffusionXLModel(DiffusersModel):
 
+
+class StableDiffusionXLModel(DiffusersModel):
     # TODO: check that configs overwriten properly
     class DiffusersConfig(ModelConfigBase):
         model_format: Literal[StableDiffusionXLModelFormat.Diffusers]
         vae: Optional[str] = Field(None)
         variant: ModelVariantType
 
     class CheckpointConfig(ModelConfigBase):
@@ -49,23 +50,23 @@
         if model_format == StableDiffusionXLModelFormat.Checkpoint:
             if ckpt_config_path:
                 ckpt_config = OmegaConf.load(ckpt_config_path)
                 in_channels = ckpt_config["model"]["params"]["unet_config"]["params"]["in_channels"]
 
             else:
                 checkpoint = read_checkpoint_meta(path)
-                checkpoint = checkpoint.get('state_dict', checkpoint)
+                checkpoint = checkpoint.get("state_dict", checkpoint)
                 in_channels = checkpoint["model.diffusion_model.input_blocks.0.0.weight"].shape[1]
 
         elif model_format == StableDiffusionXLModelFormat.Diffusers:
             unet_config_path = os.path.join(path, "unet", "config.json")
             if os.path.exists(unet_config_path):
                 with open(unet_config_path, "r") as f:
                     unet_config = json.loads(f.read())
-                in_channels = unet_config['in_channels']
+                in_channels = unet_config["in_channels"]
 
             else:
                 raise Exception("Not supported stable diffusion diffusers format(possibly onnx?)")
 
         else:
             raise NotImplementedError(f"Unknown stable diffusion 2.* format: {model_format}")
 
@@ -77,19 +78,18 @@
             variant = ModelVariantType.Normal
         else:
             raise Exception("Unkown stable diffusion 2.* model format")
 
         if ckpt_config_path is None:
             # TO DO: implement picking
             pass
-        
+
         return cls.create_config(
             path=path,
             model_format=model_format,
-
             config=ckpt_config_path,
             variant=variant,
         )
 
     @classproperty
     def save_to_config(cls) -> bool:
         return True
@@ -110,15 +110,16 @@
         base_model: BaseModelType,
     ) -> str:
         # The convert script adapted from the diffusers package uses
         # strings for the base model type. To avoid making too many
         # source code changes, we simply translate here
         if isinstance(config, cls.CheckpointConfig):
             from invokeai.backend.model_management.models.stable_diffusion import _convert_ckpt_and_cache
+
             return _convert_ckpt_and_cache(
                 version=base_model,
                 model_config=config,
                 output_path=output_path,
-                use_safetensors=False, # corrupts sdxl models for some reason
+                use_safetensors=False,  # corrupts sdxl models for some reason
             )
         else:
             return model_path
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/stable_diffusion.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/stable_diffusion.py`

 * *Files 2% similar despite different names*

```diff
@@ -22,27 +22,27 @@
 from omegaconf import OmegaConf
 
 
 class StableDiffusion1ModelFormat(str, Enum):
     Checkpoint = "checkpoint"
     Diffusers = "diffusers"
 
-class StableDiffusion1Model(DiffusersModel):
 
+class StableDiffusion1Model(DiffusersModel):
     class DiffusersConfig(ModelConfigBase):
         model_format: Literal[StableDiffusion1ModelFormat.Diffusers]
         vae: Optional[str] = Field(None)
         variant: ModelVariantType
 
     class CheckpointConfig(ModelConfigBase):
         model_format: Literal[StableDiffusion1ModelFormat.Checkpoint]
         vae: Optional[str] = Field(None)
         config: str
         variant: ModelVariantType
-        
+
     def __init__(self, model_path: str, base_model: BaseModelType, model_type: ModelType):
         assert base_model == BaseModelType.StableDiffusion1
         assert model_type == ModelType.Main
         super().__init__(
             model_path=model_path,
             base_model=BaseModelType.StableDiffusion1,
             model_type=ModelType.Main,
@@ -55,23 +55,23 @@
         if model_format == StableDiffusion1ModelFormat.Checkpoint:
             if ckpt_config_path:
                 ckpt_config = OmegaConf.load(ckpt_config_path)
                 ckpt_config["model"]["params"]["unet_config"]["params"]["in_channels"]
 
             else:
                 checkpoint = read_checkpoint_meta(path)
-                checkpoint = checkpoint.get('state_dict', checkpoint)
+                checkpoint = checkpoint.get("state_dict", checkpoint)
                 in_channels = checkpoint["model.diffusion_model.input_blocks.0.0.weight"].shape[1]
 
         elif model_format == StableDiffusion1ModelFormat.Diffusers:
             unet_config_path = os.path.join(path, "unet", "config.json")
             if os.path.exists(unet_config_path):
                 with open(unet_config_path, "r") as f:
                     unet_config = json.loads(f.read())
-                in_channels = unet_config['in_channels']
+                in_channels = unet_config["in_channels"]
 
             else:
                 raise NotImplementedError(f"{path} is not a supported stable diffusion diffusers format")
 
         else:
             raise NotImplementedError(f"Unknown stable diffusion 1.* format: {model_format}")
 
@@ -84,15 +84,14 @@
 
         if ckpt_config_path is None:
             ckpt_config_path = _select_ckpt_config(BaseModelType.StableDiffusion1, variant)
 
         return cls.create_config(
             path=path,
             model_format=model_format,
-
             config=ckpt_config_path,
             variant=variant,
         )
 
     @classproperty
     def save_to_config(cls) -> bool:
         return True
@@ -121,24 +120,25 @@
         base_model: BaseModelType,
     ) -> str:
         if isinstance(config, cls.CheckpointConfig):
             return _convert_ckpt_and_cache(
                 version=BaseModelType.StableDiffusion1,
                 model_config=config,
                 output_path=output_path,
-           )
+            )
         else:
             return model_path
 
+
 class StableDiffusion2ModelFormat(str, Enum):
     Checkpoint = "checkpoint"
     Diffusers = "diffusers"
 
-class StableDiffusion2Model(DiffusersModel):
 
+class StableDiffusion2Model(DiffusersModel):
     # TODO: check that configs overwriten properly
     class DiffusersConfig(ModelConfigBase):
         model_format: Literal[StableDiffusion2ModelFormat.Diffusers]
         vae: Optional[str] = Field(None)
         variant: ModelVariantType
 
     class CheckpointConfig(ModelConfigBase):
@@ -163,23 +163,23 @@
         if model_format == StableDiffusion2ModelFormat.Checkpoint:
             if ckpt_config_path:
                 ckpt_config = OmegaConf.load(ckpt_config_path)
                 ckpt_config["model"]["params"]["unet_config"]["params"]["in_channels"]
 
             else:
                 checkpoint = read_checkpoint_meta(path)
-                checkpoint = checkpoint.get('state_dict', checkpoint)
+                checkpoint = checkpoint.get("state_dict", checkpoint)
                 in_channels = checkpoint["model.diffusion_model.input_blocks.0.0.weight"].shape[1]
 
         elif model_format == StableDiffusion2ModelFormat.Diffusers:
             unet_config_path = os.path.join(path, "unet", "config.json")
             if os.path.exists(unet_config_path):
                 with open(unet_config_path, "r") as f:
                     unet_config = json.loads(f.read())
-                in_channels = unet_config['in_channels']
+                in_channels = unet_config["in_channels"]
 
             else:
                 raise Exception("Not supported stable diffusion diffusers format(possibly onnx?)")
 
         else:
             raise NotImplementedError(f"Unknown stable diffusion 2.* format: {model_format}")
 
@@ -194,15 +194,14 @@
 
         if ckpt_config_path is None:
             ckpt_config_path = _select_ckpt_config(BaseModelType.StableDiffusion2, variant)
 
         return cls.create_config(
             path=path,
             model_format=model_format,
-
             config=ckpt_config_path,
             variant=variant,
         )
 
     @classproperty
     def save_to_config(cls) -> bool:
         return True
@@ -235,25 +234,27 @@
                 version=BaseModelType.StableDiffusion2,
                 model_config=config,
                 output_path=output_path,
             )
         else:
             return model_path
 
+
 # TODO: rework
 # pass precision - currently defaulting to fp16
 def _convert_ckpt_and_cache(
-        version: BaseModelType,
-        model_config: Union[StableDiffusion1Model.CheckpointConfig,
-                            StableDiffusion2Model.CheckpointConfig,
-                            StableDiffusionXLModel.CheckpointConfig,
-                            ],
-        output_path: str,
-        use_save_model: bool=False,
-        **kwargs,
+    version: BaseModelType,
+    model_config: Union[
+        StableDiffusion1Model.CheckpointConfig,
+        StableDiffusion2Model.CheckpointConfig,
+        StableDiffusionXLModel.CheckpointConfig,
+    ],
+    output_path: str,
+    use_save_model: bool = False,
+    **kwargs,
 ) -> str:
     """
     Convert the checkpoint model indicated in mconfig into a
     diffusers, cache it to disk, and return Path to converted
     file. If already on disk then just returns Path.
     """
     app_config = InvokeAIAppConfig.get_config()
@@ -266,44 +267,46 @@
     if output_path.exists():
         return output_path
 
     # to avoid circular import errors
     from ..convert_ckpt_to_diffusers import convert_ckpt_to_diffusers
     from ...util.devices import choose_torch_device, torch_dtype
 
-    model_base_to_model_type = {BaseModelType.StableDiffusion1: 'FrozenCLIPEmbedder',
-                                BaseModelType.StableDiffusion2: 'FrozenOpenCLIPEmbedder',
-                                BaseModelType.StableDiffusionXL: 'SDXL',
-                                BaseModelType.StableDiffusionXLRefiner: 'SDXL-Refiner',
-                                }
-    logger.info(f'Converting {weights} to diffusers format')
-    with SilenceWarnings():        
+    model_base_to_model_type = {
+        BaseModelType.StableDiffusion1: "FrozenCLIPEmbedder",
+        BaseModelType.StableDiffusion2: "FrozenOpenCLIPEmbedder",
+        BaseModelType.StableDiffusionXL: "SDXL",
+        BaseModelType.StableDiffusionXLRefiner: "SDXL-Refiner",
+    }
+    logger.info(f"Converting {weights} to diffusers format")
+    with SilenceWarnings():
         convert_ckpt_to_diffusers(
             weights,
             output_path,
             model_type=model_base_to_model_type[version],
             model_version=version,
             model_variant=model_config.variant,
             original_config_file=config_file,
             extract_ema=True,
             scan_needed=True,
-            from_safetensors = weights.suffix == ".safetensors",
-            precision = torch_dtype(choose_torch_device()),
+            from_safetensors=weights.suffix == ".safetensors",
+            precision=torch_dtype(choose_torch_device()),
             **kwargs,
         )
     return output_path
 
+
 def _select_ckpt_config(version: BaseModelType, variant: ModelVariantType):
     ckpt_configs = {
         BaseModelType.StableDiffusion1: {
             ModelVariantType.Normal: "v1-inference.yaml",
             ModelVariantType.Inpaint: "v1-inpainting-inference.yaml",
         },
         BaseModelType.StableDiffusion2: {
-            ModelVariantType.Normal: "v2-inference-v.yaml", # best guess, as we can't differentiate with base(512)
+            ModelVariantType.Normal: "v2-inference-v.yaml",  # best guess, as we can't differentiate with base(512)
             ModelVariantType.Inpaint: "v2-inpainting-inference.yaml",
             ModelVariantType.Depth: "v2-midas-inference.yaml",
         },
         BaseModelType.StableDiffusionXL: {
             ModelVariantType.Normal: "sd_xl_base.yaml",
             ModelVariantType.Inpaint: None,
             ModelVariantType.Depth: None,
@@ -317,12 +320,10 @@
 
     app_config = InvokeAIAppConfig.get_config()
     try:
         config_path = app_config.legacy_conf_path / ckpt_configs[version][variant]
         if config_path.is_relative_to(app_config.root_path):
             config_path = config_path.relative_to(app_config.root_path)
         return str(config_path)
-            
+
     except:
         return None
-
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/textual_inversion.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/textual_inversion.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,19 +7,21 @@
     BaseModelType,
     ModelType,
     SubModelType,
     classproperty,
     ModelNotFoundException,
     InvalidModelException,
 )
+
 # TODO: naming
 from ..lora import TextualInversionModel as TextualInversionModelRaw
 
+
 class TextualInversionModel(ModelBase):
-    #model_size: int
+    # model_size: int
 
     class Config(ModelConfigBase):
         model_format: None
 
     def __init__(self, model_path: str, base_model: BaseModelType, model_type: ModelType):
         assert model_type == ModelType.TextualInversion
         super().__init__(model_path, base_model, model_type)
@@ -61,15 +63,15 @@
     @classmethod
     def detect_format(cls, path: str):
         if not os.path.exists(path):
             raise ModelNotFoundException()
 
         if os.path.isdir(path):
             if os.path.exists(os.path.join(path, "learned_embeds.bin")):
-                return None # diffusers-ti
+                return None  # diffusers-ti
 
         if os.path.isfile(path):
             if any([path.endswith(f".{ext}") for ext in ["safetensors", "ckpt", "pt", "bin"]]):
                 return None
 
         raise InvalidModelException(f"Not a valid model: {path}")
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/model_management/models/vae.py` & `InvokeAI-3.0.1rc2/invokeai/backend/model_management/models/vae.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,32 +18,34 @@
     InvalidModelException,
     ModelNotFoundException,
 )
 from invokeai.app.services.config import InvokeAIAppConfig
 from diffusers.utils import is_safetensors_available
 from omegaconf import OmegaConf
 
+
 class VaeModelFormat(str, Enum):
     Checkpoint = "checkpoint"
     Diffusers = "diffusers"
 
+
 class VaeModel(ModelBase):
-    #vae_class: Type
-    #model_size: int
+    # vae_class: Type
+    # model_size: int
 
     class Config(ModelConfigBase):
         model_format: VaeModelFormat
 
     def __init__(self, model_path: str, base_model: BaseModelType, model_type: ModelType):
         assert model_type == ModelType.Vae
         super().__init__(model_path, base_model, model_type)
 
         try:
             config = EmptyConfigLoader.load_config(self.model_path, config_name="config.json")
-            #config = json.loads(os.path.join(self.model_path, "config.json"))
+            # config = json.loads(os.path.join(self.model_path, "config.json"))
         except:
             raise Exception("Invalid vae model! (config.json not found or invalid)")
 
         try:
             vae_class_name = config.get("_class_name", "AutoencoderKL")
             self.vae_class = self._hf_definition_to_type(["diffusers", vae_class_name])
             self.model_size = calc_model_size_by_fs(self.model_path)
@@ -91,27 +93,28 @@
         raise InvalidModelException(f"Not a valid model: {path}")
 
     @classmethod
     def convert_if_required(
         cls,
         model_path: str,
         output_path: str,
-        config: ModelConfigBase, # empty config or config of parent model
+        config: ModelConfigBase,  # empty config or config of parent model
         base_model: BaseModelType,
     ) -> str:
         if cls.detect_format(model_path) == VaeModelFormat.Checkpoint:
             return _convert_vae_ckpt_and_cache(
                 weights_path=model_path,
                 output_path=output_path,
                 base_model=base_model,
                 model_config=config,
             )
         else:
             return model_path
 
+
 # TODO: rework
 def _convert_vae_ckpt_and_cache(
     weights_path: str,
     output_path: str,
     base_model: BaseModelType,
     model_config: ModelConfigBase,
 ) -> str:
@@ -134,42 +137,41 @@
     2-depth - 256
     2-base - 512
     2 - 768
     2.1-base - 768
     2.1 - 768
     """
     image_size = 512
-        
+
     # return cached version if it exists
     if output_path.exists():
         return output_path
 
     if base_model in {BaseModelType.StableDiffusion1, BaseModelType.StableDiffusion2}:
         from .stable_diffusion import _select_ckpt_config
+
         # all sd models use same vae settings
         config_file = _select_ckpt_config(base_model, ModelVariantType.Normal)
     else:
         raise Exception(f"Vae conversion not supported for model type: {base_model}")
 
     # this avoids circular import error
     from ..convert_ckpt_to_diffusers import convert_ldm_vae_to_diffusers
-    if weights_path.suffix == '.safetensors':
+
+    if weights_path.suffix == ".safetensors":
         checkpoint = safetensors.torch.load_file(weights_path, device="cpu")
     else:
         checkpoint = torch.load(weights_path, map_location="cpu")
 
     # sometimes weights are hidden under "state_dict", and sometimes not
     if "state_dict" in checkpoint:
         checkpoint = checkpoint["state_dict"]
 
-    config = OmegaConf.load(app_config.root_path/config_file)
+    config = OmegaConf.load(app_config.root_path / config_file)
 
     vae_model = convert_ldm_vae_to_diffusers(
-        checkpoint = checkpoint,
-        vae_config = config,
-        image_size = image_size,
-    )
-    vae_model.save_pretrained(
-        output_path,
-        safe_serialization=is_safetensors_available()
+        checkpoint=checkpoint,
+        vae_config=config,
+        image_size=image_size,
     )
+    vae_model.save_pretrained(output_path, safe_serialization=is_safetensors_available())
     return output_path
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusers_pipeline.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusers_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -43,14 +43,15 @@
 from .diffusion import (
     AttentionMapSaver,
     InvokeAIDiffuserComponent,
     PostprocessingSettings,
 )
 from .offloading import FullyLoadedModelGroup, ModelGroup
 
+
 @dataclass
 class PipelineIntermediateState:
     run_id: str
     step: int
     timestep: int
     latents: torch.Tensor
     predicted_original: Optional[torch.Tensor] = None
@@ -68,28 +69,28 @@
     """
 
     forward: Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]
     mask: torch.Tensor
     initial_image_latents: torch.Tensor
 
     def __call__(
-        self, latents: torch.Tensor, t: torch.Tensor, text_embeddings: torch.Tensor, **kwargs,
+        self,
+        latents: torch.Tensor,
+        t: torch.Tensor,
+        text_embeddings: torch.Tensor,
+        **kwargs,
     ) -> torch.Tensor:
         model_input = self.add_mask_channels(latents)
         return self.forward(model_input, t, text_embeddings, **kwargs)
 
     def add_mask_channels(self, latents):
         batch_size = latents.size(0)
         # duplicate mask and latents for each batch
-        mask = einops.repeat(
-            self.mask, "b c h w -> (repeat b) c h w", repeat=batch_size
-        )
-        image_latents = einops.repeat(
-            self.initial_image_latents, "b c h w -> (repeat b) c h w", repeat=batch_size
-        )
+        mask = einops.repeat(self.mask, "b c h w -> (repeat b) c h w", repeat=batch_size)
+        image_latents = einops.repeat(self.initial_image_latents, "b c h w -> (repeat b) c h w", repeat=batch_size)
         # add mask and image as additional channels
         model_input, _ = einops.pack([latents, mask, image_latents], "b * h w")
         return model_input
 
 
 def are_like_tensors(a: torch.Tensor, b: object) -> bool:
     return isinstance(b, torch.Tensor) and (a.size() == b.size())
@@ -99,73 +100,59 @@
 class AddsMaskGuidance:
     mask: torch.FloatTensor
     mask_latents: torch.FloatTensor
     scheduler: SchedulerMixin
     noise: torch.Tensor
     _debug: Optional[Callable] = None
 
-    def __call__(
-        self, step_output: Union[BaseOutput, SchedulerOutput], t: torch.Tensor, conditioning
-    ) -> BaseOutput:
+    def __call__(self, step_output: Union[BaseOutput, SchedulerOutput], t: torch.Tensor, conditioning) -> BaseOutput:
         output_class = step_output.__class__  # We'll create a new one with masked data.
 
         # The problem with taking SchedulerOutput instead of the model output is that we're less certain what's in it.
         # It's reasonable to assume the first thing is prev_sample, but then does it have other things
         # like pred_original_sample? Should we apply the mask to them too?
         # But what if there's just some other random field?
         prev_sample = step_output[0]
         # Mask anything that has the same shape as prev_sample, return others as-is.
         return output_class(
             {
-                k: (
-                    self.apply_mask(v, self._t_for_field(k, t))
-                    if are_like_tensors(prev_sample, v)
-                    else v
-                )
+                k: (self.apply_mask(v, self._t_for_field(k, t)) if are_like_tensors(prev_sample, v) else v)
                 for k, v in step_output.items()
             }
         )
 
     def _t_for_field(self, field_name: str, t):
         if field_name == "pred_original_sample":
             return self.scheduler.timesteps[-1]
         return t
 
     def apply_mask(self, latents: torch.Tensor, t) -> torch.Tensor:
         batch_size = latents.size(0)
-        mask = einops.repeat(
-            self.mask, "b c h w -> (repeat b) c h w", repeat=batch_size
-        )
+        mask = einops.repeat(self.mask, "b c h w -> (repeat b) c h w", repeat=batch_size)
         if t.dim() == 0:
             # some schedulers expect t to be one-dimensional.
             # TODO: file diffusers bug about inconsistency?
             t = einops.repeat(t, "-> batch", batch=batch_size)
         # Noise shouldn't be re-randomized between steps here. The multistep schedulers
         # get very confused about what is happening from step to step when we do that.
         mask_latents = self.scheduler.add_noise(self.mask_latents, self.noise, t)
         # TODO: Do we need to also apply scheduler.scale_model_input? Or is add_noise appropriately scaled already?
         # mask_latents = self.scheduler.scale_model_input(mask_latents, t)
-        mask_latents = einops.repeat(
-            mask_latents, "b c h w -> (repeat b) c h w", repeat=batch_size
-        )
-        masked_input = torch.lerp(
-            mask_latents.to(dtype=latents.dtype), latents, mask.to(dtype=latents.dtype)
-        )
+        mask_latents = einops.repeat(mask_latents, "b c h w -> (repeat b) c h w", repeat=batch_size)
+        masked_input = torch.lerp(mask_latents.to(dtype=latents.dtype), latents, mask.to(dtype=latents.dtype))
         if self._debug:
             self._debug(masked_input, f"t={t} lerped")
         return masked_input
 
 
 def trim_to_multiple_of(*args, multiple_of=8):
     return tuple((x - x % multiple_of) for x in args)
 
 
-def image_resized_to_grid_as_tensor(
-    image: PIL.Image.Image, normalize: bool = True, multiple_of=8
-) -> torch.FloatTensor:
+def image_resized_to_grid_as_tensor(image: PIL.Image.Image, normalize: bool = True, multiple_of=8) -> torch.FloatTensor:
     """
 
     :param image: input image
     :param normalize: scale the range to [-1, 1] instead of [0, 1]
     :param multiple_of: resize the input so both dimensions are a multiple of this
     """
     w, h = trim_to_multiple_of(*image.size, multiple_of=multiple_of)
@@ -207,14 +194,15 @@
         for result in self.generator_method(*args, **kwargs):
             if callback is not None and isinstance(result, self.callback_arg_type):
                 callback(result)
         if result is None:
             raise AssertionError("why was that an empty generator?")
         return result
 
+
 @dataclass
 class ControlNetData:
     model: ControlNetModel = Field(default=None)
     image_tensor: torch.Tensor = Field(default=None)
     weight: Union[float, List[float]] = Field(default=1.0)
     begin_step_percent: float = Field(default=0.0)
     end_step_percent: float = Field(default=1.0)
@@ -337,56 +325,46 @@
             unet=unet,
             scheduler=scheduler,
             safety_checker=safety_checker,
             feature_extractor=feature_extractor,
             # FIXME: can't currently register control module
             # control_model=control_model,
         )
-        self.invokeai_diffuser = InvokeAIDiffuserComponent(
-            self.unet, self._unet_forward
-        )
+        self.invokeai_diffuser = InvokeAIDiffuserComponent(self.unet, self._unet_forward)
 
         self._model_group = FullyLoadedModelGroup(execution_device or self.unet.device)
         self._model_group.install(*self._submodels)
         self.control_model = control_model
 
     def _adjust_memory_efficient_attention(self, latents: torch.Tensor):
         """
         if xformers is available, use it, otherwise use sliced attention.
         """
         config = InvokeAIAppConfig.get_config()
-        if (
-            torch.cuda.is_available()
-            and is_xformers_available()
-            and not config.disable_xformers
-        ):
+        if torch.cuda.is_available() and is_xformers_available() and not config.disable_xformers:
             self.enable_xformers_memory_efficient_attention()
         else:
             if self.device.type == "cpu" or self.device.type == "mps":
                 mem_free = psutil.virtual_memory().free
             elif self.device.type == "cuda":
                 mem_free, _ = torch.cuda.mem_get_info(normalize_device(self.device))
             else:
                 raise ValueError(f"unrecognized device {self.device}")
             # input tensor of [1, 4, h/8, w/8]
             # output tensor of [16, (h/8 * w/8), (h/8 * w/8)]
-            bytes_per_element_needed_for_baddbmm_duplication = (
-                latents.element_size() + 4
-            )
+            bytes_per_element_needed_for_baddbmm_duplication = latents.element_size() + 4
             max_size_required_for_baddbmm = (
                 16
                 * latents.size(dim=2)
                 * latents.size(dim=3)
                 * latents.size(dim=2)
                 * latents.size(dim=3)
                 * bytes_per_element_needed_for_baddbmm_duplication
             )
-            if max_size_required_for_baddbmm > (
-                mem_free * 3.0 / 4.0
-            ):  # 3.3 / 4.0 is from old Invoke code
+            if max_size_required_for_baddbmm > (mem_free * 3.0 / 4.0):  # 3.3 / 4.0 is from old Invoke code
                 self.enable_attention_slicing(slice_size="max")
             elif torch.backends.mps.is_available():
                 # diffusers recommends always enabling for mps
                 self.enable_attention_slicing(slice_size="max")
             else:
                 self.disable_attention_slicing()
 
@@ -466,15 +444,15 @@
         timesteps=None,
         additional_guidance: List[Callable] = None,
         run_id=None,
         callback: Callable[[PipelineIntermediateState], None] = None,
         control_data: List[ControlNetData] = None,
     ) -> tuple[torch.Tensor, Optional[AttentionMapSaver]]:
         if self.scheduler.config.get("cpu_only", False):
-            scheduler_device = torch.device('cpu')
+            scheduler_device = torch.device("cpu")
         else:
             scheduler_device = self._model_group.device_for(self.unet)
 
         if timesteps is None:
             self.scheduler.set_timesteps(num_inference_steps, device=scheduler_device)
             timesteps = self.scheduler.timesteps
         infer_latents_from_embeddings = GeneratorToCallbackinator(
@@ -484,15 +462,14 @@
             latents,
             timesteps,
             conditioning_data,
             noise=noise,
             run_id=run_id,
             additional_guidance=additional_guidance,
             control_data=control_data,
-
             callback=callback,
         )
         return result.latents, result.attention_map_saver
 
     def generate_latents_from_embeddings(
         self,
         latents: torch.Tensor,
@@ -507,17 +484,17 @@
         self._adjust_memory_efficient_attention(latents)
         if run_id is None:
             run_id = secrets.token_urlsafe(self.ID_LENGTH)
         if additional_guidance is None:
             additional_guidance = []
         extra_conditioning_info = conditioning_data.extra
         with self.invokeai_diffuser.custom_attention_context(
-                self.invokeai_diffuser.model,
-                extra_conditioning_info=extra_conditioning_info,
-                step_count=len(self.scheduler.timesteps),
+            self.invokeai_diffuser.model,
+            extra_conditioning_info=extra_conditioning_info,
+            step_count=len(self.scheduler.timesteps),
         ):
             yield PipelineIntermediateState(
                 run_id=run_id,
                 step=-1,
                 timestep=self.scheduler.config.num_train_timesteps,
                 latents=latents,
             )
@@ -603,37 +580,39 @@
             #      and MultiControlNet (multiple ControlNetData in list)
             for i, control_datum in enumerate(control_data):
                 control_mode = control_datum.control_mode
                 # soft_injection and cfg_injection are the two ControlNet control_mode booleans
                 #     that are combined at higher level to make control_mode enum
                 #  soft_injection determines whether to do per-layer re-weighting adjustment (if True)
                 #     or default weighting (if False)
-                soft_injection = (control_mode == "more_prompt" or control_mode == "more_control")
+                soft_injection = control_mode == "more_prompt" or control_mode == "more_control"
                 #  cfg_injection = determines whether to apply ControlNet to only the conditional (if True)
                 #      or the default both conditional and unconditional (if False)
-                cfg_injection = (control_mode == "more_control" or control_mode == "unbalanced")
+                cfg_injection = control_mode == "more_control" or control_mode == "unbalanced"
 
                 first_control_step = math.floor(control_datum.begin_step_percent * total_step_count)
                 last_control_step = math.ceil(control_datum.end_step_percent * total_step_count)
                 # only apply controlnet if current step is within the controlnet's begin/end step range
                 if step_index >= first_control_step and step_index <= last_control_step:
-
                     if cfg_injection:
                         control_latent_input = unet_latent_input
                     else:
                         # expand the latents input to control model if doing classifier free guidance
                         #    (which I think for now is always true, there is conditional elsewhere that stops execution if
                         #     classifier_free_guidance is <= 1.0 ?)
                         control_latent_input = torch.cat([unet_latent_input] * 2)
 
                     if cfg_injection:  # only applying ControlNet to conditional instead of in unconditioned
                         encoder_hidden_states = conditioning_data.text_embeddings
                         encoder_attention_mask = None
                     else:
-                        encoder_hidden_states, encoder_attention_mask = self.invokeai_diffuser._concat_conditionings_for_batch(
+                        (
+                            encoder_hidden_states,
+                            encoder_attention_mask,
+                        ) = self.invokeai_diffuser._concat_conditionings_for_batch(
                             conditioning_data.unconditioned_embeddings,
                             conditioning_data.text_embeddings,
                         )
                     if isinstance(control_datum.weight, list):
                         # if controlnet has multiple weights, use the weight for the current step
                         controlnet_weight = control_datum.weight[step_index]
                     else:
@@ -642,17 +621,17 @@
 
                     # controlnet(s) inference
                     down_samples, mid_sample = control_datum.model(
                         sample=control_latent_input,
                         timestep=timestep,
                         encoder_hidden_states=encoder_hidden_states,
                         controlnet_cond=control_datum.image_tensor,
-                        conditioning_scale=controlnet_weight, # controlnet specific, NOT the guidance scale
+                        conditioning_scale=controlnet_weight,  # controlnet specific, NOT the guidance scale
                         encoder_attention_mask=encoder_attention_mask,
-                        guess_mode=soft_injection, # this is still called guess_mode in diffusers ControlNetModel
+                        guess_mode=soft_injection,  # this is still called guess_mode in diffusers ControlNetModel
                         return_dict=False,
                     )
                     if cfg_injection:
                         # Inferred ControlNet only for the conditional batch.
                         # To apply the output of ControlNet to both the unconditional and conditional batches,
                         #    prepend zeros for unconditional batch
                         down_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_samples]
@@ -674,21 +653,19 @@
             sigma=t,
             unconditioning=conditioning_data.unconditioned_embeddings,
             conditioning=conditioning_data.text_embeddings,
             unconditional_guidance_scale=conditioning_data.guidance_scale,
             step_index=step_index,
             total_step_count=total_step_count,
             down_block_additional_residuals=down_block_res_samples,  # from controlnet(s)
-            mid_block_additional_residual=mid_block_res_sample,      # from controlnet(s)
+            mid_block_additional_residual=mid_block_res_sample,  # from controlnet(s)
         )
 
         # compute the previous noisy sample x_t -> x_t-1
-        step_output = self.scheduler.step(
-            noise_pred, timestep, latents, **conditioning_data.scheduler_args
-        )
+        step_output = self.scheduler.step(noise_pred, timestep, latents, **conditioning_data.scheduler_args)
 
         # TODO: this additional_guidance extension point feels redundant with InvokeAIDiffusionComponent.
         #    But the way things are now, scheduler runs _after_ that, so there was
         #    no way to use it to apply an operation that happens after the last scheduler.step.
         for guidance in additional_guidance:
             step_output = guidance(step_output, timestep, conditioning_data)
 
@@ -706,25 +683,24 @@
         if is_inpainting_model(self.unet) and latents.size(1) == 4:
             # Pad out normal non-inpainting inputs for an inpainting model.
             # FIXME: There are too many layers of functions and we have too many different ways of
             #     overriding things! This should get handled in a way more consistent with the other
             #     use of AddsMaskLatents.
             latents = AddsMaskLatents(
                 self._unet_forward,
-                mask=torch.ones_like(
-                    latents[:1, :1], device=latents.device, dtype=latents.dtype
-                ),
-                initial_image_latents=torch.zeros_like(
-                    latents[:1], device=latents.device, dtype=latents.dtype
-                ),
+                mask=torch.ones_like(latents[:1, :1], device=latents.device, dtype=latents.dtype),
+                initial_image_latents=torch.zeros_like(latents[:1], device=latents.device, dtype=latents.dtype),
             ).add_mask_channels(latents)
 
         # First three args should be positional, not keywords, so torch hooks can see them.
         return self.unet(
-            latents, t, text_embeddings, cross_attention_kwargs=cross_attention_kwargs,
+            latents,
+            t,
+            text_embeddings,
+            cross_attention_kwargs=cross_attention_kwargs,
             **kwargs,
         ).sample
 
     def img2img_from_embeddings(
         self,
         init_image: Union[torch.FloatTensor, PIL.Image.Image],
         strength: float,
@@ -770,17 +746,17 @@
         strength,
         noise: torch.Tensor,
         run_id=None,
         callback=None,
     ) -> InvokeAIStableDiffusionPipelineOutput:
         timesteps, _ = self.get_img2img_timesteps(num_inference_steps, strength)
         result_latents, result_attention_maps = self.latents_from_embeddings(
-            latents=initial_latents if strength < 1.0 else torch.zeros_like(
-                initial_latents, device=initial_latents.device, dtype=initial_latents.dtype
-            ),
+            latents=initial_latents
+            if strength < 1.0
+            else torch.zeros_like(initial_latents, device=initial_latents.device, dtype=initial_latents.dtype),
             num_inference_steps=num_inference_steps,
             conditioning_data=conditioning_data,
             timesteps=timesteps,
             noise=noise,
             run_id=run_id,
             callback=callback,
         )
@@ -793,22 +769,20 @@
             output = InvokeAIStableDiffusionPipelineOutput(
                 images=image,
                 nsfw_content_detected=[],
                 attention_map_saver=result_attention_maps,
             )
             return self.check_for_safety(output, dtype=conditioning_data.dtype)
 
-    def get_img2img_timesteps(
-        self, num_inference_steps: int, strength: float, device=None
-    ) -> (torch.Tensor, int):
+    def get_img2img_timesteps(self, num_inference_steps: int, strength: float, device=None) -> (torch.Tensor, int):
         img2img_pipeline = StableDiffusionImg2ImgPipeline(**self.components)
         assert img2img_pipeline.scheduler is self.scheduler
 
         if self.scheduler.config.get("cpu_only", False):
-            scheduler_device = torch.device('cpu')
+            scheduler_device = torch.device("cpu")
         else:
             scheduler_device = self._model_group.device_for(self.unet)
 
         img2img_pipeline.scheduler.set_timesteps(num_inference_steps, device=scheduler_device)
         timesteps, adjusted_steps = img2img_pipeline.get_timesteps(
             num_inference_steps, strength, device=scheduler_device
         )
@@ -845,49 +819,45 @@
             init_image = init_image.unsqueeze(0)
 
         timesteps, _ = self.get_img2img_timesteps(num_inference_steps, strength)
 
         # 6. Prepare latent variables
         # can't quite use upstream StableDiffusionImg2ImgPipeline.prepare_latents
         # because we have our own noise function
-        init_image_latents = self.non_noised_latents_from_image(
-            init_image, device=device, dtype=latents_dtype
-        )
+        init_image_latents = self.non_noised_latents_from_image(init_image, device=device, dtype=latents_dtype)
         if seed is not None:
             set_seed(seed)
         noise = noise_func(init_image_latents)
 
         if mask.dim() == 3:
             mask = mask.unsqueeze(0)
-        latent_mask = tv_resize(
-            mask, init_image_latents.shape[-2:], T.InterpolationMode.BILINEAR
-        ).to(device=device, dtype=latents_dtype)
+        latent_mask = tv_resize(mask, init_image_latents.shape[-2:], T.InterpolationMode.BILINEAR).to(
+            device=device, dtype=latents_dtype
+        )
 
         guidance: List[Callable] = []
 
         if is_inpainting_model(self.unet):
             # You'd think the inpainting model wouldn't be paying attention to the area it is going to repaint
             # (that's why there's a mask!) but it seems to really want that blanked out.
             masked_init_image = init_image * torch.where(mask < 0.5, 1, 0)
-            masked_latents = self.non_noised_latents_from_image(
-                masked_init_image, device=device, dtype=latents_dtype
-            )
+            masked_latents = self.non_noised_latents_from_image(masked_init_image, device=device, dtype=latents_dtype)
 
             # TODO: we should probably pass this in so we don't have to try/finally around setting it.
             self.invokeai_diffuser.model_forward_callback = AddsMaskLatents(
                 self._unet_forward, latent_mask, masked_latents
             )
         else:
-            guidance.append(
-                AddsMaskGuidance(latent_mask, init_image_latents, self.scheduler, noise)
-            )
+            guidance.append(AddsMaskGuidance(latent_mask, init_image_latents, self.scheduler, noise))
 
         try:
             result_latents, result_attention_maps = self.latents_from_embeddings(
-                latents=init_image_latents if strength < 1.0 else torch.zeros_like(
+                latents=init_image_latents
+                if strength < 1.0
+                else torch.zeros_like(
                     init_image_latents, device=init_image_latents.device, dtype=init_image_latents.dtype
                 ),
                 num_inference_steps=num_inference_steps,
                 conditioning_data=conditioning_data,
                 noise=noise,
                 timesteps=timesteps,
                 additional_guidance=guidance,
@@ -910,26 +880,22 @@
             return self.check_for_safety(output, dtype=conditioning_data.dtype)
 
     def non_noised_latents_from_image(self, init_image, *, device: torch.device, dtype):
         init_image = init_image.to(device=device, dtype=dtype)
         with torch.inference_mode():
             self._model_group.load(self.vae)
             init_latent_dist = self.vae.encode(init_image).latent_dist
-            init_latents = init_latent_dist.sample().to(
-                dtype=dtype
-            )  # FIXME: uses torch.randn. make reproducible!
+            init_latents = init_latent_dist.sample().to(dtype=dtype)  # FIXME: uses torch.randn. make reproducible!
 
         init_latents = 0.18215 * init_latents
         return init_latents
 
     def check_for_safety(self, output, dtype):
         with torch.inference_mode():
-            screened_images, has_nsfw_concept = self.run_safety_checker(
-                output.images, dtype=dtype
-            )
+            screened_images, has_nsfw_concept = self.run_safety_checker(output.images, dtype=dtype)
         screened_attention_map_saver = None
         if has_nsfw_concept is None or not has_nsfw_concept:
             screened_attention_map_saver = output.attention_map_saver
         return InvokeAIStableDiffusionPipelineOutput(
             screened_images,
             has_nsfw_concept,
             # block the attention maps if NSFW content is detected
@@ -945,13 +911,12 @@
     def decode_latents(self, latents):
         # Explicit call to get the vae loaded, since `decode` isn't the forward method.
         self._model_group.load(self.vae)
         return super().decode_latents(latents)
 
     def debug_latents(self, latents, msg):
         from invokeai.backend.image_util import debug_image
+
         with torch.inference_mode():
             decoded = self.numpy_to_pil(self.decode_latents(latents))
         for i, img in enumerate(decoded):
-            debug_image(
-                img, f"latents {msg} {i+1}/{len(decoded)}", debug_status=True
-            )
+            debug_image(img, f"latents {msg} {i+1}/{len(decoded)}", debug_status=True)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusion/cross_attention_control.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusion/cross_attention_control.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 from diffusers.models.unet_2d_condition import UNet2DConditionModel
 from diffusers.models.attention_processor import AttentionProcessor
 from torch import nn
 
 import invokeai.backend.util.logging as logger
 from ...util import torch_dtype
 
+
 class CrossAttentionType(enum.Enum):
     SELF = 1
     TOKENS = 2
 
 
 class Context:
     cross_attention_mask: Optional[torch.Tensor]
@@ -51,30 +52,26 @@
         self.clear_requests(cleanup=True)
 
     def register_cross_attention_modules(self, model):
         for name, module in get_cross_attention_modules(model, CrossAttentionType.SELF):
             if name in self.self_cross_attention_module_identifiers:
                 assert False, f"name {name} cannot appear more than once"
             self.self_cross_attention_module_identifiers.append(name)
-        for name, module in get_cross_attention_modules(
-            model, CrossAttentionType.TOKENS
-        ):
+        for name, module in get_cross_attention_modules(model, CrossAttentionType.TOKENS):
             if name in self.tokens_cross_attention_module_identifiers:
                 assert False, f"name {name} cannot appear more than once"
             self.tokens_cross_attention_module_identifiers.append(name)
 
     def request_save_attention_maps(self, cross_attention_type: CrossAttentionType):
         if cross_attention_type == CrossAttentionType.SELF:
             self.self_cross_attention_action = Context.Action.SAVE
         else:
             self.tokens_cross_attention_action = Context.Action.SAVE
 
-    def request_apply_saved_attention_maps(
-        self, cross_attention_type: CrossAttentionType
-    ):
+    def request_apply_saved_attention_maps(self, cross_attention_type: CrossAttentionType):
         if cross_attention_type == CrossAttentionType.SELF:
             self.self_cross_attention_action = Context.Action.APPLY
         else:
             self.tokens_cross_attention_action = Context.Action.APPLY
 
     def is_tokens_cross_attention(self, module_identifier) -> bool:
         return module_identifier in self.tokens_cross_attention_module_identifiers
@@ -135,44 +132,34 @@
         requested_dim: Optional[int],
         requested_offset: int,
         slice_size: int,
     ):
         saved_attention_dict = self.saved_cross_attention_maps[identifier]
         if requested_dim is None:
             if saved_attention_dict["dim"] is not None:
-                raise RuntimeError(
-                    f"dim mismatch: expected dim=None, have {saved_attention_dict['dim']}"
-                )
+                raise RuntimeError(f"dim mismatch: expected dim=None, have {saved_attention_dict['dim']}")
             return saved_attention_dict["slices"][0]
 
         if saved_attention_dict["dim"] == requested_dim:
             if slice_size != saved_attention_dict["slice_size"]:
                 raise RuntimeError(
                     f"slice_size mismatch: expected slice_size={slice_size}, have {saved_attention_dict['slice_size']}"
                 )
             return saved_attention_dict["slices"][requested_offset]
 
         if saved_attention_dict["dim"] is None:
             whole_saved_attention = saved_attention_dict["slices"][0]
             if requested_dim == 0:
-                return whole_saved_attention[
-                    requested_offset : requested_offset + slice_size
-                ]
+                return whole_saved_attention[requested_offset : requested_offset + slice_size]
             elif requested_dim == 1:
-                return whole_saved_attention[
-                    :, requested_offset : requested_offset + slice_size
-                ]
+                return whole_saved_attention[:, requested_offset : requested_offset + slice_size]
 
-        raise RuntimeError(
-            f"Cannot convert dim {saved_attention_dict['dim']} to requested dim {requested_dim}"
-        )
+        raise RuntimeError(f"Cannot convert dim {saved_attention_dict['dim']} to requested dim {requested_dim}")
 
-    def get_slicing_strategy(
-        self, identifier: str
-    ) -> tuple[Optional[int], Optional[int]]:
+    def get_slicing_strategy(self, identifier: str) -> tuple[Optional[int], Optional[int]]:
         saved_attention = self.saved_cross_attention_maps.get(identifier, None)
         if saved_attention is None:
             return None, None
         return saved_attention["dim"], saved_attention["slice_size"]
 
     def clear_requests(self, cleanup=True):
         self.tokens_cross_attention_action = Context.Action.NONE
@@ -197,17 +184,15 @@
         self.mem_total_gb = psutil.virtual_memory().total // (1 << 30)
         self.attention_slice_wrangler = None
         self.slicing_strategy_getter = None
         self.attention_slice_calculated_callback = None
 
     def set_attention_slice_wrangler(
         self,
-        wrangler: Optional[
-            Callable[[nn.Module, torch.Tensor, int, int, int], torch.Tensor]
-        ],
+        wrangler: Optional[Callable[[nn.Module, torch.Tensor, int, int, int], torch.Tensor]],
     ):
         """
         Set custom attention calculator to be called when attention is calculated
         :param wrangler: Callback, with args (module, suggested_attention_slice, dim, offset, slice_size),
         which returns either the suggested_attention_slice or an adjusted equivalent.
             `module` is the current Attention module for which the callback is being invoked.
             `suggested_attention_slice` is the default-calculated attention slice
@@ -215,22 +200,18 @@
                 If `dim` is >= 0, `offset` and `slice_size` specify the slice start and length.
 
         Pass None to use the default attention calculation.
         :return:
         """
         self.attention_slice_wrangler = wrangler
 
-    def set_slicing_strategy_getter(
-        self, getter: Optional[Callable[[nn.Module], tuple[int, int]]]
-    ):
+    def set_slicing_strategy_getter(self, getter: Optional[Callable[[nn.Module], tuple[int, int]]]):
         self.slicing_strategy_getter = getter
 
-    def set_attention_slice_calculated_callback(
-        self, callback: Optional[Callable[[torch.Tensor], None]]
-    ):
+    def set_attention_slice_calculated_callback(self, callback: Optional[Callable[[torch.Tensor], None]]):
         self.attention_slice_calculated_callback = callback
 
     def einsum_lowest_level(self, query, key, value, dim, offset, slice_size):
         # calculate attention scores
         # attention_scores = torch.einsum('b i d, b j d -> b i j', q, k)
         attention_scores = torch.baddbmm(
             torch.empty(
@@ -243,53 +224,39 @@
             query,
             key.transpose(-1, -2),
             beta=0,
             alpha=self.scale,
         )
 
         # calculate attention slice by taking the best scores for each latent pixel
-        default_attention_slice = attention_scores.softmax(
-            dim=-1, dtype=attention_scores.dtype
-        )
+        default_attention_slice = attention_scores.softmax(dim=-1, dtype=attention_scores.dtype)
         attention_slice_wrangler = self.attention_slice_wrangler
         if attention_slice_wrangler is not None:
-            attention_slice = attention_slice_wrangler(
-                self, default_attention_slice, dim, offset, slice_size
-            )
+            attention_slice = attention_slice_wrangler(self, default_attention_slice, dim, offset, slice_size)
         else:
             attention_slice = default_attention_slice
 
         if self.attention_slice_calculated_callback is not None:
-            self.attention_slice_calculated_callback(
-                attention_slice, dim, offset, slice_size
-            )
+            self.attention_slice_calculated_callback(attention_slice, dim, offset, slice_size)
 
         hidden_states = torch.bmm(attention_slice, value)
         return hidden_states
 
     def einsum_op_slice_dim0(self, q, k, v, slice_size):
-        r = torch.zeros(
-            q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype
-        )
+        r = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)
         for i in range(0, q.shape[0], slice_size):
             end = i + slice_size
-            r[i:end] = self.einsum_lowest_level(
-                q[i:end], k[i:end], v[i:end], dim=0, offset=i, slice_size=slice_size
-            )
+            r[i:end] = self.einsum_lowest_level(q[i:end], k[i:end], v[i:end], dim=0, offset=i, slice_size=slice_size)
         return r
 
     def einsum_op_slice_dim1(self, q, k, v, slice_size):
-        r = torch.zeros(
-            q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype
-        )
+        r = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)
         for i in range(0, q.shape[1], slice_size):
             end = i + slice_size
-            r[:, i:end] = self.einsum_lowest_level(
-                q[:, i:end], k, v, dim=1, offset=i, slice_size=slice_size
-            )
+            r[:, i:end] = self.einsum_lowest_level(q[:, i:end], k, v, dim=1, offset=i, slice_size=slice_size)
         return r
 
     def einsum_op_mps_v1(self, q, k, v):
         if q.shape[1] <= 4096:  # (512x512) max q.shape[1]: 4096
             return self.einsum_lowest_level(q, k, v, None, None, None)
         else:
             slice_size = math.floor(2**30 / (q.shape[0] * q.shape[1]))
@@ -349,14 +316,15 @@
 ):
     if is_running_diffusers:
         unet = model
         unet.set_attn_processor(restore_attention_processor or AttnProcessor())
     else:
         remove_attention_function(model)
 
+
 def setup_cross_attention_control_attention_processors(unet: UNet2DConditionModel, context: Context):
     """
     Inject attention parameters and functions into the passed in model to enable cross attention editing.
 
     :param model: The unet model to inject into.
     :return: None
     """
@@ -368,38 +336,36 @@
     max_length = 77
     # mask=1 means use base prompt attention, mask=0 means use edited prompt attention
     mask = torch.zeros(max_length, dtype=torch_dtype(device))
     indices_target = torch.arange(max_length, dtype=torch.long)
     indices = torch.arange(max_length, dtype=torch.long)
     for name, a0, a1, b0, b1 in context.arguments.edit_opcodes:
         if b0 < max_length:
-            if name == "equal":# or (name == "replace" and a1 - a0 == b1 - b0):
+            if name == "equal":  # or (name == "replace" and a1 - a0 == b1 - b0):
                 # these tokens have not been edited
                 indices[b0:b1] = indices_target[a0:a1]
                 mask[b0:b1] = 1
 
     context.cross_attention_mask = mask.to(device)
     context.cross_attention_index_map = indices.to(device)
     old_attn_processors = unet.attn_processors
     if torch.backends.mps.is_available():
         # see note in StableDiffusionGeneratorPipeline.__init__ about borked slicing on MPS
         unet.set_attn_processor(SwapCrossAttnProcessor())
     else:
         # try to re-use an existing slice size
         default_slice_size = 4
-        slice_size = next((p.slice_size for p in old_attn_processors.values() if type(p) is SlicedAttnProcessor), default_slice_size)
+        slice_size = next(
+            (p.slice_size for p in old_attn_processors.values() if type(p) is SlicedAttnProcessor), default_slice_size
+        )
         unet.set_attn_processor(SlicedSwapCrossAttnProcesser(slice_size=slice_size))
 
-def get_cross_attention_modules(
-    model, which: CrossAttentionType
-) -> list[tuple[str, InvokeAICrossAttentionMixin]]:
-
-    cross_attention_class: type = (
-        InvokeAIDiffusersCrossAttention
-    )
+
+def get_cross_attention_modules(model, which: CrossAttentionType) -> list[tuple[str, InvokeAICrossAttentionMixin]]:
+    cross_attention_class: type = InvokeAIDiffusersCrossAttention
     which_attn = "attn1" if which is CrossAttentionType.SELF else "attn2"
     attention_module_tuples = [
         (name, module)
         for name, module in model.named_modules()
         if isinstance(module, cross_attention_class) and which_attn in name
     ]
     cross_attention_modules_in_model_count = len(attention_module_tuples)
@@ -416,82 +382,63 @@
         )
     return attention_module_tuples
 
 
 def inject_attention_function(unet, context: Context):
     # ORIGINAL SOURCE CODE: https://github.com/huggingface/diffusers/blob/91ddd2a25b848df0fa1262d4f1cd98c7ccb87750/src/diffusers/models/attention.py#L276
 
-    def attention_slice_wrangler(
-        module, suggested_attention_slice: torch.Tensor, dim, offset, slice_size
-    ):
+    def attention_slice_wrangler(module, suggested_attention_slice: torch.Tensor, dim, offset, slice_size):
         # memory_usage = suggested_attention_slice.element_size() * suggested_attention_slice.nelement()
 
         attention_slice = suggested_attention_slice
 
         if context.get_should_save_maps(module.identifier):
             # print(module.identifier, "saving suggested_attention_slice of shape",
             #      suggested_attention_slice.shape, "dim", dim, "offset", offset)
-            slice_to_save = (
-                attention_slice.to("cpu") if dim is not None else attention_slice
-            )
+            slice_to_save = attention_slice.to("cpu") if dim is not None else attention_slice
             context.save_slice(
                 module.identifier,
                 slice_to_save,
                 dim=dim,
                 offset=offset,
                 slice_size=slice_size,
             )
         elif context.get_should_apply_saved_maps(module.identifier):
             # print(module.identifier, "applying saved attention slice for dim", dim, "offset", offset)
-            saved_attention_slice = context.get_slice(
-                module.identifier, dim, offset, slice_size
-            )
+            saved_attention_slice = context.get_slice(module.identifier, dim, offset, slice_size)
 
             # slice may have been offloaded to CPU
-            saved_attention_slice = saved_attention_slice.to(
-                suggested_attention_slice.device
-            )
+            saved_attention_slice = saved_attention_slice.to(suggested_attention_slice.device)
 
             if context.is_tokens_cross_attention(module.identifier):
                 index_map = context.cross_attention_index_map
-                remapped_saved_attention_slice = torch.index_select(
-                    saved_attention_slice, -1, index_map
-                )
+                remapped_saved_attention_slice = torch.index_select(saved_attention_slice, -1, index_map)
                 this_attention_slice = suggested_attention_slice
 
-                mask = context.cross_attention_mask.to(
-                    torch_dtype(suggested_attention_slice.device)
-                )
+                mask = context.cross_attention_mask.to(torch_dtype(suggested_attention_slice.device))
                 saved_mask = mask
                 this_mask = 1 - mask
-                attention_slice = (
-                    remapped_saved_attention_slice * saved_mask
-                    + this_attention_slice * this_mask
-                )
+                attention_slice = remapped_saved_attention_slice * saved_mask + this_attention_slice * this_mask
             else:
                 # just use everything
                 attention_slice = saved_attention_slice
 
         return attention_slice
 
     cross_attention_modules = get_cross_attention_modules(
         unet, CrossAttentionType.TOKENS
     ) + get_cross_attention_modules(unet, CrossAttentionType.SELF)
     for identifier, module in cross_attention_modules:
         module.identifier = identifier
         try:
             module.set_attention_slice_wrangler(attention_slice_wrangler)
-            module.set_slicing_strategy_getter(
-                lambda module: context.get_slicing_strategy(identifier)
-            )
+            module.set_slicing_strategy_getter(lambda module: context.get_slicing_strategy(identifier))
         except AttributeError as e:
             if is_attribute_error_about(e, "set_attention_slice_wrangler"):
-                print(
-                    f"TODO: implement set_attention_slice_wrangler for {type(module)}"
-                )  # TODO
+                print(f"TODO: implement set_attention_slice_wrangler for {type(module)}")  # TODO
             else:
                 raise
 
 
 def remove_attention_function(unet):
     cross_attention_modules = get_cross_attention_modules(
         unet, CrossAttentionType.TOKENS
@@ -499,17 +446,15 @@
     for identifier, module in cross_attention_modules:
         try:
             # clear wrangler callback
             module.set_attention_slice_wrangler(None)
             module.set_slicing_strategy_getter(None)
         except AttributeError as e:
             if is_attribute_error_about(e, "set_attention_slice_wrangler"):
-                print(
-                    f"TODO: implement set_attention_slice_wrangler for {type(module)}"
-                )
+                print(f"TODO: implement set_attention_slice_wrangler for {type(module)}")
             else:
                 raise
 
 
 def is_attribute_error_about(error: AttributeError, attribute: str):
     if hasattr(error, "name"):  # Python 3.10
         return error.name == attribute
@@ -526,17 +471,15 @@
     mem_reserved = stats["reserved_bytes.all.current"]
     mem_free_cuda, _ = torch.cuda.mem_get_info(device)
     mem_free_torch = mem_reserved - mem_active
     mem_free_total = mem_free_cuda + mem_free_torch
     return mem_free_total
 
 
-class InvokeAIDiffusersCrossAttention(
-    diffusers.models.attention.Attention, InvokeAICrossAttentionMixin
-):
+class InvokeAIDiffusersCrossAttention(diffusers.models.attention.Attention, InvokeAICrossAttentionMixin):
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         InvokeAICrossAttentionMixin.__init__(self)
 
     def _attention(self, query, key, value, attention_mask=None):
         # default_result = super()._attention(query,  key, value)
         if attention_mask is not None:
@@ -637,30 +580,24 @@
         attn: Attention,
         hidden_states,
         encoder_hidden_states=None,
         attention_mask=None,
         # kwargs
         swap_cross_attn_context: SwapCrossAttnContext = None,
     ):
-        attention_type = (
-            CrossAttentionType.SELF
-            if encoder_hidden_states is None
-            else CrossAttentionType.TOKENS
-        )
+        attention_type = CrossAttentionType.SELF if encoder_hidden_states is None else CrossAttentionType.TOKENS
 
         # if cross-attention control is not in play, just call through to the base implementation.
         if (
             attention_type is CrossAttentionType.SELF
             or swap_cross_attn_context is None
             or not swap_cross_attn_context.wants_cross_attention_control(attention_type)
         ):
             # print(f"SwapCrossAttnContext for {attention_type} not active - passing request to superclass")
-            return super().__call__(
-                attn, hidden_states, encoder_hidden_states, attention_mask
-            )
+            return super().__call__(attn, hidden_states, encoder_hidden_states, attention_mask)
         # else:
         #    print(f"SwapCrossAttnContext for {attention_type} active")
 
         batch_size, sequence_length, _ = hidden_states.shape
         attention_mask = attn.prepare_attention_mask(
             attention_mask=attention_mask,
             target_length=sequence_length,
@@ -695,40 +632,30 @@
         for i in range(max(1, hidden_states.shape[0] // self.slice_size)):
             start_idx = i * self.slice_size
             end_idx = (i + 1) * self.slice_size
 
             query_slice = query[start_idx:end_idx]
             original_key_slice = original_text_key[start_idx:end_idx]
             modified_key_slice = modified_text_key[start_idx:end_idx]
-            attn_mask_slice = (
-                attention_mask[start_idx:end_idx]
-                if attention_mask is not None
-                else None
-            )
+            attn_mask_slice = attention_mask[start_idx:end_idx] if attention_mask is not None else None
 
-            original_attn_slice = attn.get_attention_scores(
-                query_slice, original_key_slice, attn_mask_slice
-            )
-            modified_attn_slice = attn.get_attention_scores(
-                query_slice, modified_key_slice, attn_mask_slice
-            )
+            original_attn_slice = attn.get_attention_scores(query_slice, original_key_slice, attn_mask_slice)
+            modified_attn_slice = attn.get_attention_scores(query_slice, modified_key_slice, attn_mask_slice)
 
             # because the prompt modifications may result in token sequences shifted forwards or backwards,
             # the original attention probabilities must be remapped to account for token index changes in the
             # modified prompt
             remapped_original_attn_slice = torch.index_select(
                 original_attn_slice, -1, swap_cross_attn_context.index_map
             )
 
             # only some tokens taken from the original attention probabilities. this is controlled by the mask.
             mask = swap_cross_attn_context.mask
             inverse_mask = 1 - mask
-            attn_slice = (
-                remapped_original_attn_slice * mask + modified_attn_slice * inverse_mask
-            )
+            attn_slice = remapped_original_attn_slice * mask + modified_attn_slice * inverse_mask
 
             del remapped_original_attn_slice, modified_attn_slice
 
             attn_slice = torch.bmm(attn_slice, modified_value[start_idx:end_idx])
             hidden_states[start_idx:end_idx] = attn_slice
 
         # done
@@ -740,10 +667,8 @@
         hidden_states = attn.to_out[1](hidden_states)
 
         return hidden_states
 
 
 class SwapCrossAttnProcessor(SlicedSwapCrossAttnProcesser):
     def __init__(self):
-        super(SwapCrossAttnProcessor, self).__init__(
-            slice_size=int(1e9)
-        )  # massive slice size = don't slice
+        super(SwapCrossAttnProcessor, self).__init__(slice_size=int(1e9))  # massive slice size = don't slice
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusion/cross_attention_map_saving.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusion/cross_attention_map_saving.py`

 * *Files 3% similar despite different names*

```diff
@@ -55,41 +55,35 @@
         latents_width = self.latents_shape[1]
 
         merged = None
 
         for key, maps in self.collated_maps.items():
             # maps has shape [(H*W), N] for N tokens
             # but we want [N, H, W]
-            this_scale_factor = math.sqrt(
-                maps.shape[0] / (latents_width * latents_height)
-            )
+            this_scale_factor = math.sqrt(maps.shape[0] / (latents_width * latents_height))
             this_maps_height = int(float(latents_height) * this_scale_factor)
             this_maps_width = int(float(latents_width) * this_scale_factor)
             # and we need to do some dimension juggling
             maps = torch.reshape(
                 torch.swapdims(maps, 0, 1),
                 [num_tokens, this_maps_height, this_maps_width],
             )
 
             # scale to output size if necessary
             if this_scale_factor != 1:
-                maps = tv_resize(
-                    maps, [latents_height, latents_width], InterpolationMode.BICUBIC
-                )
+                maps = tv_resize(maps, [latents_height, latents_width], InterpolationMode.BICUBIC)
 
             # normalize
             maps_min = torch.min(maps)
             maps_range = torch.max(maps) - maps_min
             # print(f"map {key} size {[this_maps_width, this_maps_height]} range {[maps_min, maps_min + maps_range]}")
             maps_normalized = (maps - maps_min) / maps_range
             # expand to (-0.1, 1.1) and clamp
             maps_normalized_expanded = maps_normalized * 1.1 - 0.05
-            maps_normalized_expanded_clamped = torch.clamp(
-                maps_normalized_expanded, 0, 1
-            )
+            maps_normalized_expanded_clamped = torch.clamp(maps_normalized_expanded, 0, 1)
 
             # merge together, producing a vertical stack
             maps_stacked = torch.reshape(
                 maps_normalized_expanded_clamped,
                 [num_tokens * latents_height, latents_width],
             )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/diffusion/shared_invokeai_diffusion.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/diffusion/shared_invokeai_diffusion.py`

 * *Files 12% similar despite different names*

```diff
@@ -27,14 +27,15 @@
     Callable[
         [torch.Tensor, torch.Tensor, torch.Tensor, Optional[dict[str, Any]]],
         torch.Tensor,
     ],
     Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor],
 ]
 
+
 @dataclass(frozen=True)
 class PostprocessingSettings:
     threshold: float
     warmup: float
     h_symmetry_time_pct: Optional[float]
     v_symmetry_time_pct: Optional[float]
 
@@ -77,22 +78,20 @@
         self.cross_attention_control_context = None
         self.sequential_guidance = config.sequential_guidance
 
     @classmethod
     @contextmanager
     def custom_attention_context(
         cls,
-        unet: UNet2DConditionModel, # note: also may futz with the text encoder depending on requested LoRAs
+        unet: UNet2DConditionModel,  # note: also may futz with the text encoder depending on requested LoRAs
         extra_conditioning_info: Optional[ExtraConditioningInfo],
-        step_count: int
+        step_count: int,
     ):
         old_attn_processors = None
-        if extra_conditioning_info and (
-            extra_conditioning_info.wants_cross_attention_control
-        ):
+        if extra_conditioning_info and (extra_conditioning_info.wants_cross_attention_control):
             old_attn_processors = unet.attn_processors
             # Load lora conditions into the model
             if extra_conditioning_info.wants_cross_attention_control:
                 cross_attention_control_context = Context(
                     arguments=extra_conditioning_info.cross_attention_control_args,
                     step_count=step_count,
                 )
@@ -112,35 +111,23 @@
     def setup_attention_map_saving(self, saver: AttentionMapSaver):
         def callback(slice, dim, offset, slice_size, key):
             if dim is not None:
                 # sliced tokens attention map saving is not implemented
                 return
             saver.add_attention_maps(slice, key)
 
-        tokens_cross_attention_modules = get_cross_attention_modules(
-            self.model, CrossAttentionType.TOKENS
-        )
+        tokens_cross_attention_modules = get_cross_attention_modules(self.model, CrossAttentionType.TOKENS)
         for identifier, module in tokens_cross_attention_modules:
-            key = (
-                "down"
-                if identifier.startswith("down")
-                else "up"
-                if identifier.startswith("up")
-                else "mid"
-            )
+            key = "down" if identifier.startswith("down") else "up" if identifier.startswith("up") else "mid"
             module.set_attention_slice_calculated_callback(
-                lambda slice, dim, offset, slice_size, key=key: callback(
-                    slice, dim, offset, slice_size, key
-                )
+                lambda slice, dim, offset, slice_size, key=key: callback(slice, dim, offset, slice_size, key)
             )
 
     def remove_attention_map_saving(self):
-        tokens_cross_attention_modules = get_cross_attention_modules(
-            self.model, CrossAttentionType.TOKENS
-        )
+        tokens_cross_attention_modules = get_cross_attention_modules(self.model, CrossAttentionType.TOKENS)
         for _, module in tokens_cross_attention_modules:
             module.set_attention_slice_calculated_callback(None)
 
     def do_diffusion_step(
         self,
         x: torch.Tensor,
         sigma: torch.Tensor,
@@ -167,26 +154,28 @@
         else:
             guidance_scale = unconditional_guidance_scale
 
         cross_attention_control_types_to_do = []
         context: Context = self.cross_attention_control_context
         if self.cross_attention_control_context is not None:
             percent_through = step_index / total_step_count
-            cross_attention_control_types_to_do = (
-                context.get_active_cross_attention_control_types_for_step(
-                    percent_through
-                )
+            cross_attention_control_types_to_do = context.get_active_cross_attention_control_types_for_step(
+                percent_through
             )
 
         wants_cross_attention_control = len(cross_attention_control_types_to_do) > 0
         wants_hybrid_conditioning = isinstance(conditioning, dict)
 
         if wants_hybrid_conditioning:
             unconditioned_next_x, conditioned_next_x = self._apply_hybrid_conditioning(
-                x, sigma, unconditioning, conditioning, **kwargs,
+                x,
+                sigma,
+                unconditioning,
+                conditioning,
+                **kwargs,
             )
         elif wants_cross_attention_control:
             (
                 unconditioned_next_x,
                 conditioned_next_x,
             ) = self._apply_cross_attention_controlled_conditioning(
                 x,
@@ -197,73 +186,93 @@
                 **kwargs,
             )
         elif self.sequential_guidance:
             (
                 unconditioned_next_x,
                 conditioned_next_x,
             ) = self._apply_standard_conditioning_sequentially(
-                x, sigma, unconditioning, conditioning, **kwargs,
+                x,
+                sigma,
+                unconditioning,
+                conditioning,
+                **kwargs,
             )
 
         else:
             (
                 unconditioned_next_x,
                 conditioned_next_x,
             ) = self._apply_standard_conditioning(
-                x, sigma, unconditioning, conditioning, **kwargs,
+                x,
+                sigma,
+                unconditioning,
+                conditioning,
+                **kwargs,
             )
 
         combined_next_x = self._combine(
             # unconditioned_next_x, conditioned_next_x, unconditional_guidance_scale
-            unconditioned_next_x, conditioned_next_x, guidance_scale
+            unconditioned_next_x,
+            conditioned_next_x,
+            guidance_scale,
         )
 
         return combined_next_x
 
     def do_latent_postprocessing(
         self,
         postprocessing_settings: PostprocessingSettings,
         latents: torch.Tensor,
         sigma,
         step_index,
         total_step_count,
     ) -> torch.Tensor:
         if postprocessing_settings is not None:
             percent_through = step_index / total_step_count
-            latents = self.apply_threshold(
-                postprocessing_settings, latents, percent_through
-            )
-            latents = self.apply_symmetry(
-                postprocessing_settings, latents, percent_through
-            )
+            latents = self.apply_threshold(postprocessing_settings, latents, percent_through)
+            latents = self.apply_symmetry(postprocessing_settings, latents, percent_through)
         return latents
 
     def _concat_conditionings_for_batch(self, unconditioning, conditioning):
         def _pad_conditioning(cond, target_len, encoder_attention_mask):
-            conditioning_attention_mask = torch.ones((cond.shape[0], cond.shape[1]), device=cond.device, dtype=cond.dtype)
+            conditioning_attention_mask = torch.ones(
+                (cond.shape[0], cond.shape[1]), device=cond.device, dtype=cond.dtype
+            )
 
             if cond.shape[1] < max_len:
-                conditioning_attention_mask = torch.cat([
-                    conditioning_attention_mask,
-                    torch.zeros((cond.shape[0], max_len - cond.shape[1]), device=cond.device, dtype=cond.dtype),
-                ], dim=1)
-
-                cond = torch.cat([
-                    cond,
-                    torch.zeros((cond.shape[0], max_len - cond.shape[1], cond.shape[2]), device=cond.device, dtype=cond.dtype),
-                ], dim=1)
+                conditioning_attention_mask = torch.cat(
+                    [
+                        conditioning_attention_mask,
+                        torch.zeros((cond.shape[0], max_len - cond.shape[1]), device=cond.device, dtype=cond.dtype),
+                    ],
+                    dim=1,
+                )
+
+                cond = torch.cat(
+                    [
+                        cond,
+                        torch.zeros(
+                            (cond.shape[0], max_len - cond.shape[1], cond.shape[2]),
+                            device=cond.device,
+                            dtype=cond.dtype,
+                        ),
+                    ],
+                    dim=1,
+                )
 
             if encoder_attention_mask is None:
                 encoder_attention_mask = conditioning_attention_mask
             else:
-                encoder_attention_mask = torch.cat([
-                    encoder_attention_mask,
-                    conditioning_attention_mask,
-                ])
-            
+                encoder_attention_mask = torch.cat(
+                    [
+                        encoder_attention_mask,
+                        conditioning_attention_mask,
+                    ]
+                )
+
             return cond, encoder_attention_mask
 
         encoder_attention_mask = None
         if unconditioning.shape[1] != conditioning.shape[1]:
             max_len = max(unconditioning.shape[1], conditioning.shape[1])
             unconditioning, encoder_attention_mask = _pad_conditioning(unconditioning, max_len, encoder_attention_mask)
             conditioning, encoder_attention_mask = _pad_conditioning(conditioning, max_len, encoder_attention_mask)
@@ -273,19 +282,19 @@
     # methods below are called from do_diffusion_step and should be considered private to this class.
 
     def _apply_standard_conditioning(self, x, sigma, unconditioning, conditioning, **kwargs):
         # fast batched path
         x_twice = torch.cat([x] * 2)
         sigma_twice = torch.cat([sigma] * 2)
 
-        both_conditionings, encoder_attention_mask = self._concat_conditionings_for_batch(
-            unconditioning, conditioning
-        )
+        both_conditionings, encoder_attention_mask = self._concat_conditionings_for_batch(unconditioning, conditioning)
         both_results = self.model_forward_callback(
-            x_twice, sigma_twice, both_conditionings,
+            x_twice,
+            sigma_twice,
+            both_conditionings,
             encoder_attention_mask=encoder_attention_mask,
             **kwargs,
         )
         unconditioned_next_x, conditioned_next_x = both_results.chunk(2)
         return unconditioned_next_x, conditioned_next_x
 
     def _apply_standard_conditioning_sequentially(
@@ -308,21 +317,25 @@
 
         uncond_mid_block, cond_mid_block = None, None
         mid_block_additional_residual = kwargs.pop("mid_block_additional_residual", None)
         if mid_block_additional_residual is not None:
             uncond_mid_block, cond_mid_block = mid_block_additional_residual.chunk(2)
 
         unconditioned_next_x = self.model_forward_callback(
-            x, sigma, unconditioning,
+            x,
+            sigma,
+            unconditioning,
             down_block_additional_residuals=uncond_down_block,
             mid_block_additional_residual=uncond_mid_block,
             **kwargs,
         )
         conditioned_next_x = self.model_forward_callback(
-            x, sigma, conditioning,
+            x,
+            sigma,
+            conditioning,
             down_block_additional_residuals=cond_down_block,
             mid_block_additional_residual=cond_mid_block,
             **kwargs,
         )
         return unconditioned_next_x, conditioned_next_x
 
     # TODO: looks unused
@@ -331,21 +344,23 @@
         assert isinstance(unconditioning, dict)
         x_twice = torch.cat([x] * 2)
         sigma_twice = torch.cat([sigma] * 2)
         both_conditionings = dict()
         for k in conditioning:
             if isinstance(conditioning[k], list):
                 both_conditionings[k] = [
-                    torch.cat([unconditioning[k][i], conditioning[k][i]])
-                    for i in range(len(conditioning[k]))
+                    torch.cat([unconditioning[k][i], conditioning[k][i]]) for i in range(len(conditioning[k]))
                 ]
             else:
                 both_conditionings[k] = torch.cat([unconditioning[k], conditioning[k]])
         unconditioned_next_x, conditioned_next_x = self.model_forward_callback(
-            x_twice, sigma_twice, both_conditionings, **kwargs,
+            x_twice,
+            sigma_twice,
+            both_conditionings,
+            **kwargs,
         ).chunk(2)
         return unconditioned_next_x, conditioned_next_x
 
     def _apply_cross_attention_controlled_conditioning(
         self,
         x: torch.Tensor,
         sigma,
@@ -384,17 +399,15 @@
             {"swap_cross_attn_context": cross_attn_processor_context},
             down_block_additional_residuals=uncond_down_block,
             mid_block_additional_residual=uncond_mid_block,
             **kwargs,
         )
 
         # do requested cross attention types for conditioning (positive prompt)
-        cross_attn_processor_context.cross_attention_types_to_do = (
-            cross_attention_control_types_to_do
-        )
+        cross_attn_processor_context.cross_attention_types_to_do = cross_attention_control_types_to_do
         conditioned_next_x = self.model_forward_callback(
             x,
             sigma,
             conditioning,
             {"swap_cross_attn_context": cross_attn_processor_context},
             down_block_additional_residuals=cond_down_block,
             mid_block_additional_residual=cond_mid_block,
@@ -410,83 +423,62 @@
 
     def apply_threshold(
         self,
         postprocessing_settings: PostprocessingSettings,
         latents: torch.Tensor,
         percent_through: float,
     ) -> torch.Tensor:
-        if (
-            postprocessing_settings.threshold is None
-            or postprocessing_settings.threshold == 0.0
-        ):
+        if postprocessing_settings.threshold is None or postprocessing_settings.threshold == 0.0:
             return latents
 
         threshold = postprocessing_settings.threshold
         warmup = postprocessing_settings.warmup
 
         if percent_through < warmup:
-            current_threshold = threshold + threshold * 5 * (
-                1 - (percent_through / warmup)
-            )
+            current_threshold = threshold + threshold * 5 * (1 - (percent_through / warmup))
         else:
             current_threshold = threshold
 
         if current_threshold <= 0:
             return latents
 
         maxval = latents.max().item()
         minval = latents.min().item()
 
         scale = 0.7  # default value from #395
 
         if self.debug_thresholding:
             std, mean = [i.item() for i in torch.std_mean(latents)]
-            outside = torch.count_nonzero(
-                (latents < -current_threshold) | (latents > current_threshold)
-            )
-            logger.info(
-                f"Threshold: %={percent_through} threshold={current_threshold:.3f} (of {threshold:.3f})"
-                )
-            logger.debug(
-                f"min, mean, max = {minval:.3f}, {mean:.3f}, {maxval:.3f}\tstd={std}"
-            )
-            logger.debug(
-                f"{outside / latents.numel() * 100:.2f}% values outside threshold"
-            )
+            outside = torch.count_nonzero((latents < -current_threshold) | (latents > current_threshold))
+            logger.info(f"Threshold: %={percent_through} threshold={current_threshold:.3f} (of {threshold:.3f})")
+            logger.debug(f"min, mean, max = {minval:.3f}, {mean:.3f}, {maxval:.3f}\tstd={std}")
+            logger.debug(f"{outside / latents.numel() * 100:.2f}% values outside threshold")
 
         if maxval < current_threshold and minval > -current_threshold:
             return latents
 
         num_altered = 0
 
         # MPS torch.rand_like is fine because torch.rand_like is wrapped in generate.py!
 
         if maxval > current_threshold:
             latents = torch.clone(latents)
             maxval = np.clip(maxval * scale, 1, current_threshold)
             num_altered += torch.count_nonzero(latents > maxval)
-            latents[latents > maxval] = (
-                torch.rand_like(latents[latents > maxval]) * maxval
-            )
+            latents[latents > maxval] = torch.rand_like(latents[latents > maxval]) * maxval
 
         if minval < -current_threshold:
             latents = torch.clone(latents)
             minval = np.clip(minval * scale, -current_threshold, -1)
             num_altered += torch.count_nonzero(latents < minval)
-            latents[latents < minval] = (
-                torch.rand_like(latents[latents < minval]) * minval
-            )
+            latents[latents < minval] = torch.rand_like(latents[latents < minval]) * minval
 
         if self.debug_thresholding:
-            logger.debug(
-                f"min,     , max = {minval:.3f},        , {maxval:.3f}\t(scaled by {scale})"
-            )
-            logger.debug(
-                f"{num_altered / latents.numel() * 100:.2f}% values altered"
-            )
+            logger.debug(f"min,     , max = {minval:.3f},        , {maxval:.3f}\t(scaled by {scale})")
+            logger.debug(f"{num_altered / latents.numel() * 100:.2f}% values altered")
 
         return latents
 
     def apply_symmetry(
         self,
         postprocessing_settings: PostprocessingSettings,
         latents: torch.Tensor,
@@ -497,23 +489,19 @@
             self.last_percent_through = 0.0
 
         if postprocessing_settings is None:
             return latents
 
         # Check for out of bounds
         h_symmetry_time_pct = postprocessing_settings.h_symmetry_time_pct
-        if h_symmetry_time_pct is not None and (
-            h_symmetry_time_pct <= 0.0 or h_symmetry_time_pct > 1.0
-        ):
+        if h_symmetry_time_pct is not None and (h_symmetry_time_pct <= 0.0 or h_symmetry_time_pct > 1.0):
             h_symmetry_time_pct = None
 
         v_symmetry_time_pct = postprocessing_settings.v_symmetry_time_pct
-        if v_symmetry_time_pct is not None and (
-            v_symmetry_time_pct <= 0.0 or v_symmetry_time_pct > 1.0
-        ):
+        if v_symmetry_time_pct is not None and (v_symmetry_time_pct <= 0.0 or v_symmetry_time_pct > 1.0):
             v_symmetry_time_pct = None
 
         dev = latents.device.type
 
         latents.to(device="cpu")
 
         if (
@@ -550,40 +538,32 @@
 
         self.last_percent_through = percent_through
         return latents.to(device=dev)
 
     def estimate_percent_through(self, step_index, sigma):
         if step_index is not None and self.cross_attention_control_context is not None:
             # percent_through will never reach 1.0 (but this is intended)
-            return float(step_index) / float(
-                self.cross_attention_control_context.step_count
-            )
+            return float(step_index) / float(self.cross_attention_control_context.step_count)
         # find the best possible index of the current sigma in the sigma sequence
         smaller_sigmas = torch.nonzero(self.model.sigmas <= sigma)
         sigma_index = smaller_sigmas[-1].item() if smaller_sigmas.shape[0] > 0 else 0
         # flip because sigmas[0] is for the fully denoised image
         # percent_through must be <1
         return 1.0 - float(sigma_index + 1) / float(self.model.sigmas.shape[0])
         # print('estimated percent_through', percent_through, 'from sigma', sigma.item())
 
     # todo: make this work
     @classmethod
-    def apply_conjunction(
-        cls, x, t, forward_func, uc, c_or_weighted_c_list, global_guidance_scale
-    ):
+    def apply_conjunction(cls, x, t, forward_func, uc, c_or_weighted_c_list, global_guidance_scale):
         x_in = torch.cat([x] * 2)
         t_in = torch.cat([t] * 2)  # aka sigmas
 
         deltas = None
         uncond_latents = None
-        weighted_cond_list = (
-            c_or_weighted_c_list
-            if type(c_or_weighted_c_list) is list
-            else [(c_or_weighted_c_list, 1)]
-        )
+        weighted_cond_list = c_or_weighted_c_list if type(c_or_weighted_c_list) is list else [(c_or_weighted_c_list, 1)]
 
         # below is fugly omg
         conditionings = [uc] + [c for c, weight in weighted_cond_list]
         weights = [1] + [weight for c, weight in weighted_cond_list]
         chunk_count = ceil(len(conditionings) / 2)
         deltas = None
         for chunk_index in range(chunk_count):
@@ -604,22 +584,18 @@
                 deltas = latents_b - uncond_latents
             else:
                 deltas = torch.cat((deltas, latents_a - uncond_latents))
                 if latents_b is not None:
                     deltas = torch.cat((deltas, latents_b - uncond_latents))
 
         # merge the weighted deltas together into a single merged delta
-        per_delta_weights = torch.tensor(
-            weights[1:], dtype=deltas.dtype, device=deltas.device
-        )
+        per_delta_weights = torch.tensor(weights[1:], dtype=deltas.dtype, device=deltas.device)
         normalize = False
         if normalize:
             per_delta_weights /= torch.sum(per_delta_weights)
-        reshaped_weights = per_delta_weights.reshape(
-            per_delta_weights.shape + (1, 1, 1)
-        )
+        reshaped_weights = per_delta_weights.reshape(per_delta_weights.shape + (1, 1, 1))
         deltas_merged = torch.sum(deltas * reshaped_weights, dim=0, keepdim=True)
 
         # old_return_value = super().forward(x, sigma, uncond, cond, cond_scale)
         # assert(0 == len(torch.nonzero(old_return_value - (uncond_latents + deltas_merged * cond_scale))))
 
         return uncond_latents + deltas_merged * global_guidance_scale
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/image_degradation/bsrgan.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/image_degradation/bsrgan.py`

 * *Files 1% similar despite different names*

```diff
@@ -257,17 +257,15 @@
           title={Learning a single convolutional super-resolution network for multiple degradations},
           author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},
           booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
           pages={3262--3271},
           year={2018}
         }
     """
-    x = ndimage.filters.convolve(
-        x, np.expand_dims(k, axis=2), mode="wrap"
-    )  # 'nearest' | 'mirror'
+    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode="wrap")  # 'nearest' | 'mirror'
     x = bicubic_degradation(x, sf=sf)
     return x
 
 
 def dpsr_degradation(x, k, sf=3):
     """bicubic downsampling + blur
     Args:
@@ -385,79 +383,63 @@
 #     return img
 
 
 def add_Gaussian_noise(img, noise_level1=2, noise_level2=25):
     noise_level = random.randint(noise_level1, noise_level2)
     rnum = np.random.rand()
     if rnum > 0.6:  # add color Gaussian noise
-        img = img + np.random.normal(0, noise_level / 255.0, img.shape).astype(
-            np.float32
-        )
+        img = img + np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)
     elif rnum < 0.4:  # add grayscale Gaussian noise
-        img = img + np.random.normal(
-            0, noise_level / 255.0, (*img.shape[:2], 1)
-        ).astype(np.float32)
+        img = img + np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)
     else:  # add  noise
         L = noise_level2 / 255.0
         D = np.diag(np.random.rand(3))
         U = orth(np.random.rand(3, 3))
         conv = np.dot(np.dot(np.transpose(U), D), U)
-        img = img + np.random.multivariate_normal(
-            [0, 0, 0], np.abs(L**2 * conv), img.shape[:2]
-        ).astype(np.float32)
+        img = img + np.random.multivariate_normal([0, 0, 0], np.abs(L**2 * conv), img.shape[:2]).astype(np.float32)
     img = np.clip(img, 0.0, 1.0)
     return img
 
 
 def add_speckle_noise(img, noise_level1=2, noise_level2=25):
     noise_level = random.randint(noise_level1, noise_level2)
     img = np.clip(img, 0.0, 1.0)
     rnum = random.random()
     if rnum > 0.6:
-        img += img * np.random.normal(0, noise_level / 255.0, img.shape).astype(
-            np.float32
-        )
+        img += img * np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)
     elif rnum < 0.4:
-        img += img * np.random.normal(
-            0, noise_level / 255.0, (*img.shape[:2], 1)
-        ).astype(np.float32)
+        img += img * np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)
     else:
         L = noise_level2 / 255.0
         D = np.diag(np.random.rand(3))
         U = orth(np.random.rand(3, 3))
         conv = np.dot(np.dot(np.transpose(U), D), U)
-        img += img * np.random.multivariate_normal(
-            [0, 0, 0], np.abs(L**2 * conv), img.shape[:2]
-        ).astype(np.float32)
+        img += img * np.random.multivariate_normal([0, 0, 0], np.abs(L**2 * conv), img.shape[:2]).astype(np.float32)
     img = np.clip(img, 0.0, 1.0)
     return img
 
 
 def add_Poisson_noise(img):
     img = np.clip((img * 255.0).round(), 0, 255) / 255.0
     vals = 10 ** (2 * random.random() + 2.0)  # [2, 4]
     if random.random() < 0.5:
         img = np.random.poisson(img * vals).astype(np.float32) / vals
     else:
         img_gray = np.dot(img[..., :3], [0.299, 0.587, 0.114])
         img_gray = np.clip((img_gray * 255.0).round(), 0, 255) / 255.0
-        noise_gray = (
-            np.random.poisson(img_gray * vals).astype(np.float32) / vals - img_gray
-        )
+        noise_gray = np.random.poisson(img_gray * vals).astype(np.float32) / vals - img_gray
         img += noise_gray[:, :, np.newaxis]
     img = np.clip(img, 0.0, 1.0)
     return img
 
 
 def add_JPEG_noise(img):
     quality_factor = random.randint(30, 95)
     img = cv2.cvtColor(util.single2uint(img), cv2.COLOR_RGB2BGR)
-    result, encimg = cv2.imencode(
-        ".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), quality_factor]
-    )
+    result, encimg = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), quality_factor])
     img = cv2.imdecode(encimg, 1)
     img = cv2.cvtColor(util.uint2single(img), cv2.COLOR_BGR2RGB)
     return img
 
 
 def random_crop(lq, hq, sf=4, lq_patchsize=64):
     h, w = lq.shape[:2]
@@ -536,17 +518,15 @@
                     (int(1 / sf1 * img.shape[1]), int(1 / sf1 * img.shape[0])),
                     interpolation=random.choice([1, 2, 3]),
                 )
             else:
                 k = fspecial("gaussian", 25, random.uniform(0.1, 0.6 * sf))
                 k_shifted = shift_pixel(k, sf)
                 k_shifted = k_shifted / k_shifted.sum()  # blur with shifted kernel
-                img = ndimage.filters.convolve(
-                    img, np.expand_dims(k_shifted, axis=2), mode="mirror"
-                )
+                img = ndimage.filters.convolve(img, np.expand_dims(k_shifted, axis=2), mode="mirror")
                 img = img[0::sf, 0::sf, ...]  # nearest downsampling
             img = np.clip(img, 0.0, 1.0)
 
         elif i == 3:
             # downsample3
             img = cv2.resize(
                 img,
@@ -642,17 +622,15 @@
                     ),
                     interpolation=random.choice([1, 2, 3]),
                 )
             else:
                 k = fspecial("gaussian", 25, random.uniform(0.1, 0.6 * sf))
                 k_shifted = shift_pixel(k, sf)
                 k_shifted = k_shifted / k_shifted.sum()  # blur with shifted kernel
-                image = ndimage.filters.convolve(
-                    image, np.expand_dims(k_shifted, axis=2), mode="mirror"
-                )
+                image = ndimage.filters.convolve(image, np.expand_dims(k_shifted, axis=2), mode="mirror")
                 image = image[0::sf, 0::sf, ...]  # nearest downsampling
             image = np.clip(image, 0.0, 1.0)
 
         elif i == 3:
             # downsample3
             image = cv2.resize(
                 image,
@@ -792,27 +770,23 @@
     print("resizing to", h)
     sf = 4
     deg_fn = partial(degradation_bsrgan_variant, sf=sf)
     for i in range(20):
         print(i)
         img_lq = deg_fn(img)
         print(img_lq)
-        img_lq_bicubic = albumentations.SmallestMaxSize(
-            max_size=h, interpolation=cv2.INTER_CUBIC
-        )(image=img)["image"]
+        img_lq_bicubic = albumentations.SmallestMaxSize(max_size=h, interpolation=cv2.INTER_CUBIC)(image=img)["image"]
         print(img_lq.shape)
         print("bicubic", img_lq_bicubic.shape)
         print(img_hq.shape)
         lq_nearest = cv2.resize(
             util.single2uint(img_lq),
             (int(sf * img_lq.shape[1]), int(sf * img_lq.shape[0])),
             interpolation=0,
         )
         lq_bicubic_nearest = cv2.resize(
             util.single2uint(img_lq_bicubic),
             (int(sf * img_lq.shape[1]), int(sf * img_lq.shape[0])),
             interpolation=0,
         )
-        img_concat = np.concatenate(
-            [lq_bicubic_nearest, lq_nearest, util.single2uint(img_hq)], axis=1
-        )
+        img_concat = np.concatenate([lq_bicubic_nearest, lq_nearest, util.single2uint(img_hq)], axis=1)
         util.imsave(img_concat, str(i) + ".png")
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/image_degradation/bsrgan_light.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/image_degradation/bsrgan_light.py`

 * *Files 4% similar despite different names*

```diff
@@ -257,17 +257,15 @@
           title={Learning a single convolutional super-resolution network for multiple degradations},
           author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},
           booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
           pages={3262--3271},
           year={2018}
         }
     """
-    x = ndimage.filters.convolve(
-        x, np.expand_dims(k, axis=2), mode="wrap"
-    )  # 'nearest' | 'mirror'
+    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode="wrap")  # 'nearest' | 'mirror'
     x = bicubic_degradation(x, sf=sf)
     return x
 
 
 def dpsr_degradation(x, k, sf=3):
     """bicubic downsampling + blur
     Args:
@@ -389,79 +387,63 @@
 #     return img
 
 
 def add_Gaussian_noise(img, noise_level1=2, noise_level2=25):
     noise_level = random.randint(noise_level1, noise_level2)
     rnum = np.random.rand()
     if rnum > 0.6:  # add color Gaussian noise
-        img = img + np.random.normal(0, noise_level / 255.0, img.shape).astype(
-            np.float32
-        )
+        img = img + np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)
     elif rnum < 0.4:  # add grayscale Gaussian noise
-        img = img + np.random.normal(
-            0, noise_level / 255.0, (*img.shape[:2], 1)
-        ).astype(np.float32)
+        img = img + np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)
     else:  # add  noise
         L = noise_level2 / 255.0
         D = np.diag(np.random.rand(3))
         U = orth(np.random.rand(3, 3))
         conv = np.dot(np.dot(np.transpose(U), D), U)
-        img = img + np.random.multivariate_normal(
-            [0, 0, 0], np.abs(L**2 * conv), img.shape[:2]
-        ).astype(np.float32)
+        img = img + np.random.multivariate_normal([0, 0, 0], np.abs(L**2 * conv), img.shape[:2]).astype(np.float32)
     img = np.clip(img, 0.0, 1.0)
     return img
 
 
 def add_speckle_noise(img, noise_level1=2, noise_level2=25):
     noise_level = random.randint(noise_level1, noise_level2)
     img = np.clip(img, 0.0, 1.0)
     rnum = random.random()
     if rnum > 0.6:
-        img += img * np.random.normal(0, noise_level / 255.0, img.shape).astype(
-            np.float32
-        )
+        img += img * np.random.normal(0, noise_level / 255.0, img.shape).astype(np.float32)
     elif rnum < 0.4:
-        img += img * np.random.normal(
-            0, noise_level / 255.0, (*img.shape[:2], 1)
-        ).astype(np.float32)
+        img += img * np.random.normal(0, noise_level / 255.0, (*img.shape[:2], 1)).astype(np.float32)
     else:
         L = noise_level2 / 255.0
         D = np.diag(np.random.rand(3))
         U = orth(np.random.rand(3, 3))
         conv = np.dot(np.dot(np.transpose(U), D), U)
-        img += img * np.random.multivariate_normal(
-            [0, 0, 0], np.abs(L**2 * conv), img.shape[:2]
-        ).astype(np.float32)
+        img += img * np.random.multivariate_normal([0, 0, 0], np.abs(L**2 * conv), img.shape[:2]).astype(np.float32)
     img = np.clip(img, 0.0, 1.0)
     return img
 
 
 def add_Poisson_noise(img):
     img = np.clip((img * 255.0).round(), 0, 255) / 255.0
     vals = 10 ** (2 * random.random() + 2.0)  # [2, 4]
     if random.random() < 0.5:
         img = np.random.poisson(img * vals).astype(np.float32) / vals
     else:
         img_gray = np.dot(img[..., :3], [0.299, 0.587, 0.114])
         img_gray = np.clip((img_gray * 255.0).round(), 0, 255) / 255.0
-        noise_gray = (
-            np.random.poisson(img_gray * vals).astype(np.float32) / vals - img_gray
-        )
+        noise_gray = np.random.poisson(img_gray * vals).astype(np.float32) / vals - img_gray
         img += noise_gray[:, :, np.newaxis]
     img = np.clip(img, 0.0, 1.0)
     return img
 
 
 def add_JPEG_noise(img):
     quality_factor = random.randint(80, 95)
     img = cv2.cvtColor(util.single2uint(img), cv2.COLOR_RGB2BGR)
-    result, encimg = cv2.imencode(
-        ".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), quality_factor]
-    )
+    result, encimg = cv2.imencode(".jpg", img, [int(cv2.IMWRITE_JPEG_QUALITY), quality_factor])
     img = cv2.imdecode(encimg, 1)
     img = cv2.cvtColor(util.uint2single(img), cv2.COLOR_BGR2RGB)
     return img
 
 
 def random_crop(lq, hq, sf=4, lq_patchsize=64):
     h, w = lq.shape[:2]
@@ -540,17 +522,15 @@
                     (int(1 / sf1 * img.shape[1]), int(1 / sf1 * img.shape[0])),
                     interpolation=random.choice([1, 2, 3]),
                 )
             else:
                 k = fspecial("gaussian", 25, random.uniform(0.1, 0.6 * sf))
                 k_shifted = shift_pixel(k, sf)
                 k_shifted = k_shifted / k_shifted.sum()  # blur with shifted kernel
-                img = ndimage.filters.convolve(
-                    img, np.expand_dims(k_shifted, axis=2), mode="mirror"
-                )
+                img = ndimage.filters.convolve(img, np.expand_dims(k_shifted, axis=2), mode="mirror")
                 img = img[0::sf, 0::sf, ...]  # nearest downsampling
             img = np.clip(img, 0.0, 1.0)
 
         elif i == 3:
             # downsample3
             img = cv2.resize(
                 img,
@@ -649,17 +629,15 @@
                     ),
                     interpolation=random.choice([1, 2, 3]),
                 )
             else:
                 k = fspecial("gaussian", 25, random.uniform(0.1, 0.6 * sf))
                 k_shifted = shift_pixel(k, sf)
                 k_shifted = k_shifted / k_shifted.sum()  # blur with shifted kernel
-                image = ndimage.filters.convolve(
-                    image, np.expand_dims(k_shifted, axis=2), mode="mirror"
-                )
+                image = ndimage.filters.convolve(image, np.expand_dims(k_shifted, axis=2), mode="mirror")
                 image = image[0::sf, 0::sf, ...]  # nearest downsampling
 
             image = np.clip(image, 0.0, 1.0)
 
         elif i == 3:
             # downsample3
             image = cv2.resize(
@@ -701,27 +679,25 @@
     deg_fn = partial(degradation_bsrgan_variant, sf=sf)
     for i in range(20):
         print(i)
         img_hq = img
         img_lq = deg_fn(img)["image"]
         img_hq, img_lq = util.uint2single(img_hq), util.uint2single(img_lq)
         print(img_lq)
-        img_lq_bicubic = albumentations.SmallestMaxSize(
-            max_size=h, interpolation=cv2.INTER_CUBIC
-        )(image=img_hq)["image"]
+        img_lq_bicubic = albumentations.SmallestMaxSize(max_size=h, interpolation=cv2.INTER_CUBIC)(image=img_hq)[
+            "image"
+        ]
         print(img_lq.shape)
         print("bicubic", img_lq_bicubic.shape)
         print(img_hq.shape)
         lq_nearest = cv2.resize(
             util.single2uint(img_lq),
             (int(sf * img_lq.shape[1]), int(sf * img_lq.shape[0])),
             interpolation=0,
         )
         lq_bicubic_nearest = cv2.resize(
             util.single2uint(img_lq_bicubic),
             (int(sf * img_lq.shape[1]), int(sf * img_lq.shape[0])),
             interpolation=0,
         )
-        img_concat = np.concatenate(
-            [lq_bicubic_nearest, lq_nearest, util.single2uint(img_hq)], axis=1
-        )
+        img_concat = np.concatenate([lq_bicubic_nearest, lq_nearest, util.single2uint(img_hq)], axis=1)
         util.imsave(img_concat, str(i) + ".png")
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/image_degradation/utils_image.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/image_degradation/utils_image.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 import numpy as np
 import torch
 from torchvision.utils import make_grid
 
 # import matplotlib.pyplot as plt   # TODO: check with Dominik, also bsrgan.py vs bsrgan_light.py
 
 import invokeai.backend.util.logging as logger
+
 os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
 
 
 """
 # --------------------------------------------
 # Kai Zhang (github: https://github.com/cszn)
 # 03/Mar/2019
@@ -292,30 +293,22 @@
 # --------------------------------------------
 
 
 # convert uint to 4-dimensional torch tensor
 def uint2tensor4(img):
     if img.ndim == 2:
         img = np.expand_dims(img, axis=2)
-    return (
-        torch.from_numpy(np.ascontiguousarray(img))
-        .permute(2, 0, 1)
-        .float()
-        .div(255.0)
-        .unsqueeze(0)
-    )
+    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.0).unsqueeze(0)
 
 
 # convert uint to 3-dimensional torch tensor
 def uint2tensor3(img):
     if img.ndim == 2:
         img = np.expand_dims(img, axis=2)
-    return (
-        torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.0)
-    )
+    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.0)
 
 
 # convert 2/3/4-dimensional torch tensor to uint
 def tensor2uint(img):
     img = img.data.squeeze().float().clamp_(0, 1).cpu().numpy()
     if img.ndim == 3:
         img = np.transpose(img, (1, 2, 0))
@@ -330,20 +323,15 @@
 # convert single (HxWxC) to 3-dimensional torch tensor
 def single2tensor3(img):
     return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float()
 
 
 # convert single (HxWxC) to 4-dimensional torch tensor
 def single2tensor4(img):
-    return (
-        torch.from_numpy(np.ascontiguousarray(img))
-        .permute(2, 0, 1)
-        .float()
-        .unsqueeze(0)
-    )
+    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().unsqueeze(0)
 
 
 # convert torch tensor to single
 def tensor2single(img):
     img = img.data.squeeze().float().cpu().numpy()
     if img.ndim == 3:
         img = np.transpose(img, (1, 2, 0))
@@ -358,20 +346,15 @@
         img = np.transpose(img, (1, 2, 0))
     elif img.ndim == 2:
         img = np.expand_dims(img, axis=2)
     return img
 
 
 def single2tensor5(img):
-    return (
-        torch.from_numpy(np.ascontiguousarray(img))
-        .permute(2, 0, 1, 3)
-        .float()
-        .unsqueeze(0)
-    )
+    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float().unsqueeze(0)
 
 
 def single32tensor5(img):
     return torch.from_numpy(np.ascontiguousarray(img)).float().unsqueeze(0).unsqueeze(0)
 
 
 def single42tensor4(img):
@@ -381,34 +364,28 @@
 # from skimage.io import imread, imsave
 def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):
     """
     Converts a torch Tensor into an image Numpy array of BGR channel order
     Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order
     Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)
     """
-    tensor = (
-        tensor.squeeze().float().cpu().clamp_(*min_max)
-    )  # squeeze first, then clamp
+    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # squeeze first, then clamp
     tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]
     n_dim = tensor.dim()
     if n_dim == 4:
         n_img = len(tensor)
         img_np = make_grid(tensor, nrow=int(math.sqrt(n_img)), normalize=False).numpy()
         img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR
     elif n_dim == 3:
         img_np = tensor.numpy()
         img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR
     elif n_dim == 2:
         img_np = tensor.numpy()
     else:
-        raise TypeError(
-            "Only support 4D, 3D and 2D tensor. But received with dimension: {:d}".format(
-                n_dim
-            )
-        )
+        raise TypeError("Only support 4D, 3D and 2D tensor. But received with dimension: {:d}".format(n_dim))
     if out_type == np.uint8:
         img_np = (img_np * 255.0).round()
         # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.
     return img_np.astype(out_type)
 
 
 """
@@ -740,17 +717,15 @@
     mu1_sq = mu1**2
     mu2_sq = mu2**2
     mu1_mu2 = mu1 * mu2
     sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq
     sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq
     sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2
 
-    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / (
-        (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)
-    )
+    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
     return ssim_map.mean()
 
 
 """
 # --------------------------------------------
 # matlab's bicubic imresize (numpy and torch) [0, 1]
 # --------------------------------------------
@@ -763,17 +738,15 @@
     absx2 = absx**2
     absx3 = absx**3
     return (1.5 * absx3 - 2.5 * absx2 + 1) * ((absx <= 1).type_as(absx)) + (
         -0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2
     ) * (((absx > 1) * (absx <= 2)).type_as(absx))
 
 
-def calculate_weights_indices(
-    in_length, out_length, scale, kernel, kernel_width, antialiasing
-):
+def calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):
     if (scale < 1) and (antialiasing):
         # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width
         kernel_width = kernel_width / scale
 
     # Output-space coordinates
     x = torch.linspace(1, out_length, out_length)
 
@@ -789,17 +762,17 @@
     # computation?  Note: it's OK to use an extra pixel here; if the
     # corresponding weights are all zero, it will be eliminated at the end
     # of this function.
     P = math.ceil(kernel_width) + 2
 
     # The indices of the input pixels involved in computing the k-th output
     # pixel are in row k of the indices matrix.
-    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(
-        0, P - 1, P
-    ).view(1, P).expand(out_length, P)
+    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(1, P).expand(
+        out_length, P
+    )
 
     # The weights used to compute the k-th output pixel are in row k of the
     # weights matrix.
     distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices
     # apply cubic kernel
     if (scale < 1) and (antialiasing):
         weights = scale * cubic(distance_to_center * scale)
@@ -872,17 +845,15 @@
     img_aug.narrow(1, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)
 
     out_1 = torch.FloatTensor(in_C, out_H, in_W)
     kernel_width = weights_H.size(1)
     for i in range(out_H):
         idx = int(indices_H[i][0])
         for j in range(out_C):
-            out_1[j, i, :] = (
-                img_aug[j, idx : idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])
-            )
+            out_1[j, i, :] = img_aug[j, idx : idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])
 
     # process W dimension
     # symmetric copying
     out_1_aug = torch.FloatTensor(in_C, out_H, in_W + sym_len_Ws + sym_len_We)
     out_1_aug.narrow(2, sym_len_Ws, in_W).copy_(out_1)
 
     sym_patch = out_1[:, :, :sym_len_Ws]
@@ -955,17 +926,15 @@
     img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)
 
     out_1 = torch.FloatTensor(out_H, in_W, in_C)
     kernel_width = weights_H.size(1)
     for i in range(out_H):
         idx = int(indices_H[i][0])
         for j in range(out_C):
-            out_1[i, :, j] = (
-                img_aug[idx : idx + kernel_width, :, j].transpose(0, 1).mv(weights_H[i])
-            )
+            out_1[i, :, j] = img_aug[idx : idx + kernel_width, :, j].transpose(0, 1).mv(weights_H[i])
 
     # process W dimension
     # symmetric copying
     out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)
     out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)
 
     sym_patch = out_1[:, :sym_len_Ws, :]
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/offloading.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/offloading.py`

 * *Files 2% similar despite different names*

```diff
@@ -91,18 +91,15 @@
 
     @abstractmethod
     def __contains__(self, model):
         """Check if the model is a member of this group."""
         pass
 
     def __repr__(self) -> str:
-        return (
-            f"<{self.__class__.__name__} object at {id(self):x}: "
-            f"device={self.execution_device} >"
-        )
+        return f"<{self.__class__.__name__} object at {id(self):x}: " f"device={self.execution_device} >"
 
 
 class LazilyLoadedModelGroup(ModelGroup):
     """
     Only one model from this group is loaded on the GPU at a time.
 
     Running the forward method of a model will displace the previously-loaded model,
@@ -139,16 +136,15 @@
     def uninstall_all(self):
         self.uninstall(*self._hooks.keys())
 
     def _pre_hook(self, module: torch.nn.Module, forward_input):
         self.load(module)
         if len(forward_input) == 0:
             warnings.warn(
-                f"Hook for {module.__class__.__name__} got no input. "
-                f"Inputs must be positional, not keywords.",
+                f"Hook for {module.__class__.__name__} got no input. " f"Inputs must be positional, not keywords.",
                 stacklevel=3,
             )
         return send_to_device(forward_input, self.execution_device)
 
     def load(self, module):
         if not self.is_current_model(module):
             self.offload_current()
@@ -157,17 +153,15 @@
     def offload_current(self):
         module = self._current_model_ref()
         if module is not NO_MODEL:
             module.to(OFFLOAD_DEVICE)
         self.clear_current_model()
 
     def _load(self, module: torch.nn.Module) -> torch.nn.Module:
-        assert (
-            self.is_empty()
-        ), f"A model is already loaded: {self._current_model_ref()}"
+        assert self.is_empty(), f"A model is already loaded: {self._current_model_ref()}"
         module = module.to(self.execution_device)
         self.set_current_model(module)
         return module
 
     def is_current_model(self, model: torch.nn.Module) -> bool:
         """Is the given model the one currently loaded on the execution device?"""
         return self._current_model_ref() is model
@@ -188,20 +182,16 @@
         self.execution_device = device
         current = self._current_model_ref()
         if current is not NO_MODEL:
             current.to(device)
 
     def device_for(self, model):
         if model not in self:
-            raise KeyError(
-                f"This does not manage this model {type(model).__name__}", model
-            )
-        return (
-            self.execution_device
-        )  # this implementation only dispatches to one device
+            raise KeyError(f"This does not manage this model {type(model).__name__}", model)
+        return self.execution_device  # this implementation only dispatches to one device
 
     def ready(self):
         pass  # always ready to load on-demand
 
     def __contains__(self, model):
         return model in self._hooks
 
@@ -252,16 +242,12 @@
         self.execution_device = device
         for model in self._models:
             if model.device != OFFLOAD_DEVICE:
                 model.to(device)
 
     def device_for(self, model):
         if model not in self:
-            raise KeyError(
-                "This does not manage this model f{type(model).__name__}", model
-            )
-        return (
-            self.execution_device
-        )  # this implementation only dispatches to one device
+            raise KeyError("This does not manage this model f{type(model).__name__}", model)
+        return self.execution_device  # this implementation only dispatches to one device
 
     def __contains__(self, model):
         return model in self._models
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/stable_diffusion/schedulers/schedulers.py` & `InvokeAI-3.0.1rc2/invokeai/backend/stable_diffusion/schedulers/schedulers.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,11 +1,23 @@
-from diffusers import DDIMScheduler, DPMSolverMultistepScheduler, KDPM2DiscreteScheduler, \
-    KDPM2AncestralDiscreteScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, \
-    HeunDiscreteScheduler, LMSDiscreteScheduler, PNDMScheduler, UniPCMultistepScheduler, \
-    DPMSolverSinglestepScheduler, DEISMultistepScheduler, DDPMScheduler, DPMSolverSDEScheduler
+from diffusers import (
+    DDIMScheduler,
+    DPMSolverMultistepScheduler,
+    KDPM2DiscreteScheduler,
+    KDPM2AncestralDiscreteScheduler,
+    EulerDiscreteScheduler,
+    EulerAncestralDiscreteScheduler,
+    HeunDiscreteScheduler,
+    LMSDiscreteScheduler,
+    PNDMScheduler,
+    UniPCMultistepScheduler,
+    DPMSolverSinglestepScheduler,
+    DEISMultistepScheduler,
+    DDPMScheduler,
+    DPMSolverSDEScheduler,
+)
 
 SCHEDULER_MAP = dict(
     ddim=(DDIMScheduler, dict()),
     ddpm=(DDPMScheduler, dict()),
     deis=(DEISMultistepScheduler, dict()),
     lms=(LMSDiscreteScheduler, dict(use_karras_sigmas=False)),
     lms_k=(LMSDiscreteScheduler, dict(use_karras_sigmas=True)),
@@ -17,13 +29,13 @@
     euler_a=(EulerAncestralDiscreteScheduler, dict()),
     kdpm_2=(KDPM2DiscreteScheduler, dict()),
     kdpm_2_a=(KDPM2AncestralDiscreteScheduler, dict()),
     dpmpp_2s=(DPMSolverSinglestepScheduler, dict(use_karras_sigmas=False)),
     dpmpp_2s_k=(DPMSolverSinglestepScheduler, dict(use_karras_sigmas=True)),
     dpmpp_2m=(DPMSolverMultistepScheduler, dict(use_karras_sigmas=False)),
     dpmpp_2m_k=(DPMSolverMultistepScheduler, dict(use_karras_sigmas=True)),
-    dpmpp_2m_sde=(DPMSolverMultistepScheduler, dict(use_karras_sigmas=False, algorithm_type='sde-dpmsolver++')),
-    dpmpp_2m_sde_k=(DPMSolverMultistepScheduler, dict(use_karras_sigmas=True, algorithm_type='sde-dpmsolver++')),
+    dpmpp_2m_sde=(DPMSolverMultistepScheduler, dict(use_karras_sigmas=False, algorithm_type="sde-dpmsolver++")),
+    dpmpp_2m_sde_k=(DPMSolverMultistepScheduler, dict(use_karras_sigmas=True, algorithm_type="sde-dpmsolver++")),
     dpmpp_sde=(DPMSolverSDEScheduler, dict(use_karras_sigmas=False, noise_sampler_seed=0)),
     dpmpp_sde_k=(DPMSolverSDEScheduler, dict(use_karras_sigmas=True, noise_sampler_seed=0)),
-    unipc=(UniPCMultistepScheduler, dict(cpu_only=True))
+    unipc=(UniPCMultistepScheduler, dict(cpu_only=True)),
 )
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/training/textual_inversion_training.py` & `InvokeAI-3.0.1rc2/invokeai/backend/training/textual_inversion_training.py`

 * *Files 2% similar despite different names*

```diff
@@ -41,15 +41,15 @@
 from PIL import Image
 from torch.utils.data import Dataset
 from torchvision import transforms
 from tqdm.auto import tqdm
 from transformers import CLIPTextModel, CLIPTokenizer
 
 # invokeai stuff
-from invokeai.app.services.config import InvokeAIAppConfig,PagingArgumentParser
+from invokeai.app.services.config import InvokeAIAppConfig, PagingArgumentParser
 from invokeai.app.services.model_manager_service import ModelManagerService
 from invokeai.backend.model_management.models import SubModelType
 
 if version.parse(version.parse(PIL.__version__).base_version) >= version.parse("9.1.0"):
     PIL_INTERPOLATION = {
         "linear": PIL.Image.Resampling.BILINEAR,
         "bilinear": PIL.Image.Resampling.BILINEAR,
@@ -71,32 +71,24 @@
 # Will error if the minimal version of diffusers is not installed. Remove at your own risks.
 check_min_version("0.10.0.dev0")
 
 
 logger = get_logger(__name__)
 
 
-def save_progress(
-    text_encoder, placeholder_token_id, accelerator, placeholder_token, save_path
-):
+def save_progress(text_encoder, placeholder_token_id, accelerator, placeholder_token, save_path):
     logger.info("Saving embeddings")
-    learned_embeds = (
-        accelerator.unwrap_model(text_encoder)
-        .get_input_embeddings()
-        .weight[placeholder_token_id]
-    )
+    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]
     learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}
     torch.save(learned_embeds_dict, save_path)
 
 
 def parse_args():
     config = InvokeAIAppConfig.get_config()
-    parser = PagingArgumentParser(
-        description="Textual inversion training"
-    )
+    parser = PagingArgumentParser(description="Textual inversion training")
     general_group = parser.add_argument_group("General")
     model_group = parser.add_argument_group("Models and Paths")
     image_group = parser.add_argument_group("Training Image Location and Options")
     trigger_group = parser.add_argument_group("Trigger Token")
     training_group = parser.add_argument_group("Training Parameters")
     checkpointing_group = parser.add_argument_group("Checkpointing and Resume")
     integration_group = parser.add_argument_group("Integration")
@@ -217,17 +209,15 @@
     )
     training_group.add_argument(
         "--repeats",
         type=int,
         default=100,
         help="How many times to repeat the training data.",
     )
-    training_group.add_argument(
-        "--seed", type=int, default=None, help="A seed for reproducible training."
-    )
+    training_group.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
     training_group.add_argument(
         "--train_batch_size",
         type=int,
         default=16,
         help="Batch size (per device) for the training dataloader.",
     )
     training_group.add_argument("--num_train_epochs", type=int, default=100)
@@ -283,17 +273,15 @@
     )
     training_group.add_argument(
         "--adam_beta2",
         type=float,
         default=0.999,
         help="The beta2 parameter for the Adam optimizer.",
     )
-    training_group.add_argument(
-        "--adam_weight_decay", type=float, default=1e-2, help="Weight decay to use."
-    )
+    training_group.add_argument("--adam_weight_decay", type=float, default=1e-2, help="Weight decay to use.")
     training_group.add_argument(
         "--adam_epsilon",
         type=float,
         default=1e-08,
         help="Epsilon value for the Adam optimizer",
     )
     training_group.add_argument(
@@ -438,17 +426,15 @@
         self.center_crop = center_crop
         self.flip_p = flip_p
 
         self.image_paths = [
             self.data_root / file_path
             for file_path in self.data_root.iterdir()
             if file_path.is_file()
-            and file_path.name.endswith(
-                (".png", ".PNG", ".jpg", ".JPG", ".jpeg", ".JPEG", ".gif", ".GIF")
-            )
+            and file_path.name.endswith((".png", ".PNG", ".jpg", ".JPG", ".jpeg", ".JPEG", ".gif", ".GIF"))
         ]
 
         self.num_images = len(self.image_paths)
         self._length = self.num_images
 
         if set == "train":
             self._length = self.num_images * repeats
@@ -456,19 +442,15 @@
         self.interpolation = {
             "linear": PIL_INTERPOLATION["linear"],
             "bilinear": PIL_INTERPOLATION["bilinear"],
             "bicubic": PIL_INTERPOLATION["bicubic"],
             "lanczos": PIL_INTERPOLATION["lanczos"],
         }[interpolation]
 
-        self.templates = (
-            imagenet_style_templates_small
-            if learnable_property == "style"
-            else imagenet_templates_small
-        )
+        self.templates = imagenet_style_templates_small if learnable_property == "style" else imagenet_templates_small
         self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)
 
     def __len__(self):
         return self._length
 
     def __getitem__(self, i):
         example = {}
@@ -496,32 +478,28 @@
             (
                 h,
                 w,
             ) = (
                 img.shape[0],
                 img.shape[1],
             )
-            img = img[
-                (h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2
-            ]
+            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]
 
         image = Image.fromarray(img)
         image = image.resize((self.size, self.size), resample=self.interpolation)
 
         image = self.flip_transform(image)
         image = np.array(image).astype(np.uint8)
         image = (image / 127.5 - 1.0).astype(np.float32)
 
         example["pixel_values"] = torch.from_numpy(image).permute(2, 0, 1)
         return example
 
 
-def get_full_repo_name(
-    model_id: str, organization: Optional[str] = None, token: Optional[str] = None
-):
+def get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):
     if token is None:
         token = HfFolder.get_token()
     if organization is None:
         username = whoami(token)["name"]
         return f"{username}/{model_id}"
     else:
         return f"{organization}/{model_id}"
@@ -566,17 +544,15 @@
     checkpointing_steps: int = 500,
     resume_from_checkpoint: Path = None,
     enable_xformers_memory_efficient_attention: bool = False,
     hub_model_id: str = None,
     **kwargs,
 ):
     assert model, "Please specify a base model with --model"
-    assert (
-        train_data_dir
-    ), "Please specify a directory containing the training images using --train_data_dir"
+    assert train_data_dir, "Please specify a directory containing the training images using --train_data_dir"
     assert placeholder_token, "Please specify a trigger term using --placeholder_token"
     env_local_rank = int(os.environ.get("LOCAL_RANK", -1))
     if env_local_rank != -1 and env_local_rank != local_rank:
         local_rank = env_local_rank
 
     # setting up things the way invokeai expects them
     if not os.path.isabs(output_dir):
@@ -589,15 +565,15 @@
     accelerator = Accelerator(
         gradient_accumulation_steps=gradient_accumulation_steps,
         mixed_precision=mixed_precision,
         log_with=report_to,
         project_config=accelerator_config,
     )
 
-    model_manager = ModelManagerService(config,logger)
+    model_manager = ModelManagerService(config, logger)
 
     # Make one log on every process with the configuration for debugging.
     logging.basicConfig(
         format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
         datefmt="%m/%d/%Y %H:%M:%S",
         level=logging.INFO,
     )
@@ -629,34 +605,30 @@
                     gitignore.write("step_*\n")
                 if "epoch_*" not in gitignore:
                     gitignore.write("epoch_*\n")
         elif output_dir is not None:
             os.makedirs(output_dir, exist_ok=True)
 
     known_models = model_manager.model_names()
-    model_name = model.split('/')[-1]
+    model_name = model.split("/")[-1]
     model_meta = next((mm for mm in known_models if mm[0].endswith(model_name)), None)
     assert model_meta is not None, f"Unknown model: {model}"
     model_info = model_manager.model_info(*model_meta)
-    assert (
-        model_info['model_format'] == "diffusers"
-    ), "This script only works with models of type 'diffusers'"
+    assert model_info["model_format"] == "diffusers", "This script only works with models of type 'diffusers'"
     tokenizer_info = model_manager.get_model(*model_meta, submodel=SubModelType.Tokenizer)
     noise_scheduler_info = model_manager.get_model(*model_meta, submodel=SubModelType.Scheduler)
     text_encoder_info = model_manager.get_model(*model_meta, submodel=SubModelType.TextEncoder)
     vae_info = model_manager.get_model(*model_meta, submodel=SubModelType.Vae)
     unet_info = model_manager.get_model(*model_meta, submodel=SubModelType.UNet)
 
     pipeline_args = dict(local_files_only=True)
     if tokenizer_name:
         tokenizer = CLIPTokenizer.from_pretrained(tokenizer_name, **pipeline_args)
     else:
-        tokenizer = CLIPTokenizer.from_pretrained(
-            tokenizer_info.location, subfolder='tokenizer', **pipeline_args
-        )
+        tokenizer = CLIPTokenizer.from_pretrained(tokenizer_info.location, subfolder="tokenizer", **pipeline_args)
 
     # Load scheduler and models
     noise_scheduler = DDPMScheduler.from_pretrained(
         noise_scheduler_info.location, subfolder="scheduler", **pipeline_args
     )
     text_encoder = CLIPTextModel.from_pretrained(
         text_encoder_info.location,
@@ -718,30 +690,23 @@
         text_encoder.gradient_checkpointing_enable()
         unet.enable_gradient_checkpointing()
 
     if enable_xformers_memory_efficient_attention:
         if is_xformers_available():
             unet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                "xformers is not available. Make sure it is installed correctly"
-            )
+            raise ValueError("xformers is not available. Make sure it is installed correctly")
 
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if scale_lr:
-        learning_rate = (
-            learning_rate
-            * gradient_accumulation_steps
-            * train_batch_size
-            * accelerator.num_processes
-        )
+        learning_rate = learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes
 
     # Initialize the optimizer
     optimizer = torch.optim.AdamW(
         text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings
         lr=learning_rate,
         betas=(adam_beta1, adam_beta2),
         weight_decay=adam_weight_decay,
@@ -755,23 +720,19 @@
         size=resolution,
         placeholder_token=placeholder_token,
         repeats=repeats,
         learnable_property=learnable_property,
         center_crop=center_crop,
         set="train",
     )
-    train_dataloader = torch.utils.data.DataLoader(
-        train_dataset, batch_size=train_batch_size, shuffle=True
-    )
+    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / gradient_accumulation_steps
-    )
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)
     if max_train_steps is None:
         max_train_steps = num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         lr_scheduler,
         optimizer=optimizer,
@@ -793,42 +754,36 @@
         weight_dtype = torch.bfloat16
 
     # Move vae and unet to device and cast to weight_dtype
     unet.to(accelerator.device, dtype=weight_dtype)
     vae.to(accelerator.device, dtype=weight_dtype)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / gradient_accumulation_steps
-    )
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)
     if overrode_max_train_steps:
         max_train_steps = num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
     num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
         params = locals()
         for k in params:  # init_trackers() doesn't like objects
             params[k] = str(params[k]) if isinstance(params[k], object) else params[k]
         accelerator.init_trackers("textual_inversion", config=params)
 
     # Train!
-    total_batch_size = (
-        train_batch_size * accelerator.num_processes * gradient_accumulation_steps
-    )
+    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps
 
     logger.info("***** Running training *****")
     logger.info(f"  Num examples = {len(train_dataset)}")
     logger.info(f"  Num Epochs = {num_train_epochs}")
     logger.info(f"  Instantaneous batch size per device = {train_batch_size}")
-    logger.info(
-        f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}"
-    )
+    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
     logger.info(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
     logger.info(f"  Total optimization steps = {max_train_steps}")
     global_step = 0
     first_epoch = 0
     resume_step = None
 
     # Potentially load in the weights and states from a previous save
@@ -839,64 +794,47 @@
             # Get the most recent checkpoint
             dirs = os.listdir(output_dir)
             dirs = [d for d in dirs if d.startswith("checkpoint")]
             dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
-            accelerator.print(
-                f"Checkpoint '{resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+            accelerator.print(f"Checkpoint '{resume_from_checkpoint}' does not exist. Starting a new training run.")
             resume_from_checkpoint = None
         else:
             accelerator.print(f"Resuming from checkpoint {path}")
             accelerator.load_state(os.path.join(output_dir, path))
             global_step = int(path.split("-")[1])
 
             resume_global_step = global_step * gradient_accumulation_steps
             first_epoch = global_step // num_update_steps_per_epoch
-            resume_step = resume_global_step % (
-                num_update_steps_per_epoch * gradient_accumulation_steps
-            )
+            resume_step = resume_global_step % (num_update_steps_per_epoch * gradient_accumulation_steps)
 
     # Only show the progress bar once on each machine.
     progress_bar = tqdm(
         range(global_step, max_train_steps),
         disable=not accelerator.is_local_main_process,
     )
     progress_bar.set_description("Steps")
 
     # keep original embeddings as reference
-    orig_embeds_params = (
-        accelerator.unwrap_model(text_encoder)
-        .get_input_embeddings()
-        .weight.data.clone()
-    )
+    orig_embeds_params = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight.data.clone()
 
     for epoch in range(first_epoch, num_train_epochs):
         text_encoder.train()
         for step, batch in enumerate(train_dataloader):
             # Skip steps until we reach the resumed step
-            if (
-                resume_step
-                and resume_from_checkpoint
-                and epoch == first_epoch
-                and step < resume_step
-            ):
+            if resume_step and resume_from_checkpoint and epoch == first_epoch and step < resume_step:
                 if step % gradient_accumulation_steps == 0:
                     progress_bar.update(1)
                 continue
 
             with accelerator.accumulate(text_encoder):
                 # Convert images to latent space
-                latents = (
-                    vae.encode(batch["pixel_values"].to(dtype=weight_dtype))
-                    .latent_dist.sample()
-                    .detach()
-                )
+                latents = vae.encode(batch["pixel_values"].to(dtype=weight_dtype)).latent_dist.sample().detach()
                 latents = latents * 0.18215
 
                 # Sample noise that we'll add to the latents
                 noise = torch.randn_like(latents)
                 bsz = latents.shape[0]
                 # Sample a random timestep for each image
                 timesteps = torch.randint(
@@ -908,90 +846,74 @@
                 timesteps = timesteps.long()
 
                 # Add noise to the latents according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
                 noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
 
                 # Get the text embedding for conditioning
-                encoder_hidden_states = text_encoder(batch["input_ids"])[0].to(
-                    dtype=weight_dtype
-                )
+                encoder_hidden_states = text_encoder(batch["input_ids"])[0].to(dtype=weight_dtype)
 
                 # Predict the noise residual
-                model_pred = unet(
-                    noisy_latents, timesteps, encoder_hidden_states
-                ).sample
+                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
 
                 # Get the target for loss depending on the prediction type
                 if noise_scheduler.config.prediction_type == "epsilon":
                     target = noise
                 elif noise_scheduler.config.prediction_type == "v_prediction":
                     target = noise_scheduler.get_velocity(latents, noise, timesteps)
                 else:
-                    raise ValueError(
-                        f"Unknown prediction type {noise_scheduler.config.prediction_type}"
-                    )
+                    raise ValueError(f"Unknown prediction type {noise_scheduler.config.prediction_type}")
 
                 loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
 
                 accelerator.backward(loss)
 
                 optimizer.step()
                 lr_scheduler.step()
                 optimizer.zero_grad()
 
                 # Let's make sure we don't update any embedding weights besides the newly added token
                 index_no_updates = torch.arange(len(tokenizer)) != placeholder_token_id
                 with torch.no_grad():
-                    accelerator.unwrap_model(
-                        text_encoder
-                    ).get_input_embeddings().weight[
-                        index_no_updates
-                    ] = orig_embeds_params[
+                    accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[
                         index_no_updates
-                    ]
+                    ] = orig_embeds_params[index_no_updates]
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
                 global_step += 1
                 if global_step % save_steps == 0:
-                    save_path = os.path.join(
-                        output_dir, f"learned_embeds-steps-{global_step}.bin"
-                    )
+                    save_path = os.path.join(output_dir, f"learned_embeds-steps-{global_step}.bin")
                     save_progress(
                         text_encoder,
                         placeholder_token_id,
                         accelerator,
                         placeholder_token,
                         save_path,
                     )
 
                 if global_step % checkpointing_steps == 0:
                     if accelerator.is_main_process:
-                        save_path = os.path.join(
-                            output_dir, f"checkpoint-{global_step}"
-                        )
+                        save_path = os.path.join(output_dir, f"checkpoint-{global_step}")
                         accelerator.save_state(save_path)
                         logger.info(f"Saved state to {save_path}")
 
             logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
             accelerator.log(logs, step=global_step)
 
             if global_step >= max_train_steps:
                 break
 
     # Create the pipeline using using the trained modules and save it.
     accelerator.wait_for_everyone()
     if accelerator.is_main_process:
         if push_to_hub and only_save_embeds:
-            logger.warn(
-                "Enabling full model saving because --push_to_hub=True was specified."
-            )
+            logger.warn("Enabling full model saving because --push_to_hub=True was specified.")
             save_full_model = True
         else:
             save_full_model = not only_save_embeds
         if save_full_model:
             pipeline = StableDiffusionPipeline.from_pretrained(
                 unet_info.location,
                 text_encoder=accelerator.unwrap_model(text_encoder),
@@ -1008,12 +930,10 @@
             placeholder_token_id,
             accelerator,
             placeholder_token,
             save_path,
         )
 
         if push_to_hub:
-            repo.push_to_hub(
-                commit_message="End of training", blocking=False, auto_lfs_prune=True
-            )
+            repo.push_to_hub(commit_message="End of training", blocking=False, auto_lfs_prune=True)
 
     accelerator.end_training()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/util/devices.py` & `InvokeAI-3.0.1rc2/invokeai/backend/util/devices.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 from invokeai.app.services.config import InvokeAIAppConfig
 
 CPU_DEVICE = torch.device("cpu")
 CUDA_DEVICE = torch.device("cuda")
 MPS_DEVICE = torch.device("mps")
 config = InvokeAIAppConfig.get_config()
 
+
 def choose_torch_device() -> torch.device:
     """Convenience routine for guessing which GPU device to run model on"""
     if config.always_use_cpu:
         return CPU_DEVICE
     if torch.cuda.is_available():
         return torch.device("cuda")
     if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/util/hotfixes.py` & `InvokeAI-3.0.1rc2/invokeai/backend/util/hotfixes.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,14 +16,15 @@
 from diffusers.models.unet_2d_condition import UNet2DConditionModel
 
 import diffusers
 from diffusers.models.controlnet import ControlNetConditioningEmbedding, ControlNetOutput, zero_module
 
 # Modified ControlNetModel with encoder_attention_mask argument added
 
+
 class ControlNetModel(ModelMixin, ConfigMixin):
     """
     A ControlNet model.
 
     Args:
         in_channels (`int`, defaults to 4):
             The number of channels in the input sample.
@@ -614,21 +615,20 @@
             down_block_res_samples = [sample * scale for sample, scale in zip(down_block_res_samples, scales)]
             mid_block_res_sample = mid_block_res_sample * scales[-1]  # last one
         else:
             down_block_res_samples = [sample * conditioning_scale for sample in down_block_res_samples]
             mid_block_res_sample = mid_block_res_sample * conditioning_scale
 
         if self.config.global_pool_conditions:
-            down_block_res_samples = [
-                torch.mean(sample, dim=(2, 3), keepdim=True) for sample in down_block_res_samples
-            ]
+            down_block_res_samples = [torch.mean(sample, dim=(2, 3), keepdim=True) for sample in down_block_res_samples]
             mid_block_res_sample = torch.mean(mid_block_res_sample, dim=(2, 3), keepdim=True)
 
         if not return_dict:
             return (down_block_res_samples, mid_block_res_sample)
 
         return ControlNetOutput(
             down_block_res_samples=down_block_res_samples, mid_block_res_sample=mid_block_res_sample
         )
 
+
 diffusers.ControlNetModel = ControlNetModel
-diffusers.models.controlnet.ControlNetModel = ControlNetModel
+diffusers.models.controlnet.ControlNetModel = ControlNetModel
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/util/log.py` & `InvokeAI-3.0.1rc2/invokeai/backend/util/log.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/util/logging.py` & `InvokeAI-3.0.1rc2/invokeai/backend/util/logging.py`

 * *Files 13% similar despite different names*

```diff
@@ -182,118 +182,143 @@
 from abc import abstractmethod
 from pathlib import Path
 
 from invokeai.app.services.config import InvokeAIAppConfig, get_invokeai_config
 
 try:
     import syslog
+
     SYSLOG_AVAILABLE = True
 except:
     SYSLOG_AVAILABLE = False
 
+
 # module level functions
 def debug(msg, *args, **kwargs):
     InvokeAILogger.getLogger().debug(msg, *args, **kwargs)
 
+
 def info(msg, *args, **kwargs):
     InvokeAILogger.getLogger().info(msg, *args, **kwargs)
 
+
 def warning(msg, *args, **kwargs):
     InvokeAILogger.getLogger().warning(msg, *args, **kwargs)
 
+
 def error(msg, *args, **kwargs):
     InvokeAILogger.getLogger().error(msg, *args, **kwargs)
 
+
 def critical(msg, *args, **kwargs):
     InvokeAILogger.getLogger().critical(msg, *args, **kwargs)
 
+
 def log(level, msg, *args, **kwargs):
     InvokeAILogger.getLogger().log(level, msg, *args, **kwargs)
 
+
 def disable(level=logging.CRITICAL):
     InvokeAILogger.getLogger().disable(level)
 
+
 def basicConfig(**kwargs):
     InvokeAILogger.getLogger().basicConfig(**kwargs)
 
+
 def getLogger(name: str = None) -> logging.Logger:
     return InvokeAILogger.getLogger(name)
 
 
-_FACILITY_MAP = dict(
-    LOG_KERN = syslog.LOG_KERN,
-    LOG_USER = syslog.LOG_USER,
-    LOG_MAIL = syslog.LOG_MAIL,
-    LOG_DAEMON = syslog.LOG_DAEMON,
-    LOG_AUTH = syslog.LOG_AUTH,
-    LOG_LPR = syslog.LOG_LPR,
-    LOG_NEWS = syslog.LOG_NEWS,
-    LOG_UUCP = syslog.LOG_UUCP,
-    LOG_CRON = syslog.LOG_CRON,
-    LOG_SYSLOG = syslog.LOG_SYSLOG,
-    LOG_LOCAL0 = syslog.LOG_LOCAL0,
-    LOG_LOCAL1 = syslog.LOG_LOCAL1,
-    LOG_LOCAL2 = syslog.LOG_LOCAL2,
-    LOG_LOCAL3 = syslog.LOG_LOCAL3,
-    LOG_LOCAL4 = syslog.LOG_LOCAL4,
-    LOG_LOCAL5 = syslog.LOG_LOCAL5,
-    LOG_LOCAL6 = syslog.LOG_LOCAL6,
-    LOG_LOCAL7 = syslog.LOG_LOCAL7,
-) if SYSLOG_AVAILABLE else dict()
+_FACILITY_MAP = (
+    dict(
+        LOG_KERN=syslog.LOG_KERN,
+        LOG_USER=syslog.LOG_USER,
+        LOG_MAIL=syslog.LOG_MAIL,
+        LOG_DAEMON=syslog.LOG_DAEMON,
+        LOG_AUTH=syslog.LOG_AUTH,
+        LOG_LPR=syslog.LOG_LPR,
+        LOG_NEWS=syslog.LOG_NEWS,
+        LOG_UUCP=syslog.LOG_UUCP,
+        LOG_CRON=syslog.LOG_CRON,
+        LOG_SYSLOG=syslog.LOG_SYSLOG,
+        LOG_LOCAL0=syslog.LOG_LOCAL0,
+        LOG_LOCAL1=syslog.LOG_LOCAL1,
+        LOG_LOCAL2=syslog.LOG_LOCAL2,
+        LOG_LOCAL3=syslog.LOG_LOCAL3,
+        LOG_LOCAL4=syslog.LOG_LOCAL4,
+        LOG_LOCAL5=syslog.LOG_LOCAL5,
+        LOG_LOCAL6=syslog.LOG_LOCAL6,
+        LOG_LOCAL7=syslog.LOG_LOCAL7,
+    )
+    if SYSLOG_AVAILABLE
+    else dict()
+)
 
 _SOCK_MAP = dict(
-    SOCK_STREAM = socket.SOCK_STREAM,
-    SOCK_DGRAM = socket.SOCK_DGRAM,
+    SOCK_STREAM=socket.SOCK_STREAM,
+    SOCK_DGRAM=socket.SOCK_DGRAM,
 )
 
+
 class InvokeAIFormatter(logging.Formatter):
-    '''
+    """
     Base class for logging formatter
 
-    '''
+    """
+
     def format(self, record):
         formatter = logging.Formatter(self.log_fmt(record.levelno))
         return formatter.format(record)
 
     @abstractmethod
-    def log_fmt(self, levelno: int)->str:
+    def log_fmt(self, levelno: int) -> str:
         pass
-    
+
+
 class InvokeAISyslogFormatter(InvokeAIFormatter):
-    '''
+    """
     Formatting for syslog
-    '''
-    def log_fmt(self, levelno: int)->str:
-        return '%(name)s [%(process)d] <%(levelname)s> %(message)s'
+    """
+
+    def log_fmt(self, levelno: int) -> str:
+        return "%(name)s [%(process)d] <%(levelname)s> %(message)s"
+
 
 class InvokeAILegacyLogFormatter(InvokeAIFormatter):
-    '''
+    """
     Formatting for the InvokeAI Logger (legacy version)
-    '''
+    """
+
     FORMATS = {
         logging.DEBUG: "   | %(message)s",
         logging.INFO: ">> %(message)s",
         logging.WARNING: "** %(message)s",
         logging.ERROR: "*** %(message)s",
         logging.CRITICAL: "### %(message)s",
     }
-    def log_fmt(self,levelno:int)->str:
+
+    def log_fmt(self, levelno: int) -> str:
         return self.FORMATS.get(levelno)
 
+
 class InvokeAIPlainLogFormatter(InvokeAIFormatter):
-    '''
+    """
     Custom Formatting for the InvokeAI Logger (plain version)
-    '''
-    def log_fmt(self, levelno: int)->str:
+    """
+
+    def log_fmt(self, levelno: int) -> str:
         return "[%(asctime)s]::[%(name)s]::%(levelname)s --> %(message)s"
 
+
 class InvokeAIColorLogFormatter(InvokeAIFormatter):
-    '''
+    """
     Custom Formatting for the InvokeAI Logger
-    '''
+    """
+
     # Color Codes
     grey = "\x1b[38;20m"
     yellow = "\x1b[33;20m"
     red = "\x1b[31;20m"
     cyan = "\x1b[36;20m"
     bold_red = "\x1b[31;1m"
     reset = "\x1b[0m"
@@ -304,122 +329,122 @@
 
     # Format Map
     FORMATS = {
         logging.DEBUG: cyan + log_format + reset,
         logging.INFO: grey + log_format + reset,
         logging.WARNING: yellow + log_format + reset,
         logging.ERROR: red + log_format + reset,
-        logging.CRITICAL: bold_red + log_format + reset
+        logging.CRITICAL: bold_red + log_format + reset,
     }
 
-    def log_fmt(self, levelno: int)->str:
+    def log_fmt(self, levelno: int) -> str:
         return self.FORMATS.get(levelno)
 
+
 LOG_FORMATTERS = {
-    'plain': InvokeAIPlainLogFormatter,
-    'color': InvokeAIColorLogFormatter,
-    'syslog': InvokeAISyslogFormatter,
-    'legacy': InvokeAILegacyLogFormatter,
+    "plain": InvokeAIPlainLogFormatter,
+    "color": InvokeAIColorLogFormatter,
+    "syslog": InvokeAISyslogFormatter,
+    "legacy": InvokeAILegacyLogFormatter,
 }
 
+
 class InvokeAILogger(object):
     loggers = dict()
 
     @classmethod
-    def getLogger(cls,
-                  name: str = 'InvokeAI',
-                  config: InvokeAIAppConfig=InvokeAIAppConfig.get_config())->logging.Logger:
+    def getLogger(
+        cls, name: str = "InvokeAI", config: InvokeAIAppConfig = InvokeAIAppConfig.get_config()
+    ) -> logging.Logger:
         if name in cls.loggers:
             logger = cls.loggers[name]
             logger.handlers.clear()
         else:
             logger = logging.getLogger(name)
-        logger.setLevel(config.log_level.upper()) # yes, strings work here
+        logger.setLevel(config.log_level.upper())  # yes, strings work here
         for ch in cls.getLoggers(config):
             logger.addHandler(ch)
             cls.loggers[name] = logger
         return cls.loggers[name]
 
     @classmethod
     def getLoggers(cls, config: InvokeAIAppConfig) -> list[logging.Handler]:
         handler_strs = config.log_handlers
         handlers = list()
         for handler in handler_strs:
-            handler_name,*args = handler.split('=',2)
+            handler_name, *args = handler.split("=", 2)
             args = args[0] if len(args) > 0 else None
 
             # console and file get the fancy formatter.
             # syslog gets a simple one
             # http gets no custom formatter
             formatter = LOG_FORMATTERS[config.log_format]
-            if handler_name=='console':
+            if handler_name == "console":
                 ch = logging.StreamHandler()
                 ch.setFormatter(formatter())
                 handlers.append(ch)
-                
-            elif handler_name=='syslog':
+
+            elif handler_name == "syslog":
                 ch = cls._parse_syslog_args(args)
                 handlers.append(ch)
-                
-            elif handler_name=='file':
+
+            elif handler_name == "file":
                 ch = cls._parse_file_args(args)
                 ch.setFormatter(formatter())
                 handlers.append(ch)
-                
-            elif handler_name=='http':
+
+            elif handler_name == "http":
                 ch = cls._parse_http_args(args)
                 handlers.append(ch)
         return handlers
 
     @staticmethod
-    def _parse_syslog_args(
-            args: str=None
-    )-> logging.Handler:
+    def _parse_syslog_args(args: str = None) -> logging.Handler:
         if not SYSLOG_AVAILABLE:
             raise ValueError("syslog is not available on this system")
         if not args:
-            args='/dev/log' if Path('/dev/log').exists() else 'address:localhost:514'
+            args = "/dev/log" if Path("/dev/log").exists() else "address:localhost:514"
         syslog_args = dict()
         try:
-            for a in args.split(','):
-                arg_name,*arg_value = a.split(':',2)
-                if arg_name=='address':
-                    host,*port = arg_value
-                    port = 514 if len(port)==0 else int(port[0])
-                    syslog_args['address'] = (host,port)
-                elif arg_name=='facility':
-                    syslog_args['facility'] = _FACILITY_MAP[arg_value[0]]
-                elif arg_name=='socktype':
-                    syslog_args['socktype'] = _SOCK_MAP[arg_value[0]]
+            for a in args.split(","):
+                arg_name, *arg_value = a.split(":", 2)
+                if arg_name == "address":
+                    host, *port = arg_value
+                    port = 514 if len(port) == 0 else int(port[0])
+                    syslog_args["address"] = (host, port)
+                elif arg_name == "facility":
+                    syslog_args["facility"] = _FACILITY_MAP[arg_value[0]]
+                elif arg_name == "socktype":
+                    syslog_args["socktype"] = _SOCK_MAP[arg_value[0]]
                 else:
-                    syslog_args['address'] = arg_name
+                    syslog_args["address"] = arg_name
         except:
             raise ValueError(f"{args} is not a value argument list for syslog logging")
         return logging.handlers.SysLogHandler(**syslog_args)
-            
+
     @staticmethod
-    def _parse_file_args(args: str=None)-> logging.Handler:
+    def _parse_file_args(args: str = None) -> logging.Handler:
         if not args:
             raise ValueError("please provide filename for file logging using format 'file=/path/to/logfile.txt'")
         return logging.FileHandler(args)
 
     @staticmethod
-    def _parse_http_args(args: str=None)-> logging.Handler:
+    def _parse_http_args(args: str = None) -> logging.Handler:
         if not args:
             raise ValueError("please provide destination for http logging using format 'http=url'")
-        arg_list = args.split(',')
+        arg_list = args.split(",")
         url = urllib.parse.urlparse(arg_list.pop(0))
-        if url.scheme != 'http':
+        if url.scheme != "http":
             raise ValueError(f"the http logging module can only log to HTTP URLs, but {url.scheme} was specified")
         host = url.hostname
         path = url.path
         port = url.port or 80
-        
+
         syslog_args = dict()
         for a in arg_list:
-            arg_name, *arg_value = a.split(':',2)
-            if arg_name=='method':
-                arg_value = arg_value[0] if len(arg_value)>0 else 'GET'
+            arg_name, *arg_value = a.split(":", 2)
+            if arg_name == "method":
+                arg_value = arg_value[0] if len(arg_value) > 0 else "GET"
                 syslog_args[arg_name] = arg_value
             else:  # TODO: Provide support for SSL context and credentials
                 pass
-        return logging.handlers.HTTPHandler(f'{host}:{port}',path,**syslog_args)
+        return logging.handlers.HTTPHandler(f"{host}:{port}", path, **syslog_args)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/util/mps_fixes.py` & `InvokeAI-3.0.1rc2/invokeai/backend/util/mps_fixes.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,39 +4,47 @@
 
 
 if torch.backends.mps.is_available():
     torch.empty = torch.zeros
 
 
 _torch_layer_norm = torch.nn.functional.layer_norm
+
+
 def new_layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05):
     if input.device.type == "mps" and input.dtype == torch.float16:
         input = input.float()
         if weight is not None:
             weight = weight.float()
         if bias is not None:
             bias = bias.float()
         return _torch_layer_norm(input, normalized_shape, weight, bias, eps).half()
     else:
         return _torch_layer_norm(input, normalized_shape, weight, bias, eps)
 
+
 torch.nn.functional.layer_norm = new_layer_norm
 
 
 _torch_tensor_permute = torch.Tensor.permute
+
+
 def new_torch_tensor_permute(input, *dims):
     result = _torch_tensor_permute(input, *dims)
     if input.device == "mps" and input.dtype == torch.float16:
         result = result.contiguous()
     return result
 
+
 torch.Tensor.permute = new_torch_tensor_permute
 
 
 _torch_lerp = torch.lerp
+
+
 def new_torch_lerp(input, end, weight, *, out=None):
     if input.device.type == "mps" and input.dtype == torch.float16:
         input = input.float()
         end = end.float()
         if isinstance(weight, torch.Tensor):
             weight = weight.float()
         if out is not None:
@@ -48,41 +56,57 @@
             out.copy_(out_fp32.half())
             del out_fp32
         return result.half()
 
     else:
         return _torch_lerp(input, end, weight, out=out)
 
+
 torch.lerp = new_torch_lerp
 
 
 _torch_interpolate = torch.nn.functional.interpolate
-def new_torch_interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False):
+
+
+def new_torch_interpolate(
+    input,
+    size=None,
+    scale_factor=None,
+    mode="nearest",
+    align_corners=None,
+    recompute_scale_factor=None,
+    antialias=False,
+):
     if input.device.type == "mps" and input.dtype == torch.float16:
-        return _torch_interpolate(input.float(), size, scale_factor, mode, align_corners, recompute_scale_factor, antialias).half()
+        return _torch_interpolate(
+            input.float(), size, scale_factor, mode, align_corners, recompute_scale_factor, antialias
+        ).half()
     else:
         return _torch_interpolate(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)
 
+
 torch.nn.functional.interpolate = new_torch_interpolate
 
 # TODO: refactor it
 _SlicedAttnProcessor = diffusers.models.attention_processor.SlicedAttnProcessor
+
+
 class ChunkedSlicedAttnProcessor:
     r"""
     Processor for implementing sliced attention.
 
     Args:
         slice_size (`int`, *optional*):
             The number of steps to compute attention. Uses as many slices as `attention_head_dim // slice_size`, and
             `attention_head_dim` must be a multiple of the `slice_size`.
     """
 
     def __init__(self, slice_size):
         assert isinstance(slice_size, int)
-        slice_size = 1 # TODO: maybe implement chunking in batches too when enough memory
+        slice_size = 1  # TODO: maybe implement chunking in batches too when enough memory
         self.slice_size = slice_size
         self._sliced_attn_processor = _SlicedAttnProcessor(slice_size)
 
     def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):
         if self.slice_size != 1 or attn.upcast_attention:
             return self._sliced_attn_processor(attn, hidden_states, encoder_hidden_states, attention_mask)
 
@@ -117,25 +141,35 @@
         value = attn.head_to_batch_dim(value)
 
         batch_size_attention, query_tokens, _ = query.shape
         hidden_states = torch.zeros(
             (batch_size_attention, query_tokens, dim // attn.heads), device=query.device, dtype=query.dtype
         )
 
-        chunk_tmp_tensor = torch.empty(self.slice_size, query.shape[1], key.shape[1], dtype=query.dtype, device=query.device)
+        chunk_tmp_tensor = torch.empty(
+            self.slice_size, query.shape[1], key.shape[1], dtype=query.dtype, device=query.device
+        )
 
         for i in range(batch_size_attention // self.slice_size):
             start_idx = i * self.slice_size
             end_idx = (i + 1) * self.slice_size
 
             query_slice = query[start_idx:end_idx]
             key_slice = key[start_idx:end_idx]
             attn_mask_slice = attention_mask[start_idx:end_idx] if attention_mask is not None else None
 
-            self.get_attention_scores_chunked(attn, query_slice, key_slice, attn_mask_slice, hidden_states[start_idx:end_idx], value[start_idx:end_idx], chunk_tmp_tensor)
+            self.get_attention_scores_chunked(
+                attn,
+                query_slice,
+                key_slice,
+                attn_mask_slice,
+                hidden_states[start_idx:end_idx],
+                value[start_idx:end_idx],
+                chunk_tmp_tensor,
+            )
 
         hidden_states = attn.batch_to_head_dim(hidden_states)
 
         # linear proj
         hidden_states = attn.to_out[0](hidden_states)
         # dropout
         hidden_states = attn.to_out[1](hidden_states)
@@ -146,67 +180,66 @@
         if attn.residual_connection:
             hidden_states = hidden_states + residual
 
         hidden_states = hidden_states / attn.rescale_output_factor
 
         return hidden_states
 
-
     def get_attention_scores_chunked(self, attn, query, key, attention_mask, hidden_states, value, chunk):
         # batch size = 1
         assert query.shape[0] == 1
         assert key.shape[0] == 1
         assert value.shape[0] == 1
         assert hidden_states.shape[0] == 1
 
         dtype = query.dtype
         if attn.upcast_attention:
             query = query.float()
             key = key.float()
 
-        #out_item_size = query.dtype.itemsize
-        #if attn.upcast_attention:
+        # out_item_size = query.dtype.itemsize
+        # if attn.upcast_attention:
         #    out_item_size = torch.float32.itemsize
         out_item_size = query.element_size()
         if attn.upcast_attention:
             out_item_size = 4
 
-        chunk_size = 2 ** 29
+        chunk_size = 2**29
 
         out_size = query.shape[1] * key.shape[1] * out_item_size
         chunks_count = min(query.shape[1], math.ceil((out_size - 1) / chunk_size))
         chunk_step = max(1, int(query.shape[1] / chunks_count))
 
         key = key.transpose(-1, -2)
 
         def _get_chunk_view(tensor, start, length):
             if start + length > tensor.shape[1]:
                 length = tensor.shape[1] - start
-            #print(f"view: [{tensor.shape[0]},{tensor.shape[1]},{tensor.shape[2]}] - start: {start}, length: {length}")
-            return tensor[:,start:start+length]
+            # print(f"view: [{tensor.shape[0]},{tensor.shape[1]},{tensor.shape[2]}] - start: {start}, length: {length}")
+            return tensor[:, start : start + length]
 
         for chunk_pos in range(0, query.shape[1], chunk_step):
             if attention_mask is not None:
                 torch.baddbmm(
                     _get_chunk_view(attention_mask, chunk_pos, chunk_step),
                     _get_chunk_view(query, chunk_pos, chunk_step),
                     key,
                     beta=1,
                     alpha=attn.scale,
                     out=chunk,
                 )
             else:
                 torch.baddbmm(
-                    torch.zeros((1,1,1), device=query.device, dtype=query.dtype),
+                    torch.zeros((1, 1, 1), device=query.device, dtype=query.dtype),
                     _get_chunk_view(query, chunk_pos, chunk_step),
                     key,
                     beta=0,
                     alpha=attn.scale,
                     out=chunk,
                 )
             chunk = chunk.softmax(dim=-1)
             torch.bmm(chunk, value, out=_get_chunk_view(hidden_states, chunk_pos, chunk_step))
 
-        #del chunk
+        # del chunk
 
 
 diffusers.models.attention_processor.SlicedAttnProcessor = ChunkedSlicedAttnProcessor
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/util/util.py` & `InvokeAI-3.0.1rc2/invokeai/backend/util/util.py`

 * *Files 3% similar despite different names*

```diff
@@ -28,17 +28,15 @@
     b = len(xc)
     txts = list()
     for bi in range(b):
         txt = Image.new("RGB", wh, color="white")
         draw = ImageDraw.Draw(txt)
         font = ImageFont.load_default()
         nc = int(40 * (wh[0] / 256))
-        lines = "\n".join(
-            xc[bi][start : start + nc] for start in range(0, len(xc[bi]), nc)
-        )
+        lines = "\n".join(xc[bi][start : start + nc] for start in range(0, len(xc[bi]), nc))
 
         try:
             draw.text((0, 0), lines, fill="black", font=font)
         except UnicodeEncodeError:
             logger.warning("Cant encode string for logging. Skipping.")
 
         txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0
@@ -77,17 +75,15 @@
     """
     return tensor.mean(dim=list(range(1, len(tensor.shape))))
 
 
 def count_params(model, verbose=False):
     total_params = sum(p.numel() for p in model.parameters())
     if verbose:
-        logger.debug(
-            f"{model.__class__.__name__} has {total_params * 1.e-6:.2f} M params."
-        )
+        logger.debug(f"{model.__class__.__name__} has {total_params * 1.e-6:.2f} M params.")
     return total_params
 
 
 def instantiate_from_config(config, **kwargs):
     if not "target" in config:
         if config == "__is_first_stage__":
             return None
@@ -150,29 +146,20 @@
         Q = mp.Queue(1000)
         proc = mp.Process
     else:
         Q = Queue(1000)
         proc = Thread
     # spawn processes
     if target_data_type == "ndarray":
-        arguments = [
-            [func, Q, part, i, use_worker_id]
-            for i, part in enumerate(np.array_split(data, n_proc))
-        ]
+        arguments = [[func, Q, part, i, use_worker_id] for i, part in enumerate(np.array_split(data, n_proc))]
     else:
-        step = (
-            int(len(data) / n_proc + 1)
-            if len(data) % n_proc != 0
-            else int(len(data) / n_proc)
-        )
+        step = int(len(data) / n_proc + 1) if len(data) % n_proc != 0 else int(len(data) / n_proc)
         arguments = [
             [func, Q, part, i, use_worker_id]
-            for i, part in enumerate(
-                [data[i : i + step] for i in range(0, len(data), step)]
-            )
+            for i, part in enumerate([data[i : i + step] for i in range(0, len(data), step)])
         ]
     processes = []
     for i in range(n_proc):
         p = proc(target=_do_parallel_data_prefetch, args=arguments[i])
         processes += [p]
 
     # start processes
@@ -216,17 +203,15 @@
         for r in gather_res:
             out.extend(r)
         return out
     else:
         return gather_res
 
 
-def rand_perlin_2d(
-    shape, res, device, fade=lambda t: 6 * t**5 - 15 * t**4 + 10 * t**3
-):
+def rand_perlin_2d(shape, res, device, fade=lambda t: 6 * t**5 - 15 * t**4 + 10 * t**3):
     delta = (res[0] / shape[0], res[1] / shape[1])
     d = (shape[0] // res[0], shape[1] // res[1])
 
     grid = (
         torch.stack(
             torch.meshgrid(
                 torch.arange(0, res[0], delta[0]),
@@ -261,28 +246,26 @@
     ).sum(dim=-1)
 
     n00 = dot(tile_grads([0, -1], [0, -1]), [0, 0]).to(device)
     n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0]).to(device)
     n01 = dot(tile_grads([0, -1], [1, None]), [0, -1]).to(device)
     n11 = dot(tile_grads([1, None], [1, None]), [-1, -1]).to(device)
     t = fade(grid[: shape[0], : shape[1]])
-    noise = math.sqrt(2) * torch.lerp(
-        torch.lerp(n00, n10, t[..., 0]), torch.lerp(n01, n11, t[..., 0]), t[..., 1]
-    ).to(device)
+    noise = math.sqrt(2) * torch.lerp(torch.lerp(n00, n10, t[..., 0]), torch.lerp(n01, n11, t[..., 0]), t[..., 1]).to(
+        device
+    )
     return noise.to(dtype=torch_dtype(device))
 
 
 def ask_user(question: str, answers: list):
     from itertools import chain, repeat
 
     user_prompt = f"\n>> {question} {answers}: "
     invalid_answer_msg = "Invalid answer. Please try again."
-    pose_question = chain(
-        [user_prompt], repeat("\n".join([invalid_answer_msg, user_prompt]))
-    )
+    pose_question = chain([user_prompt], repeat("\n".join([invalid_answer_msg, user_prompt])))
     user_answers = map(input, pose_question)
     valid_response = next(filter(answers.__contains__, user_answers))
     return valid_response
 
 
 # -------------------------------------
 def download_with_resume(url: str, dest: Path, access_token: str = None) -> Path:
@@ -299,17 +282,15 @@
     exist_size = 0
 
     resp = requests.get(url, header, stream=True)
     content_length = int(resp.headers.get("content-length", 0))
 
     if dest.is_dir():
         try:
-            file_name = re.search(
-                'filename="(.+)"', resp.headers.get("Content-Disposition")
-            ).group(1)
+            file_name = re.search('filename="(.+)"', resp.headers.get("Content-Disposition")).group(1)
         except:
             file_name = os.path.basename(url)
         dest = dest / file_name
     else:
         dest.parent.mkdir(parents=True, exist_ok=True)
 
     if dest.exists():
@@ -318,15 +299,15 @@
         open_mode = "ab"
         resp = requests.get(url, headers=header, stream=True)  # new request with range
 
     if exist_size > content_length:
         logger.warning("corrupt existing file found. re-downloading")
         os.remove(dest)
         exist_size = 0
-        
+
     if resp.status_code == 416 or (content_length > 0 and exist_size == content_length):
         logger.warning(f"{dest}: complete file found. Skipping.")
         return dest
     elif resp.status_code == 206 or exist_size > 0:
         logger.warning(f"{dest}: partial file found. Resuming...")
     elif resp.status_code != 200:
         logger.error(f"An error occurred during downloading {dest}: {resp.reason}")
@@ -373,26 +354,26 @@
 def image_to_dataURL(image: Image.Image, image_format: str = "PNG") -> str:
     """
     Converts an image into a base64 image dataURL.
     """
     buffered = io.BytesIO()
     image.save(buffered, format=image_format)
     mime_type = Image.MIME.get(image_format.upper(), "image/" + image_format.lower())
-    image_base64 = f"data:{mime_type};base64," + base64.b64encode(
-        buffered.getvalue()
-    ).decode("UTF-8")
+    image_base64 = f"data:{mime_type};base64," + base64.b64encode(buffered.getvalue()).decode("UTF-8")
     return image_base64
 
+
 class Chdir(object):
-    '''Context manager to chdir to desired directory and change back after context exits:
+    """Context manager to chdir to desired directory and change back after context exits:
     Args:
         path (Path): The path to the cwd
-    '''
+    """
+
     def __init__(self, path: Path):
         self.path = path
         self.original = Path().absolute()
 
     def __enter__(self):
         os.chdir(self.path)
 
-    def __exit__(self,*args):
+    def __exit__(self, *args):
         os.chdir(self.original)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/web/invoke_ai_web_server.py` & `InvokeAI-3.0.1rc2/invokeai/backend/web/invoke_ai_web_server.py`

 * *Files 2% similar despite different names*

```diff
@@ -60,18 +60,15 @@
         self.codeformer = codeformer
         self.esrgan = esrgan
 
         self.canceled = Event()
         self.ALLOWED_EXTENSIONS = {"png", "jpg", "jpeg"}
 
     def allowed_file(self, filename: str) -> bool:
-        return (
-            "." in filename
-            and filename.rsplit(".", 1)[1].lower() in self.ALLOWED_EXTENSIONS
-        )
+        return "." in filename and filename.rsplit(".", 1)[1].lower() in self.ALLOWED_EXTENSIONS
 
     def run(self):
         self.setup_app()
         self.setup_flask()
 
     def setup_flask(self):
         # Fix missing mimetypes on Windows
@@ -95,17 +92,15 @@
             # be defensive here, not sure in what form this arrives
             if isinstance(_cors, list):
                 _cors = ",".join(_cors)
             if "," in _cors:
                 _cors = _cors.split(",")
             socketio_args["cors_allowed_origins"] = _cors
 
-        self.app = Flask(
-            __name__, static_url_path="", static_folder=frontend.__path__[0]
-        )
+        self.app = Flask(__name__, static_url_path="", static_folder=frontend.__path__[0])
 
         self.socketio = SocketIO(self.app, **socketio_args)
 
         # Keep Server Alive Route
         @self.app.route("/flaskwebgui-keep-server-alive")
         def keep_alive():
             return {"message": "Server Running"}
@@ -188,17 +183,15 @@
                 if "cropVisible" in data and data["cropVisible"] == True:
                     visible_image_bbox = pil_image.getbbox()
                     pil_image = pil_image.crop(visible_image_bbox)
                     pil_image.save(file_path)
 
                 (width, height) = pil_image.size
 
-                thumbnail_path = save_thumbnail(
-                    pil_image, os.path.basename(file_path), self.thumbnail_image_path
-                )
+                thumbnail_path = save_thumbnail(pil_image, os.path.basename(file_path), self.thumbnail_image_path)
 
                 response = {
                     "url": self.get_url_from_image_path(file_path),
                     "thumbnail": self.get_url_from_image_path(thumbnail_path),
                     "mtime": mtime,
                     "width": width,
                     "height": height,
@@ -233,20 +226,16 @@
             useSSL = args.certfile or args.keyfile
             logger.info("Started Invoke AI Web Server")
             if self.host == "0.0.0.0":
                 logger.info(
                     f"Point your browser at http{'s' if useSSL else ''}://localhost:{self.port} or use the host's DNS name or IP address."
                 )
             else:
-                logger.info(
-                    "Default host address now 127.0.0.1 (localhost). Use --host 0.0.0.0 to bind any address."
-                )
-                logger.info(
-                    f"Point your browser at http{'s' if useSSL else ''}://{self.host}:{self.port}"
-                )
+                logger.info("Default host address now 127.0.0.1 (localhost). Use --host 0.0.0.0 to bind any address.")
+                logger.info(f"Point your browser at http{'s' if useSSL else ''}://{self.host}:{self.port}")
             if not useSSL:
                 self.socketio.run(app=self.app, host=self.host, port=self.port)
             else:
                 self.socketio.run(
                     app=self.app,
                     host=self.host,
                     port=self.port,
@@ -388,53 +377,38 @@
                     )
             except Exception as e:
                 self.handle_exceptions(e)
 
         @socketio.on("convertToDiffusers")
         def convert_to_diffusers(model_to_convert: dict):
             try:
-                if model_info := self.generate.model_manager.model_info(
-                    model_name=model_to_convert["model_name"]
-                ):
+                if model_info := self.generate.model_manager.model_info(model_name=model_to_convert["model_name"]):
                     if "weights" in model_info:
                         ckpt_path = Path(model_info["weights"])
                         original_config_file = Path(model_info["config"])
                         model_name = model_to_convert["model_name"]
                         model_description = model_info["description"]
                     else:
-                        self.socketio.emit(
-                            "error", {"message": "Model is not a valid checkpoint file"}
-                        )
+                        self.socketio.emit("error", {"message": "Model is not a valid checkpoint file"})
                 else:
-                    self.socketio.emit(
-                        "error", {"message": "Could not retrieve model info."}
-                    )
+                    self.socketio.emit("error", {"message": "Could not retrieve model info."})
 
                 if not ckpt_path.is_absolute():
                     ckpt_path = Path(Globals.root, ckpt_path)
 
                 if original_config_file and not original_config_file.is_absolute():
                     original_config_file = Path(Globals.root, original_config_file)
 
-                diffusers_path = Path(
-                    ckpt_path.parent.absolute(), f"{model_name}_diffusers"
-                )
+                diffusers_path = Path(ckpt_path.parent.absolute(), f"{model_name}_diffusers")
 
                 if model_to_convert["save_location"] == "root":
-                    diffusers_path = Path(
-                        global_converted_ckpts_dir(), f"{model_name}_diffusers"
-                    )
+                    diffusers_path = Path(global_converted_ckpts_dir(), f"{model_name}_diffusers")
 
-                if (
-                    model_to_convert["save_location"] == "custom"
-                    and model_to_convert["custom_location"] is not None
-                ):
-                    diffusers_path = Path(
-                        model_to_convert["custom_location"], f"{model_name}_diffusers"
-                    )
+                if model_to_convert["save_location"] == "custom" and model_to_convert["custom_location"] is not None:
+                    diffusers_path = Path(model_to_convert["custom_location"], f"{model_name}_diffusers")
 
                 if diffusers_path.exists():
                     shutil.rmtree(diffusers_path)
 
                 self.generate.model_manager.convert_and_import(
                     ckpt_path,
                     diffusers_path,
@@ -458,18 +432,15 @@
             except Exception as e:
                 self.handle_exceptions(e)
 
         @socketio.on("mergeDiffusersModels")
         def merge_diffusers_models(model_merge_info: dict):
             try:
                 models_to_merge = model_merge_info["models_to_merge"]
-                model_ids_or_paths = [
-                    self.generate.model_manager.model_name_or_path(x)
-                    for x in models_to_merge
-                ]
+                model_ids_or_paths = [self.generate.model_manager.model_name_or_path(x) for x in models_to_merge]
                 merged_pipe = merge_diffusion_models(
                     model_ids_or_paths,
                     model_merge_info["alpha"],
                     model_merge_info["interp"],
                     model_merge_info["force"],
                 )
 
@@ -483,23 +454,19 @@
 
                 merged_model_config = dict(
                     model_name=model_merge_info["merged_model_name"],
                     description=f'Merge of models {", ".join(models_to_merge)}',
                     commit_to_conf=opt.conf,
                 )
 
-                if vae := self.generate.model_manager.config[models_to_merge[0]].get(
-                    "vae", None
-                ):
+                if vae := self.generate.model_manager.config[models_to_merge[0]].get("vae", None):
                     logger.info(f"Using configured VAE assigned to {models_to_merge[0]}")
                     merged_model_config.update(vae=vae)
 
-                self.generate.model_manager.import_diffuser_model(
-                    dump_path, **merged_model_config
-                )
+                self.generate.model_manager.import_diffuser_model(dump_path, **merged_model_config)
                 new_model_list = self.generate.model_manager.list_models()
 
                 socketio.emit(
                     "modelsMerged",
                     {
                         "merged_models": models_to_merge,
                         "merged_model_name": model_merge_info["merged_model_name"],
@@ -521,17 +488,15 @@
                         os.remove(f)
                         thumbnail_path = os.path.join(
                             self.thumbnail_image_path,
                             os.path.splitext(os.path.basename(f))[0] + ".webp",
                         )
                         os.remove(thumbnail_path)
                     except Exception as e:
-                        socketio.emit(
-                            "error", {"message": f"Unable to delete {f}: {str(e)}"}
-                        )
+                        socketio.emit("error", {"message": f"Unable to delete {f}: {str(e)}"})
                         pass
 
                 socketio.emit("tempFolderEmptied")
             except Exception as e:
                 self.handle_exceptions(e)
 
         @socketio.on("requestSaveStagingAreaImageToGallery")
@@ -546,17 +511,15 @@
                 else:
                     metadata = {}
 
                 pil_image = Image.open(new_path)
 
                 (width, height) = pil_image.size
 
-                thumbnail_path = save_thumbnail(
-                    pil_image, os.path.basename(new_path), self.thumbnail_image_path
-                )
+                thumbnail_path = save_thumbnail(pil_image, os.path.basename(new_path), self.thumbnail_image_path)
 
                 image_array = [
                     {
                         "url": self.get_url_from_image_path(new_path),
                         "thumbnail": self.get_url_from_image_path(thumbnail_path),
                         "mtime": os.path.getmtime(new_path),
                         "metadata": metadata,
@@ -573,26 +536,22 @@
 
             except Exception as e:
                 self.handle_exceptions(e)
 
         @socketio.on("requestLatestImages")
         def handle_request_latest_images(category, latest_mtime):
             try:
-                base_path = (
-                    self.result_path if category == "result" else self.init_image_path
-                )
+                base_path = self.result_path if category == "result" else self.init_image_path
 
                 paths = []
 
                 for ext in ("*.png", "*.jpg", "*.jpeg"):
                     paths.extend(glob.glob(os.path.join(base_path, ext)))
 
-                image_paths = sorted(
-                    paths, key=lambda x: os.path.getmtime(x), reverse=True
-                )
+                image_paths = sorted(paths, key=lambda x: os.path.getmtime(x), reverse=True)
 
                 image_paths = list(
                     filter(
                         lambda x: os.path.getmtime(x) > latest_mtime,
                         image_paths,
                     )
                 )
@@ -605,61 +564,51 @@
                             metadata = retrieve_metadata(path)
                         else:
                             metadata = {}
 
                         pil_image = Image.open(path)
                         (width, height) = pil_image.size
 
-                        thumbnail_path = save_thumbnail(
-                            pil_image, os.path.basename(path), self.thumbnail_image_path
-                        )
+                        thumbnail_path = save_thumbnail(pil_image, os.path.basename(path), self.thumbnail_image_path)
 
                         image_array.append(
                             {
                                 "url": self.get_url_from_image_path(path),
-                                "thumbnail": self.get_url_from_image_path(
-                                    thumbnail_path
-                                ),
+                                "thumbnail": self.get_url_from_image_path(thumbnail_path),
                                 "mtime": os.path.getmtime(path),
                                 "metadata": metadata.get("sd-metadata"),
                                 "dreamPrompt": metadata.get("Dream"),
                                 "width": width,
                                 "height": height,
                                 "category": category,
                             }
                         )
                     except Exception as e:
-                        socketio.emit(
-                            "error", {"message": f"Unable to load {path}: {str(e)}"}
-                        )
+                        socketio.emit("error", {"message": f"Unable to load {path}: {str(e)}"})
                         pass
 
                 socketio.emit(
                     "galleryImages",
                     {"images": image_array, "category": category},
                 )
             except Exception as e:
                 self.handle_exceptions(e)
 
         @socketio.on("requestImages")
         def handle_request_images(category, earliest_mtime=None):
             try:
                 page_size = 50
 
-                base_path = (
-                    self.result_path if category == "result" else self.init_image_path
-                )
+                base_path = self.result_path if category == "result" else self.init_image_path
 
                 paths = []
                 for ext in ("*.png", "*.jpg", "*.jpeg"):
                     paths.extend(glob.glob(os.path.join(base_path, ext)))
 
-                image_paths = sorted(
-                    paths, key=lambda x: os.path.getmtime(x), reverse=True
-                )
+                image_paths = sorted(paths, key=lambda x: os.path.getmtime(x), reverse=True)
 
                 if earliest_mtime:
                     image_paths = list(
                         filter(
                             lambda x: os.path.getmtime(x) < earliest_mtime,
                             image_paths,
                         )
@@ -675,69 +624,57 @@
                             metadata = retrieve_metadata(path)
                         else:
                             metadata = {}
 
                         pil_image = Image.open(path)
                         (width, height) = pil_image.size
 
-                        thumbnail_path = save_thumbnail(
-                            pil_image, os.path.basename(path), self.thumbnail_image_path
-                        )
+                        thumbnail_path = save_thumbnail(pil_image, os.path.basename(path), self.thumbnail_image_path)
 
                         image_array.append(
                             {
                                 "url": self.get_url_from_image_path(path),
-                                "thumbnail": self.get_url_from_image_path(
-                                    thumbnail_path
-                                ),
+                                "thumbnail": self.get_url_from_image_path(thumbnail_path),
                                 "mtime": os.path.getmtime(path),
                                 "metadata": metadata.get("sd-metadata"),
                                 "dreamPrompt": metadata.get("Dream"),
                                 "width": width,
                                 "height": height,
                                 "category": category,
                             }
                         )
                     except Exception as e:
                         logger.info(f"Unable to load {path}")
-                        socketio.emit(
-                            "error", {"message": f"Unable to load {path}: {str(e)}"}
-                        )
+                        socketio.emit("error", {"message": f"Unable to load {path}: {str(e)}"})
                         pass
 
                 socketio.emit(
                     "galleryImages",
                     {
                         "images": image_array,
                         "areMoreImagesAvailable": areMoreImagesAvailable,
                         "category": category,
                     },
                 )
             except Exception as e:
                 self.handle_exceptions(e)
 
         @socketio.on("generateImage")
-        def handle_generate_image_event(
-            generation_parameters, esrgan_parameters, facetool_parameters
-        ):
+        def handle_generate_image_event(generation_parameters, esrgan_parameters, facetool_parameters):
             try:
                 # truncate long init_mask/init_img base64 if needed
                 printable_parameters = {
                     **generation_parameters,
                 }
 
                 if "init_img" in generation_parameters:
-                    printable_parameters["init_img"] = (
-                        printable_parameters["init_img"][:64] + "..."
-                    )
+                    printable_parameters["init_img"] = printable_parameters["init_img"][:64] + "..."
 
                 if "init_mask" in generation_parameters:
-                    printable_parameters["init_mask"] = (
-                        printable_parameters["init_mask"][:64] + "..."
-                    )
+                    printable_parameters["init_mask"] = printable_parameters["init_mask"][:64] + "..."
 
                 logger.info(f"Image Generation Parameters:\n\n{printable_parameters}\n")
                 logger.info(f"ESRGAN Parameters: {esrgan_parameters}")
                 logger.info(f"Facetool Parameters: {facetool_parameters}")
 
                 self.generate_images(
                     generation_parameters,
@@ -746,26 +683,22 @@
                 )
             except Exception as e:
                 self.handle_exceptions(e)
 
         @socketio.on("runPostprocessing")
         def handle_run_postprocessing(original_image, postprocessing_parameters):
             try:
-                logger.info(
-                    f'Postprocessing requested for "{original_image["url"]}": {postprocessing_parameters}'
-                )
+                logger.info(f'Postprocessing requested for "{original_image["url"]}": {postprocessing_parameters}')
 
                 progress = Progress()
 
                 socketio.emit("progressUpdate", progress.to_formatted_dict())
                 eventlet.sleep(0)
 
-                original_image_path = self.get_image_path_from_url(
-                    original_image["url"]
-                )
+                original_image_path = self.get_image_path_from_url(original_image["url"])
 
                 image = Image.open(original_image_path)
 
                 try:
                     seed = original_image["metadata"]["image"]["seed"]
                 except KeyError:
                     seed = "unknown_seed"
@@ -797,22 +730,18 @@
                     )
                 elif postprocessing_parameters["type"] == "codeformer":
                     image = self.codeformer.process(
                         image=image,
                         strength=postprocessing_parameters["facetool_strength"],
                         fidelity=postprocessing_parameters["codeformer_fidelity"],
                         seed=seed,
-                        device="cpu"
-                        if str(self.generate.device) == "mps"
-                        else self.generate.device,
+                        device="cpu" if str(self.generate.device) == "mps" else self.generate.device,
                     )
                 else:
-                    raise TypeError(
-                        f'{postprocessing_parameters["type"]} is not a valid postprocessing type'
-                    )
+                    raise TypeError(f'{postprocessing_parameters["type"]} is not a valid postprocessing type')
 
                 progress.set_current_status("common.statusSavingImage")
                 socketio.emit("progressUpdate", progress.to_formatted_dict())
                 eventlet.sleep(0)
 
                 postprocessing_parameters["seed"] = seed
                 metadata = self.parameters_to_post_processed_image_metadata(
@@ -828,17 +757,15 @@
                     image,
                     command,
                     metadata,
                     self.result_path,
                     postprocessing=postprocessing_parameters["type"],
                 )
 
-                thumbnail_path = save_thumbnail(
-                    image, os.path.basename(path), self.thumbnail_image_path
-                )
+                thumbnail_path = save_thumbnail(image, os.path.basename(path), self.thumbnail_image_path)
 
                 self.write_log_message(
                     f'[Postprocessed] "{original_image_path}" > "{path}": {postprocessing_parameters}'
                 )
 
                 progress.mark_complete()
                 socketio.emit("progressUpdate", progress.to_formatted_dict())
@@ -897,25 +824,21 @@
             "model": "stable diffusion",
             "model_weights": active_model_name,
             "model_hash": self.generate.model_hash,
             "app_id": APP_ID,
             "app_version": APP_VERSION,
         }
 
-    def generate_images(
-        self, generation_parameters, esrgan_parameters, facetool_parameters
-    ):
+    def generate_images(self, generation_parameters, esrgan_parameters, facetool_parameters):
         try:
             self.canceled.clear()
 
             step_index = 1
             prior_variations = (
-                generation_parameters["with_variations"]
-                if "with_variations" in generation_parameters
-                else []
+                generation_parameters["with_variations"] if "with_variations" in generation_parameters else []
             )
 
             actual_generation_mode = generation_parameters["generation_mode"]
             original_bounding_box = None
 
             progress = Progress(generation_parameters=generation_parameters)
 
@@ -939,17 +862,15 @@
                 So we need to convert each into a PIL Image.
                 """
 
                 init_img_url = generation_parameters["init_img"]
 
                 original_bounding_box = generation_parameters["bounding_box"].copy()
 
-                initial_image = dataURL_to_image(
-                    generation_parameters["init_img"]
-                ).convert("RGBA")
+                initial_image = dataURL_to_image(generation_parameters["init_img"]).convert("RGBA")
 
                 """
                 The outpaint image and mask are pre-cropped by the UI, so the bounding box we pass
                 to the generator should be:
                     {
                         "x": 0,
                         "y": 0,
@@ -958,21 +879,17 @@
                     }
                 """
 
                 generation_parameters["bounding_box"]["x"] = 0
                 generation_parameters["bounding_box"]["y"] = 0
 
                 # Convert mask dataURL to an image and convert to greyscale
-                mask_image = dataURL_to_image(
-                    generation_parameters["init_mask"]
-                ).convert("L")
+                mask_image = dataURL_to_image(generation_parameters["init_mask"]).convert("L")
 
-                actual_generation_mode = get_canvas_generation_mode(
-                    initial_image, mask_image
-                )
+                actual_generation_mode = get_canvas_generation_mode(initial_image, mask_image)
 
                 """
                 Apply the mask to the init image, creating a "mask" image with
                 transparency where inpainting should occur. This is the kind of
                 mask that prompt2image() needs.
                 """
                 alpha_mask = initial_image.copy()
@@ -1014,17 +931,15 @@
                     generation_parameters.pop("tile_size", None)
                     generation_parameters.pop("force_outpaint", None)
                     generation_parameters.pop("infill_method", None)
 
             elif generation_parameters["generation_mode"] == "img2img":
                 init_img_url = generation_parameters["init_img"]
                 init_img_path = self.get_image_path_from_url(init_img_url)
-                generation_parameters["init_img"] = Image.open(init_img_path).convert(
-                    "RGB"
-                )
+                generation_parameters["init_img"] = Image.open(init_img_path).convert("RGB")
 
             def image_progress(intermediate_state: PipelineIntermediateState):
                 if self.canceled.is_set():
                     raise CanceledException
 
                 nonlocal step_index
                 nonlocal generation_parameters
@@ -1042,28 +957,24 @@
                     "txt2img": "common.statusGeneratingTextToImage",
                     "img2img": "common.statusGeneratingImageToImage",
                     "inpainting": "common.statusGeneratingInpainting",
                     "outpainting": "common.statusGeneratingOutpainting",
                 }
 
                 progress.set_current_step(step + 1)
-                progress.set_current_status(
-                    f"{generation_messages[actual_generation_mode]}"
-                )
+                progress.set_current_status(f"{generation_messages[actual_generation_mode]}")
                 progress.set_current_status_has_steps(True)
 
                 if (
                     generation_parameters["progress_images"]
                     and step % generation_parameters["save_intermediates"] == 0
                     and step < generation_parameters["steps"] - 1
                 ):
                     image = self.generate.sample_to_image(sample)
-                    metadata = self.parameters_to_generated_image_metadata(
-                        generation_parameters
-                    )
+                    metadata = self.parameters_to_generated_image_metadata(generation_parameters)
                     command = parameters_to_command(generation_parameters)
 
                     (width, height) = image.size
 
                     path = self.save_result_image(
                         image,
                         command,
@@ -1136,23 +1047,18 @@
 
                 self.socketio.emit("progressUpdate", progress.to_formatted_dict())
                 eventlet.sleep(0)
 
                 all_parameters = generation_parameters
                 postprocessing = False
 
-                if (
-                    "variation_amount" in all_parameters
-                    and all_parameters["variation_amount"] > 0
-                ):
+                if "variation_amount" in all_parameters and all_parameters["variation_amount"] > 0:
                     first_seed = first_seed or seed
                     this_variation = [[seed, all_parameters["variation_amount"]]]
-                    all_parameters["with_variations"] = (
-                        prior_variations + this_variation
-                    )
+                    all_parameters["with_variations"] = prior_variations + this_variation
                     all_parameters["seed"] = first_seed
                 elif "with_variations" in all_parameters:
                     all_parameters["seed"] = first_seed
                 else:
                     all_parameters["seed"] = seed
 
                 if self.canceled.is_set():
@@ -1182,17 +1088,15 @@
                 if self.canceled.is_set():
                     raise CanceledException
 
                 if facetool_parameters:
                     if facetool_parameters["type"] == "gfpgan":
                         progress.set_current_status("common.statusRestoringFacesGFPGAN")
                     elif facetool_parameters["type"] == "codeformer":
-                        progress.set_current_status(
-                            "common.statusRestoringFacesCodeFormer"
-                        )
+                        progress.set_current_status("common.statusRestoringFacesCodeFormer")
 
                     progress.set_current_status_has_steps(False)
                     self.socketio.emit("progressUpdate", progress.to_formatted_dict())
                     eventlet.sleep(0)
 
                     if facetool_parameters["type"] == "gfpgan":
                         image = self.gfpgan.process(
@@ -1202,37 +1106,29 @@
                         )
                     elif facetool_parameters["type"] == "codeformer":
                         image = self.codeformer.process(
                             image=image,
                             strength=facetool_parameters["strength"],
                             fidelity=facetool_parameters["codeformer_fidelity"],
                             seed=seed,
-                            device="cpu"
-                            if str(self.generate.device) == "mps"
-                            else self.generate.device,
+                            device="cpu" if str(self.generate.device) == "mps" else self.generate.device,
                         )
-                        all_parameters["codeformer_fidelity"] = facetool_parameters[
-                            "codeformer_fidelity"
-                        ]
+                        all_parameters["codeformer_fidelity"] = facetool_parameters["codeformer_fidelity"]
 
                     postprocessing = True
-                    all_parameters["facetool_strength"] = facetool_parameters[
-                        "strength"
-                    ]
+                    all_parameters["facetool_strength"] = facetool_parameters["strength"]
                     all_parameters["facetool_type"] = facetool_parameters["type"]
 
                 progress.set_current_status("common.statusSavingImage")
                 self.socketio.emit("progressUpdate", progress.to_formatted_dict())
                 eventlet.sleep(0)
 
                 # restore the stashed URLS and discard the paths, we are about to send the result to client
                 all_parameters["init_img"] = (
-                    init_img_url
-                    if generation_parameters["generation_mode"] == "img2img"
-                    else ""
+                    init_img_url if generation_parameters["generation_mode"] == "img2img" else ""
                 )
 
                 if "init_mask" in all_parameters:
                     # TODO: store the mask in metadata
                     all_parameters["init_mask"] = ""
 
                 if generation_parameters["generation_mode"] == "unifiedCanvas":
@@ -1242,30 +1138,27 @@
 
                 command = parameters_to_command(all_parameters)
 
                 (width, height) = image.size
 
                 generated_image_outdir = (
                     self.result_path
-                    if generation_parameters["generation_mode"]
-                    in ["txt2img", "img2img"]
+                    if generation_parameters["generation_mode"] in ["txt2img", "img2img"]
                     else self.temp_image_path
                 )
 
                 path = self.save_result_image(
                     image,
                     command,
                     metadata,
                     generated_image_outdir,
                     postprocessing=postprocessing,
                 )
 
-                thumbnail_path = save_thumbnail(
-                    image, os.path.basename(path), self.thumbnail_image_path
-                )
+                thumbnail_path = save_thumbnail(image, os.path.basename(path), self.thumbnail_image_path)
 
                 logger.info(f'Image generated: "{path}"\n')
                 self.write_log_message(f'[Generated] "{path}": {command}')
 
                 if progress.total_iterations > progress.current_iteration:
                     progress.set_current_step(1)
                     progress.set_current_status("common.statusIterationComplete")
@@ -1277,22 +1170,18 @@
                 eventlet.sleep(0)
 
                 parsed_prompt, _ = get_prompt_structure(generation_parameters["prompt"])
                 with self.generate.model_context as model:
                     tokens = (
                         None
                         if type(parsed_prompt) is Blend
-                        else get_tokens_for_prompt_object(
-                                model.tokenizer, parsed_prompt
-                        )
+                        else get_tokens_for_prompt_object(model.tokenizer, parsed_prompt)
                     )
                 attention_maps_image_base64_url = (
-                    None
-                    if attention_maps_image is None
-                    else image_to_dataURL(attention_maps_image)
+                    None if attention_maps_image is None else image_to_dataURL(attention_maps_image)
                 )
 
                 self.socketio.emit(
                     "generationResult",
                     {
                         "url": self.get_url_from_image_path(path),
                         "thumbnail": self.get_url_from_image_path(thumbnail_path),
@@ -1378,82 +1267,67 @@
             if "facetool_strength" in parameters:
                 facetool_parameters = {
                     "type": str(parameters["facetool_type"]),
                     "strength": float(parameters["facetool_strength"]),
                 }
 
                 if parameters["facetool_type"] == "codeformer":
-                    facetool_parameters["fidelity"] = float(
-                        parameters["codeformer_fidelity"]
-                    )
+                    facetool_parameters["fidelity"] = float(parameters["codeformer_fidelity"])
 
                 postprocessing.append(facetool_parameters)
 
             if "upscale" in parameters:
                 postprocessing.append(
                     {
                         "type": "esrgan",
                         "scale": int(parameters["upscale"][0]),
                         "denoise_str": int(parameters["upscale"][1]),
                         "strength": float(parameters["upscale"][2]),
                     }
                 )
 
-            rfc_dict["postprocessing"] = (
-                postprocessing if len(postprocessing) > 0 else None
-            )
+            rfc_dict["postprocessing"] = postprocessing if len(postprocessing) > 0 else None
 
             # semantic drift
             rfc_dict["sampler"] = parameters["sampler_name"]
 
             # 'variations' should always exist and be an array, empty or consisting of {'seed': seed, 'weight': weight} pairs
             variations = []
 
             if "with_variations" in parameters:
-                variations = [
-                    {"seed": x[0], "weight": x[1]}
-                    for x in parameters["with_variations"]
-                ]
+                variations = [{"seed": x[0], "weight": x[1]} for x in parameters["with_variations"]]
 
             rfc_dict["variations"] = variations
 
             if rfc_dict["type"] == "img2img":
                 rfc_dict["strength"] = parameters["strength"]
                 rfc_dict["fit"] = parameters["fit"]  # TODO: Noncompliant
-                rfc_dict["orig_hash"] = calculate_init_img_hash(
-                    self.get_image_path_from_url(parameters["init_img"])
-                )
-                rfc_dict["init_image_path"] = parameters[
-                    "init_img"
-                ]  # TODO: Noncompliant
+                rfc_dict["orig_hash"] = calculate_init_img_hash(self.get_image_path_from_url(parameters["init_img"]))
+                rfc_dict["init_image_path"] = parameters["init_img"]  # TODO: Noncompliant
 
             metadata["image"] = rfc_dict
 
             return metadata
 
         except Exception as e:
             self.handle_exceptions(e)
 
-    def parameters_to_post_processed_image_metadata(
-        self, parameters, original_image_path
-    ):
+    def parameters_to_post_processed_image_metadata(self, parameters, original_image_path):
         try:
             current_metadata = retrieve_metadata(original_image_path)["sd-metadata"]
             postprocessing_metadata = {}
 
             """
             if we don't have an original image metadata to reconstruct,
             need to record the original image and its hash
             """
             if "image" not in current_metadata:
                 current_metadata["image"] = {}
 
-                orig_hash = calculate_init_img_hash(
-                    self.get_image_path_from_url(original_image_path)
-                )
+                orig_hash = calculate_init_img_hash(self.get_image_path_from_url(original_image_path))
 
                 postprocessing_metadata["orig_path"] = (original_image_path,)
                 postprocessing_metadata["orig_hash"] = orig_hash
 
             if parameters["type"] == "esrgan":
                 postprocessing_metadata["type"] = "esrgan"
                 postprocessing_metadata["scale"] = parameters["upscale"][0]
@@ -1469,17 +1343,15 @@
 
             else:
                 raise TypeError(f"Invalid type: {parameters['type']}")
 
             if "postprocessing" in current_metadata["image"] and isinstance(
                 current_metadata["image"]["postprocessing"], list
             ):
-                current_metadata["image"]["postprocessing"].append(
-                    postprocessing_metadata
-                )
+                current_metadata["image"]["postprocessing"].append(postprocessing_metadata)
             else:
                 current_metadata["image"]["postprocessing"] = [postprocessing_metadata]
 
             return current_metadata
 
         except Exception as e:
             self.handle_exceptions(e)
@@ -1552,37 +1424,25 @@
         except Exception as e:
             self.handle_exceptions(e)
 
     def get_image_path_from_url(self, url):
         """Given a url to an image used by the client, returns the absolute file path to that image"""
         try:
             if "init-images" in url:
-                return os.path.abspath(
-                    os.path.join(self.init_image_path, os.path.basename(url))
-                )
+                return os.path.abspath(os.path.join(self.init_image_path, os.path.basename(url)))
             elif "mask-images" in url:
-                return os.path.abspath(
-                    os.path.join(self.mask_image_path, os.path.basename(url))
-                )
+                return os.path.abspath(os.path.join(self.mask_image_path, os.path.basename(url)))
             elif "intermediates" in url:
-                return os.path.abspath(
-                    os.path.join(self.intermediate_path, os.path.basename(url))
-                )
+                return os.path.abspath(os.path.join(self.intermediate_path, os.path.basename(url)))
             elif "temp-images" in url:
-                return os.path.abspath(
-                    os.path.join(self.temp_image_path, os.path.basename(url))
-                )
+                return os.path.abspath(os.path.join(self.temp_image_path, os.path.basename(url)))
             elif "thumbnails" in url:
-                return os.path.abspath(
-                    os.path.join(self.thumbnail_image_path, os.path.basename(url))
-                )
+                return os.path.abspath(os.path.join(self.thumbnail_image_path, os.path.basename(url)))
             else:
-                return os.path.abspath(
-                    os.path.join(self.result_path, os.path.basename(url))
-                )
+                return os.path.abspath(os.path.join(self.result_path, os.path.basename(url)))
         except Exception as e:
             self.handle_exceptions(e)
 
     def get_url_from_image_path(self, path):
         """Given an absolute file path to an image, returns the URL that the client can use to load the image"""
         try:
             if "init-images" in path:
@@ -1628,26 +1488,22 @@
 
 class Progress:
     def __init__(self, generation_parameters=None):
         self.current_step = 1
         self.total_steps = (
             self._calculate_real_steps(
                 steps=generation_parameters["steps"],
-                strength=generation_parameters["strength"]
-                if "strength" in generation_parameters
-                else None,
+                strength=generation_parameters["strength"] if "strength" in generation_parameters else None,
                 has_init_image="init_img" in generation_parameters,
             )
             if generation_parameters
             else 1
         )
         self.current_iteration = 1
-        self.total_iterations = (
-            generation_parameters["iterations"] if generation_parameters else 1
-        )
+        self.total_iterations = generation_parameters["iterations"] if generation_parameters else 1
         self.current_status = "common.statusPreparing"
         self.is_processing = True
         self.current_status_has_steps = False
         self.has_error = False
 
     def set_current_step(self, current_step):
         self.current_step = current_step
@@ -1699,17 +1555,15 @@
         return math.floor(strength * steps) if has_init_image else steps
 
 
 class CanceledException(Exception):
     pass
 
 
-def copy_image_from_bounding_box(
-    image: ImageType, x: int, y: int, width: int, height: int
-) -> ImageType:
+def copy_image_from_bounding_box(image: ImageType, x: int, y: int, width: int, height: int) -> ImageType:
     """
     Returns a copy an image, cropped to a bounding box.
     """
     with image as im:
         bounds = (x, y, x + width, y + height)
         im_cropped = im.crop(bounds)
         return im_cropped
@@ -1736,17 +1590,15 @@
 def image_to_dataURL(image: ImageType, image_format: str = "PNG") -> str:
     """
     Converts an image into a base64 image dataURL.
     """
     buffered = io.BytesIO()
     image.save(buffered, format=image_format)
     mime_type = Image.MIME.get(image_format.upper(), "image/" + image_format.lower())
-    image_base64 = f"data:{mime_type};base64," + base64.b64encode(
-        buffered.getvalue()
-    ).decode("UTF-8")
+    image_base64 = f"data:{mime_type};base64," + base64.b64encode(buffered.getvalue()).decode("UTF-8")
     return image_base64
 
 
 def dataURL_to_bytes(dataURL: str) -> bytes:
     """
     Converts a base64 image dataURL into bytes.
     The dataURL is split on the first comma.
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/web/modules/create_cmd_parser.py` & `InvokeAI-3.0.1rc2/invokeai/backend/web/modules/create_cmd_parser.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/web/modules/get_canvas_generation_mode.py` & `InvokeAI-3.0.1rc2/invokeai/backend/web/modules/get_canvas_generation_mode.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,17 +36,15 @@
 
     # Get alpha from init_img
     init_img_alpha = init_img.split()[-1]
     init_img_alpha_mask = init_img_alpha.convert("L")
     init_img_has_transparency = check_for_any_transparency(init_img)
 
     if init_img_has_transparency:
-        init_img_is_fully_transparent = (
-            True if init_img_alpha_mask.getbbox() is None else False
-        )
+        init_img_is_fully_transparent = True if init_img_alpha_mask.getbbox() is None else False
 
     """
     Mask images are white in areas where no change should be made, black where changes
     should be made.
     """
 
     # Fit the mask to init_img's size and convert it to greyscale
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/web/modules/parameters.py` & `InvokeAI-3.0.1rc2/invokeai/backend/web/modules/parameters.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
     "ddim",
     "ddpm",
     "deis",
     "lms",
     "lms_k",
     "pndm",
     "heun",
-    'heun_k',
+    "heun_k",
     "euler",
     "euler_k",
     "euler_a",
     "kdpm_2",
     "kdpm_2_a",
     "dpmpp_2s",
     "dpmpp_2s_k",
@@ -72,13 +72,11 @@
     if "codeformer_fidelity" in params:
         switches.append(f'-cf {params["codeformer_fidelity"]}')
     if "upscale" in params and params["upscale"]:
         switches.append(f'-U {params["upscale"][0]} {params["upscale"][1]}')
     if "variation_amount" in params and params["variation_amount"] > 0:
         switches.append(f'-v {params["variation_amount"]}')
         if "with_variations" in params:
-            seed_weight_pairs = ",".join(
-                f"{seed}:{weight}" for seed, weight in params["with_variations"]
-            )
+            seed_weight_pairs = ",".join(f"{seed}:{weight}" for seed, weight in params["with_variations"])
             switches.append(f"-V {seed_weight_pairs}")
 
     return " ".join(switches)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/backend/web/modules/parse_seed_weights.py` & `InvokeAI-3.0.1rc2/invokeai/backend/web/modules/parse_seed_weights.py`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/INITIAL_MODELS.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/INITIAL_MODELS.yaml`

 * *Files 2% similar despite different names*

```diff
@@ -19,18 +19,14 @@
 sdxl/main/stable-diffusion-xl-base-1-0:
    description: Stable Diffusion XL base model (12 GB)
    repo_id: stabilityai/stable-diffusion-xl-base-1.0
    recommended: False
 sdxl-refiner/main/stable-diffusion-xl-refiner-1-0:
    description: Stable Diffusion XL refiner model (12 GB)
    repo_id: stabilityai/stable-diffusion-xl-refiner-1.0
-   recommended: False
-sdxl-refiner/main/stable-diffusion-xl-refiner-1-0:
-   description: Stable Diffusion XL refiner model (12 GB)
-   repo_id: stabilityai/stable-diffusion-xl-refiner-1.0
    recommended: false
 sdxl/vae/sdxl-1-0-vae-fix:
    description: Fine tuned version of the SDXL-1.0 VAE
    repo_id: madebyollin/sdxl-vae-fp16-fix
    recommended: true
 sd-1/main/Analog-Diffusion:
    description: An SD-1.5 model trained on diverse analog photographs (2.13 GB)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/models.yaml.example` & `InvokeAI-3.0.1rc2/invokeai/configs/models.yaml.example`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/sd_xl_base.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/sd_xl_base.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/sd_xl_refiner.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/sd_xl_refiner.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-finetune.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-finetune.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-finetune_style.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-finetune_style.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-inference.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-inference.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-inpainting-inference.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-inpainting-inference.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v1-m1-finetune.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v1-m1-finetune.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v2-inference-v.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v2-inference-v.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v2-inference.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v2-inference.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v2-inpainting-inference-v.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v2-inpainting-inference-v.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/configs/stable-diffusion/v2-inpainting-inference.yaml` & `InvokeAI-3.0.1rc2/invokeai/configs/stable-diffusion/v2-inpainting-inference.yaml`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/install/invokeai_update.py` & `InvokeAI-3.0.1rc2/invokeai/frontend/install/invokeai_update.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,66 +1,70 @@
-'''
+"""
 Minimalist updater script. Prompts user for the tag or branch to update to and runs
 pip install <path_to_git_source>.
-'''
+"""
 import os
 import platform
 import pkg_resources
 import psutil
 import requests
 from rich import box, print
 from rich.console import Console, group
 from rich.panel import Panel
 from rich.prompt import Prompt
 from rich.style import Style
 
 from invokeai.version import __version__
 
-INVOKE_AI_SRC="https://github.com/invoke-ai/InvokeAI/archive"
-INVOKE_AI_TAG="https://github.com/invoke-ai/InvokeAI/archive/refs/tags"
-INVOKE_AI_BRANCH="https://github.com/invoke-ai/InvokeAI/archive/refs/heads"
-INVOKE_AI_REL="https://api.github.com/repos/invoke-ai/InvokeAI/releases"
+INVOKE_AI_SRC = "https://github.com/invoke-ai/InvokeAI/archive"
+INVOKE_AI_TAG = "https://github.com/invoke-ai/InvokeAI/archive/refs/tags"
+INVOKE_AI_BRANCH = "https://github.com/invoke-ai/InvokeAI/archive/refs/heads"
+INVOKE_AI_REL = "https://api.github.com/repos/invoke-ai/InvokeAI/releases"
 
 OS = platform.uname().system
 ARCH = platform.uname().machine
 
 if OS == "Windows":
     # Windows terminals look better without a background colour
     console = Console(style=Style(color="grey74"))
 else:
     console = Console(style=Style(color="grey74", bgcolor="grey19"))
 
-def get_versions()->dict:
+
+def get_versions() -> dict:
     return requests.get(url=INVOKE_AI_REL).json()
 
-def invokeai_is_running()->bool:
+
+def invokeai_is_running() -> bool:
     for p in psutil.process_iter():
         try:
             cmdline = p.cmdline()
-            matches = [x for x in cmdline if x.endswith(('invokeai','invokeai.exe'))]
+            matches = [x for x in cmdline if x.endswith(("invokeai", "invokeai.exe"))]
             if matches:
-                print(f':exclamation: [bold red]An InvokeAI instance appears to be running as process {p.pid}[/red bold]')
+                print(
+                    f":exclamation: [bold red]An InvokeAI instance appears to be running as process {p.pid}[/red bold]"
+                )
                 return True
-        except (psutil.AccessDenied,psutil.NoSuchProcess):
+        except (psutil.AccessDenied, psutil.NoSuchProcess):
             continue
     return False
 
+
 def welcome(versions: dict):
-    
     @group()
     def text():
-        yield f'InvokeAI Version: [bold yellow]{__version__}'
-        yield ''
-        yield 'This script will update InvokeAI to the latest release, or to a development version of your choice.'
-        yield ''
-        yield '[bold yellow]Options:'
-        yield f'''[1] Update to the latest official release ([italic]{versions[0]['tag_name']}[/italic])
+        yield f"InvokeAI Version: [bold yellow]{__version__}"
+        yield ""
+        yield "This script will update InvokeAI to the latest release, or to a development version of your choice."
+        yield ""
+        yield "[bold yellow]Options:"
+        yield f"""[1] Update to the latest official release ([italic]{versions[0]['tag_name']}[/italic])
 [2] Update to the bleeding-edge development version ([italic]main[/italic])
 [3] Manually enter the [bold]tag name[/bold] for the version you wish to update to
-[4] Manually enter the [bold]branch name[/bold] for the version you wish to update to'''        
+[4] Manually enter the [bold]branch name[/bold] for the version you wish to update to"""
 
     console.rule()
     print(
         Panel(
             title="[bold wheat1]InvokeAI Updater",
             renderable=text(),
             box=box.DOUBLE,
@@ -68,61 +72,63 @@
             padding=(1, 2),
             style=Style(bgcolor="grey23", color="orange1"),
             subtitle=f"[bold grey39]{OS}-{ARCH}",
         )
     )
     console.line()
 
+
 def get_extras():
-    extras = ''
+    extras = ""
     try:
-        dist = pkg_resources.get_distribution('xformers')
-        extras = '[xformers]'
+        dist = pkg_resources.get_distribution("xformers")
+        extras = "[xformers]"
     except pkg_resources.DistributionNotFound:
         pass
     return extras
 
+
 def main():
     versions = get_versions()
     if invokeai_is_running():
-        print(f':exclamation: [bold red]Please terminate all running instances of InvokeAI before updating.[/red bold]')
-        input('Press any key to continue...')
+        print(f":exclamation: [bold red]Please terminate all running instances of InvokeAI before updating.[/red bold]")
+        input("Press any key to continue...")
         return
 
     welcome(versions)
 
     tag = None
     branch = None
     release = None
-    choice = Prompt.ask('Choice:',choices=['1','2','3','4'],default='1')
-    
-    if choice=='1':
-        release = versions[0]['tag_name']
-    elif choice=='2':
-        release = 'main'
-    elif choice=='3':
-        tag = Prompt.ask('Enter an InvokeAI tag name')
-    elif choice=='4':
-        branch = Prompt.ask('Enter an InvokeAI branch name')
+    choice = Prompt.ask("Choice:", choices=["1", "2", "3", "4"], default="1")
+
+    if choice == "1":
+        release = versions[0]["tag_name"]
+    elif choice == "2":
+        release = "main"
+    elif choice == "3":
+        tag = Prompt.ask("Enter an InvokeAI tag name")
+    elif choice == "4":
+        branch = Prompt.ask("Enter an InvokeAI branch name")
 
     extras = get_extras()
 
-    print(f':crossed_fingers: Upgrading to [yellow]{tag if tag else release}[/yellow]')
+    print(f":crossed_fingers: Upgrading to [yellow]{tag if tag else release}[/yellow]")
     if release:
         cmd = f'pip install "invokeai{extras} @ {INVOKE_AI_SRC}/{release}.zip" --use-pep517 --upgrade'
     elif tag:
         cmd = f'pip install "invokeai{extras} @ {INVOKE_AI_TAG}/{tag}.zip" --use-pep517 --upgrade'
     else:
         cmd = f'pip install "invokeai{extras} @ {INVOKE_AI_BRANCH}/{branch}.zip" --use-pep517 --upgrade'
-    print('')
-    print('')
-    if os.system(cmd)==0:
-        print(f':heavy_check_mark: Upgrade successful')
+    print("")
+    print("")
+    if os.system(cmd) == 0:
+        print(f":heavy_check_mark: Upgrade successful")
     else:
-        print(f':exclamation: [bold red]Upgrade failed[/red bold]')
-    
+        print(f":exclamation: [bold red]Upgrade failed[/red bold]")
+
+
 if __name__ == "__main__":
     try:
         main()
     except KeyboardInterrupt:
         pass
-
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/install/model_install.py` & `InvokeAI-3.0.1rc2/invokeai/frontend/install/model_install.py`

 * *Files 4% similar despite different names*

```diff
@@ -52,371 +52,366 @@
 
 config = InvokeAIAppConfig.get_config()
 logger = InvokeAILogger.getLogger()
 
 # build a table mapping all non-printable characters to None
 # for stripping control characters
 # from https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python
-NOPRINT_TRANS_TABLE = {
-    i: None for i in range(0, sys.maxunicode + 1) if not chr(i).isprintable()
-}
+NOPRINT_TRANS_TABLE = {i: None for i in range(0, sys.maxunicode + 1) if not chr(i).isprintable()}
 
-def make_printable(s:str)->str:
-    '''Replace non-printable characters in a string'''
+
+def make_printable(s: str) -> str:
+    """Replace non-printable characters in a string"""
     return s.translate(NOPRINT_TRANS_TABLE)
 
+
 class addModelsForm(CyclingForm, npyscreen.FormMultiPage):
     # for responsive resizing set to False, but this seems to cause a crash!
     FIX_MINIMUM_SIZE_WHEN_CREATED = True
-    
+
     # for persistence
     current_tab = 0
 
     def __init__(self, parentApp, name, multipage=False, *args, **keywords):
         self.multipage = multipage
         self.subprocess = None
         super().__init__(parentApp=parentApp, name=name, *args, **keywords)
 
     def create(self):
         self.keypress_timeout = 10
         self.counter = 0
         self.subprocess_connection = None
 
         if not config.model_conf_path.exists():
-            with open(config.model_conf_path,'w') as file:
-                print('# InvokeAI model configuration file',file=file)
+            with open(config.model_conf_path, "w") as file:
+                print("# InvokeAI model configuration file", file=file)
         self.installer = ModelInstall(config)
         self.all_models = self.installer.all_models()
         self.starter_models = self.installer.starter_models()
-        self.model_labels = self._get_model_labels()        
+        self.model_labels = self._get_model_labels()
         window_width, window_height = get_terminal_size()
 
         self.nextrely -= 1
         self.add_widget_intelligent(
             npyscreen.FixedText,
             value="Use ctrl-N and ctrl-P to move to the <N>ext and <P>revious fields. Cursor keys navigate, and <space> selects.",
             editable=False,
             color="CAUTION",
         )
         self.nextrely += 1
         self.tabs = self.add_widget_intelligent(
             SingleSelectColumns,
             values=[
-                'STARTER MODELS',
-                'MORE MODELS',
-                'CONTROLNETS',
-                'LORA/LYCORIS',
-                'TEXTUAL INVERSION',
+                "STARTER MODELS",
+                "MORE MODELS",
+                "CONTROLNETS",
+                "LORA/LYCORIS",
+                "TEXTUAL INVERSION",
             ],
             value=[self.current_tab],
-            columns = 5,
-            max_height = 2,
+            columns=5,
+            max_height=2,
             relx=8,
-            scroll_exit = True,
+            scroll_exit=True,
         )
         self.tabs.on_changed = self._toggle_tables
 
         top_of_table = self.nextrely
         self.starter_pipelines = self.add_starter_pipelines()
         bottom_of_table = self.nextrely
 
         self.nextrely = top_of_table
         self.pipeline_models = self.add_pipeline_widgets(
-            model_type=ModelType.Main,
-            window_width=window_width,
-            exclude = self.starter_models
+            model_type=ModelType.Main, window_width=window_width, exclude=self.starter_models
         )
         # self.pipeline_models['autoload_pending'] = True
-        bottom_of_table = max(bottom_of_table,self.nextrely)
+        bottom_of_table = max(bottom_of_table, self.nextrely)
 
         self.nextrely = top_of_table
         self.controlnet_models = self.add_model_widgets(
             model_type=ModelType.ControlNet,
             window_width=window_width,
         )
-        bottom_of_table = max(bottom_of_table,self.nextrely)
+        bottom_of_table = max(bottom_of_table, self.nextrely)
 
         self.nextrely = top_of_table
         self.lora_models = self.add_model_widgets(
             model_type=ModelType.Lora,
             window_width=window_width,
         )
-        bottom_of_table = max(bottom_of_table,self.nextrely)
+        bottom_of_table = max(bottom_of_table, self.nextrely)
 
         self.nextrely = top_of_table
         self.ti_models = self.add_model_widgets(
             model_type=ModelType.TextualInversion,
             window_width=window_width,
         )
-        bottom_of_table = max(bottom_of_table,self.nextrely)
-                
-        self.nextrely = bottom_of_table+1
+        bottom_of_table = max(bottom_of_table, self.nextrely)
+
+        self.nextrely = bottom_of_table + 1
 
         self.monitor = self.add_widget_intelligent(
             BufferBox,
-            name='Log Messages',
+            name="Log Messages",
             editable=False,
-            max_height = 8,
+            max_height=8,
         )
-        
+
         self.nextrely += 1
         done_label = "APPLY CHANGES"
         back_label = "BACK"
         cancel_label = "CANCEL"
         current_position = self.nextrely
         if self.multipage:
             self.back_button = self.add_widget_intelligent(
                 npyscreen.ButtonPress,
                 name=back_label,
                 when_pressed_function=self.on_back,
             )
         else:
             self.nextrely = current_position
             self.cancel_button = self.add_widget_intelligent(
-                npyscreen.ButtonPress,
-                name=cancel_label,
-                when_pressed_function=self.on_cancel
+                npyscreen.ButtonPress, name=cancel_label, when_pressed_function=self.on_cancel
             )
             self.nextrely = current_position
             self.ok_button = self.add_widget_intelligent(
                 npyscreen.ButtonPress,
                 name=done_label,
                 relx=(window_width - len(done_label)) // 2,
-                 when_pressed_function=self.on_execute
+                when_pressed_function=self.on_execute,
             )
 
         label = "APPLY CHANGES & EXIT"
         self.nextrely = current_position
         self.done = self.add_widget_intelligent(
             npyscreen.ButtonPress,
             name=label,
-            relx=window_width-len(label)-15,
+            relx=window_width - len(label) - 15,
             when_pressed_function=self.on_done,
         )
 
         # This restores the selected page on return from an installation
-        for i in range(1,self.current_tab+1):
+        for i in range(1, self.current_tab + 1):
             self.tabs.h_cursor_line_down(1)
         self._toggle_tables([self.current_tab])
 
-    ############# diffusers tab ##########        
-    def add_starter_pipelines(self)->dict[str, npyscreen.widget]:
-        '''Add widgets responsible for selecting diffusers models'''
+    ############# diffusers tab ##########
+    def add_starter_pipelines(self) -> dict[str, npyscreen.widget]:
+        """Add widgets responsible for selecting diffusers models"""
         widgets = dict()
         models = self.all_models
         starters = self.starter_models
         starter_model_labels = self.model_labels
-        
-        self.installed_models = sorted(
-            [x for x in starters if models[x].installed]
-        )
+
+        self.installed_models = sorted([x for x in starters if models[x].installed])
 
         widgets.update(
-            label1 = self.add_widget_intelligent(
+            label1=self.add_widget_intelligent(
                 CenteredTitleText,
                 name="Select from a starter set of Stable Diffusion models from HuggingFace.",
                 editable=False,
                 labelColor="CAUTION",
             )
         )
-        
+
         self.nextrely -= 1
         # if user has already installed some initial models, then don't patronize them
         # by showing more recommendations
-        show_recommended = len(self.installed_models)==0
+        show_recommended = len(self.installed_models) == 0
         keys = [x for x in models.keys() if x in starters]
         widgets.update(
-            models_selected = self.add_widget_intelligent(
+            models_selected=self.add_widget_intelligent(
                 MultiSelectColumns,
                 columns=1,
                 name="Install Starter Models",
                 values=[starter_model_labels[x] for x in keys],
                 value=[
                     keys.index(x)
                     for x in keys
-                    if (show_recommended and models[x].recommended) \
-                    or (x in self.installed_models)
+                    if (show_recommended and models[x].recommended) or (x in self.installed_models)
                 ],
                 max_height=len(starters) + 1,
                 relx=4,
                 scroll_exit=True,
             ),
-            models = keys,
+            models=keys,
         )
 
         self.nextrely += 1
         return widgets
 
     ############# Add a set of model install widgets ########
-    def add_model_widgets(self,
-                          model_type: ModelType,
-                          window_width: int=120,
-                          install_prompt: str=None,
-                          exclude: set=set(),
-                          )->dict[str,npyscreen.widget]:
-        '''Generic code to create model selection widgets'''
+    def add_model_widgets(
+        self,
+        model_type: ModelType,
+        window_width: int = 120,
+        install_prompt: str = None,
+        exclude: set = set(),
+    ) -> dict[str, npyscreen.widget]:
+        """Generic code to create model selection widgets"""
         widgets = dict()
-        model_list = [x for x in self.all_models if self.all_models[x].model_type==model_type and not x in exclude]
+        model_list = [x for x in self.all_models if self.all_models[x].model_type == model_type and not x in exclude]
         model_labels = [self.model_labels[x] for x in model_list]
 
-        show_recommended = len(self.installed_models)==0
+        show_recommended = len(self.installed_models) == 0
         if len(model_list) > 0:
             max_width = max([len(x) for x in model_labels])
-            columns = window_width // (max_width+8)  # 8 characters for "[x] " and padding
-            columns = min(len(model_list),columns) or 1
-            prompt = install_prompt or f"Select the desired {model_type.value.title()} models to install. Unchecked models will be purged from disk."
+            columns = window_width // (max_width + 8)  # 8 characters for "[x] " and padding
+            columns = min(len(model_list), columns) or 1
+            prompt = (
+                install_prompt
+                or f"Select the desired {model_type.value.title()} models to install. Unchecked models will be purged from disk."
+            )
 
             widgets.update(
-                label1 = self.add_widget_intelligent(
+                label1=self.add_widget_intelligent(
                     CenteredTitleText,
                     name=prompt,
                     editable=False,
                     labelColor="CAUTION",
                 )
             )
 
             widgets.update(
-                models_selected = self.add_widget_intelligent(
+                models_selected=self.add_widget_intelligent(
                     MultiSelectColumns,
                     columns=columns,
                     name=f"Install {model_type} Models",
                     values=model_labels,
                     value=[
                         model_list.index(x)
                         for x in model_list
-                        if (show_recommended and self.all_models[x].recommended) \
-                            or self.all_models[x].installed
+                        if (show_recommended and self.all_models[x].recommended) or self.all_models[x].installed
                     ],
-                    max_height=len(model_list)//columns + 1,
+                    max_height=len(model_list) // columns + 1,
                     relx=4,
                     scroll_exit=True,
                 ),
-                models = model_list,
+                models=model_list,
             )
 
         self.nextrely += 1
         widgets.update(
-            download_ids = self.add_widget_intelligent(
+            download_ids=self.add_widget_intelligent(
                 TextBox,
-                name = "Additional URLs, or HuggingFace repo_ids to install (Space separated. Use shift-control-V to paste):",
+                name="Additional URLs, or HuggingFace repo_ids to install (Space separated. Use shift-control-V to paste):",
                 max_height=4,
                 scroll_exit=True,
                 editable=True,
             )
         )
         return widgets
 
     ### Tab for arbitrary diffusers widgets ###
-    def add_pipeline_widgets(self,
-                             model_type: ModelType=ModelType.Main,
-                             window_width: int=120,
-                             **kwargs,
-                             )->dict[str,npyscreen.widget]:
-        '''Similar to add_model_widgets() but adds some additional widgets at the bottom
-        to support the autoload directory'''
+    def add_pipeline_widgets(
+        self,
+        model_type: ModelType = ModelType.Main,
+        window_width: int = 120,
+        **kwargs,
+    ) -> dict[str, npyscreen.widget]:
+        """Similar to add_model_widgets() but adds some additional widgets at the bottom
+        to support the autoload directory"""
         widgets = self.add_model_widgets(
-            model_type = model_type,
-            window_width = window_width,
+            model_type=model_type,
+            window_width=window_width,
             install_prompt=f"Additional {model_type.value.title()} models already installed.",
             **kwargs,
         )
 
         return widgets
 
     def resize(self):
         super().resize()
-        if (s := self.starter_pipelines.get("models_selected")):
+        if s := self.starter_pipelines.get("models_selected"):
             keys = [x for x in self.all_models.keys() if x in self.starter_models]
             s.values = [self.model_labels[x] for x in keys]
 
     def _toggle_tables(self, value=None):
         selected_tab = value[0]
         widgets = [
             self.starter_pipelines,
             self.pipeline_models,
             self.controlnet_models,
             self.lora_models,
             self.ti_models,
         ]
 
         for group in widgets:
-            for k,v in group.items():
+            for k, v in group.items():
                 try:
                     v.hidden = True
                     v.editable = False
                 except:
                     pass
-        for k,v in widgets[selected_tab].items():
+        for k, v in widgets[selected_tab].items():
             try:
                 v.hidden = False
-                if not isinstance(v,(npyscreen.FixedText, npyscreen.TitleFixedText, CenteredTitleText)):
+                if not isinstance(v, (npyscreen.FixedText, npyscreen.TitleFixedText, CenteredTitleText)):
                     v.editable = True
             except:
                 pass
         self.__class__.current_tab = selected_tab  # for persistence
         self.display()
 
-    def _get_model_labels(self) -> dict[str,str]:
+    def _get_model_labels(self) -> dict[str, str]:
         window_width, window_height = get_terminal_size()
         checkbox_width = 4
         spacing_width = 2
-        
+
         models = self.all_models
         label_width = max([len(models[x].name) for x in models])
         description_width = window_width - label_width - checkbox_width - spacing_width
 
         result = dict()
         for x in models.keys():
             description = models[x].description
-            description = description[0 : description_width - 3] + "..." \
-                if description and len(description) > description_width \
-                   else description if description else ''
-            result[x] =  f"%-{label_width}s %s" % (models[x].name, description)
+            description = (
+                description[0 : description_width - 3] + "..."
+                if description and len(description) > description_width
+                else description
+                if description
+                else ""
+            )
+            result[x] = f"%-{label_width}s %s" % (models[x].name, description)
         return result
-            
+
     def _get_columns(self) -> int:
         window_width, window_height = get_terminal_size()
-        cols = (
-            4
-            if window_width > 240
-            else 3
-            if window_width > 160
-            else 2
-            if window_width > 80
-            else 1
-        )
+        cols = 4 if window_width > 240 else 3 if window_width > 160 else 2 if window_width > 80 else 1
         return min(cols, len(self.installed_models))
 
-    def confirm_deletions(self, selections: InstallSelections)->bool:
+    def confirm_deletions(self, selections: InstallSelections) -> bool:
         remove_models = selections.remove_models
         if len(remove_models) > 0:
             mods = "\n".join([ModelManager.parse_key(x)[0] for x in remove_models])
-            return npyscreen.notify_ok_cancel(f"These unchecked models will be deleted from disk. Continue?\n---------\n{mods}")
+            return npyscreen.notify_ok_cancel(
+                f"These unchecked models will be deleted from disk. Continue?\n---------\n{mods}"
+            )
         else:
             return True
 
     def on_execute(self):
         self.marshall_arguments()
         app = self.parentApp
         if not self.confirm_deletions(app.install_selections):
             return
-            
-        self.monitor.entry_widget.buffer(['Processing...'],scroll_end=True)
+
+        self.monitor.entry_widget.buffer(["Processing..."], scroll_end=True)
         self.ok_button.hidden = True
         self.display()
-        
+
         # for communication with the subprocess
         parent_conn, child_conn = Pipe()
         p = Process(
-            target = process_and_execute,
+            target=process_and_execute,
             kwargs=dict(
-                opt = app.program_opts,
-                selections = app.install_selections,
-                conn_out = child_conn,
-            )
+                opt=app.program_opts,
+                selections=app.install_selections,
+                conn_out=child_conn,
+            ),
         )
         p.start()
         child_conn.close()
         self.subprocess_connection = parent_conn
         self.subprocess = p
         app.install_selections = InstallSelections()
         # process_and_execute(app.opt, app.install_selections)
@@ -425,301 +420,312 @@
         self.parentApp.switchFormPrevious()
         self.editing = False
 
     def on_cancel(self):
         self.parentApp.setNextForm(None)
         self.parentApp.user_cancelled = True
         self.editing = False
-        
+
     def on_done(self):
         self.marshall_arguments()
         if not self.confirm_deletions(self.parentApp.install_selections):
             return
         self.parentApp.setNextForm(None)
         self.parentApp.user_cancelled = False
         self.editing = False
-        
+
     ########## This routine monitors the child process that is performing model installation and removal #####
     def while_waiting(self):
-        '''Called during idle periods. Main task is to update the Log Messages box with messages
-        from the child process that does the actual installation/removal'''
+        """Called during idle periods. Main task is to update the Log Messages box with messages
+        from the child process that does the actual installation/removal"""
         c = self.subprocess_connection
         if not c:
             return
-        
+
         monitor_widget = self.monitor.entry_widget
         while c.poll():
             try:
-                data = c.recv_bytes().decode('utf-8')
-                data.strip('\n')
+                data = c.recv_bytes().decode("utf-8")
+                data.strip("\n")
 
                 # processing child is requesting user input to select the
                 # right configuration file
-                if data.startswith('*need v2 config'):
-                    _,model_path,*_ = data.split(":",2)
+                if data.startswith("*need v2 config"):
+                    _, model_path, *_ = data.split(":", 2)
                     self._return_v2_config(model_path)
 
                 # processing child is done
-                elif data=='*done*':
+                elif data == "*done*":
                     self._close_subprocess_and_regenerate_form()
                     break
 
                 # update the log message box
                 else:
-                    data=make_printable(data)
-                    data=data.replace('[A','')
+                    data = make_printable(data)
+                    data = data.replace("[A", "")
                     monitor_widget.buffer(
-                        textwrap.wrap(data,
-                                      width=monitor_widget.width,
-                                      subsequent_indent='   ',
-                                      ),
-                        scroll_end=True
+                        textwrap.wrap(
+                            data,
+                            width=monitor_widget.width,
+                            subsequent_indent="   ",
+                        ),
+                        scroll_end=True,
                     )
                     self.display()
-            except (EOFError,OSError):
+            except (EOFError, OSError):
                 self.subprocess_connection = None
 
-    def _return_v2_config(self,model_path: str):
+    def _return_v2_config(self, model_path: str):
         c = self.subprocess_connection
         model_name = Path(model_path).name
         message = select_stable_diffusion_config_file(model_name=model_name)
-        c.send_bytes(message.encode('utf-8'))
+        c.send_bytes(message.encode("utf-8"))
 
     def _close_subprocess_and_regenerate_form(self):
         app = self.parentApp
         self.subprocess_connection.close()
         self.subprocess_connection = None
-        self.monitor.entry_widget.buffer(['** Action Complete **'])
+        self.monitor.entry_widget.buffer(["** Action Complete **"])
         self.display()
-        
+
         # rebuild the form, saving and restoring some of the fields that need to be preserved.
         saved_messages = self.monitor.entry_widget.values
         # autoload_dir = str(config.root_path / self.pipeline_models['autoload_directory'].value)
         # autoscan = self.pipeline_models['autoscan_on_startup'].value
-        
+
         app.main_form = app.addForm(
-            "MAIN", addModelsForm, name="Install Stable Diffusion Models", multipage=self.multipage,
+            "MAIN",
+            addModelsForm,
+            name="Install Stable Diffusion Models",
+            multipage=self.multipage,
         )
         app.switchForm("MAIN")
-        
+
         app.main_form.monitor.entry_widget.values = saved_messages
-        app.main_form.monitor.entry_widget.buffer([''],scroll_end=True)
+        app.main_form.monitor.entry_widget.buffer([""], scroll_end=True)
         # app.main_form.pipeline_models['autoload_directory'].value = autoload_dir
         # app.main_form.pipeline_models['autoscan_on_startup'].value = autoscan
-        
+
     def marshall_arguments(self):
         """
         Assemble arguments and store as attributes of the application:
         .starter_models: dict of model names to install from INITIAL_CONFIGURE.yaml
                          True  => Install
                          False => Remove
         .scan_directory: Path to a directory of models to scan and import
         .autoscan_on_startup:  True if invokeai should scan and import at startup time
         .import_model_paths:   list of URLs, repo_ids and file paths to import
         """
         selections = self.parentApp.install_selections
         all_models = self.all_models
 
         # Defined models (in INITIAL_CONFIG.yaml or models.yaml) to add/remove
-        ui_sections = [self.starter_pipelines, self.pipeline_models,
-                       self.controlnet_models, self.lora_models, self.ti_models]
+        ui_sections = [
+            self.starter_pipelines,
+            self.pipeline_models,
+            self.controlnet_models,
+            self.lora_models,
+            self.ti_models,
+        ]
         for section in ui_sections:
-            if not 'models_selected' in section:
+            if not "models_selected" in section:
                 continue
-            selected = set([section['models'][x] for x in section['models_selected'].value])
+            selected = set([section["models"][x] for x in section["models_selected"].value])
             models_to_install = [x for x in selected if not self.all_models[x].installed]
-            models_to_remove = [x for x in section['models'] if x not in selected and self.all_models[x].installed]
+            models_to_remove = [x for x in section["models"] if x not in selected and self.all_models[x].installed]
             selections.remove_models.extend(models_to_remove)
-            selections.install_models.extend(all_models[x].path or all_models[x].repo_id \
-                                             for x in models_to_install if all_models[x].path or all_models[x].repo_id)
+            selections.install_models.extend(
+                all_models[x].path or all_models[x].repo_id
+                for x in models_to_install
+                if all_models[x].path or all_models[x].repo_id
+            )
 
         # models located in the 'download_ids" section
         for section in ui_sections:
-            if downloads := section.get('download_ids'):
+            if downloads := section.get("download_ids"):
                 selections.install_models.extend(downloads.value.split())
 
         # load directory and whether to scan on startup
         # if self.parentApp.autoload_pending:
         #     selections.scan_directory = str(config.root_path / self.pipeline_models['autoload_directory'].value)
         #     self.parentApp.autoload_pending = False
         # selections.autoscan_on_startup = self.pipeline_models['autoscan_on_startup'].value
 
+
 class AddModelApplication(npyscreen.NPSAppManaged):
-    def __init__(self,opt):
+    def __init__(self, opt):
         super().__init__()
         self.program_opts = opt
         self.user_cancelled = False
         # self.autoload_pending = True
         self.install_selections = InstallSelections()
 
     def onStart(self):
         npyscreen.setTheme(npyscreen.Themes.DefaultTheme)
         self.main_form = self.addForm(
-            "MAIN", addModelsForm, name="Install Stable Diffusion Models", cycle_widgets=False,
+            "MAIN",
+            addModelsForm,
+            name="Install Stable Diffusion Models",
+            cycle_widgets=False,
         )
 
-class StderrToMessage():
+
+class StderrToMessage:
     def __init__(self, connection: Connection):
         self.connection = connection
 
-    def write(self, data:str):
-        self.connection.send_bytes(data.encode('utf-8'))
+    def write(self, data: str):
+        self.connection.send_bytes(data.encode("utf-8"))
 
     def flush(self):
         pass
 
+
 # --------------------------------------------------------
-def ask_user_for_prediction_type(model_path: Path,
-                                 tui_conn: Connection=None
-                                 )->SchedulerPredictionType:
+def ask_user_for_prediction_type(model_path: Path, tui_conn: Connection = None) -> SchedulerPredictionType:
     if tui_conn:
-        logger.debug('Waiting for user response...')
-        return _ask_user_for_pt_tui(model_path, tui_conn)        
+        logger.debug("Waiting for user response...")
+        return _ask_user_for_pt_tui(model_path, tui_conn)
     else:
         return _ask_user_for_pt_cmdline(model_path)
 
-def _ask_user_for_pt_cmdline(model_path: Path)->SchedulerPredictionType:
+
+def _ask_user_for_pt_cmdline(model_path: Path) -> SchedulerPredictionType:
     choices = [SchedulerPredictionType.Epsilon, SchedulerPredictionType.VPrediction, None]
     print(
-f"""
+        f"""
 Please select the type of the V2 checkpoint named {model_path.name}:
 [1] A model based on Stable Diffusion v2 trained on 512 pixel images (SD-2-base)
 [2] A model based on Stable Diffusion v2 trained on 768 pixel images (SD-2-768)
 [3] Skip this model and come back later.
 """
-        )
+    )
     choice = None
     ok = False
     while not ok:
         try:
-            choice = input('select> ').strip()
-            choice = choices[int(choice)-1]
+            choice = input("select> ").strip()
+            choice = choices[int(choice) - 1]
             ok = True
         except (ValueError, IndexError):
-            print(f'{choice} is not a valid choice')
+            print(f"{choice} is not a valid choice")
         except EOFError:
             return
     return choice
-        
-def _ask_user_for_pt_tui(model_path: Path, tui_conn: Connection)->SchedulerPredictionType:
+
+
+def _ask_user_for_pt_tui(model_path: Path, tui_conn: Connection) -> SchedulerPredictionType:
     try:
-        tui_conn.send_bytes(f'*need v2 config for:{model_path}'.encode('utf-8'))
+        tui_conn.send_bytes(f"*need v2 config for:{model_path}".encode("utf-8"))
         # note that we don't do any status checking here
-        response = tui_conn.recv_bytes().decode('utf-8')
+        response = tui_conn.recv_bytes().decode("utf-8")
         if response is None:
             return None
-        elif response == 'epsilon':
+        elif response == "epsilon":
             return SchedulerPredictionType.epsilon
-        elif response == 'v':
+        elif response == "v":
             return SchedulerPredictionType.VPrediction
-        elif response == 'abort':
-            logger.info('Conversion aborted')
+        elif response == "abort":
+            logger.info("Conversion aborted")
             return None
         else:
             return response
     except:
         return None
-        
+
+
 # --------------------------------------------------------
-def process_and_execute(opt: Namespace,
-                        selections: InstallSelections,
-                        conn_out: Connection=None,
-                        ):
+def process_and_execute(
+    opt: Namespace,
+    selections: InstallSelections,
+    conn_out: Connection = None,
+):
     # set up so that stderr is sent to conn_out
     if conn_out:
         translator = StderrToMessage(conn_out)
         sys.stderr = translator
         sys.stdout = translator
         logger = InvokeAILogger.getLogger()
         logger.handlers.clear()
         logger.addHandler(logging.StreamHandler(translator))
 
-    installer = ModelInstall(config, prediction_type_helper=lambda x: ask_user_for_prediction_type(x,conn_out))
+    installer = ModelInstall(config, prediction_type_helper=lambda x: ask_user_for_prediction_type(x, conn_out))
     installer.install(selections)
 
     if conn_out:
-        conn_out.send_bytes('*done*'.encode('utf-8'))
+        conn_out.send_bytes("*done*".encode("utf-8"))
         conn_out.close()
 
-def do_listings(opt)->bool:
+
+def do_listings(opt) -> bool:
     """List installed models of various sorts, and return
     True if any were requested."""
     model_manager = ModelManager(config.model_conf_path)
-    if opt.list_models == 'diffusers':
+    if opt.list_models == "diffusers":
         print("Diffuser models:")
         model_manager.print_models()
-    elif opt.list_models == 'controlnets':
+    elif opt.list_models == "controlnets":
         print("Installed Controlnet Models:")
         cnm = model_manager.list_controlnet_models()
-        print(textwrap.indent("\n".join([x for x in cnm if cnm[x]]),prefix='   '))
-    elif opt.list_models == 'loras':
+        print(textwrap.indent("\n".join([x for x in cnm if cnm[x]]), prefix="   "))
+    elif opt.list_models == "loras":
         print("Installed LoRA/LyCORIS Models:")
         cnm = model_manager.list_lora_models()
-        print(textwrap.indent("\n".join([x for x in cnm if cnm[x]]),prefix='   '))
-    elif opt.list_models == 'tis':
+        print(textwrap.indent("\n".join([x for x in cnm if cnm[x]]), prefix="   "))
+    elif opt.list_models == "tis":
         print("Installed Textual Inversion Embeddings:")
         cnm = model_manager.list_ti_models()
-        print(textwrap.indent("\n".join([x for x in cnm if cnm[x]]),prefix='   '))
+        print(textwrap.indent("\n".join([x for x in cnm if cnm[x]]), prefix="   "))
     else:
         return False
     return True
 
+
 # --------------------------------------------------------
 def select_and_download_models(opt: Namespace):
-    precision = (
-        "float32"
-        if opt.full_precision
-        else choose_precision(torch.device(choose_torch_device()))
-    )
+    precision = "float32" if opt.full_precision else choose_precision(torch.device(choose_torch_device()))
     config.precision = precision
     helper = lambda x: ask_user_for_prediction_type(x)
     # if do_listings(opt):
     # pass
-    
+
     installer = ModelInstall(config, prediction_type_helper=helper)
     if opt.list_models:
         installer.list_models(opt.list_models)
     elif opt.add or opt.delete:
-        selections = InstallSelections(
-            install_models = opt.add or [],
-            remove_models = opt.delete or []
-        )
+        selections = InstallSelections(install_models=opt.add or [], remove_models=opt.delete or [])
         installer.install(selections)
     elif opt.default_only:
-        selections = InstallSelections(
-            install_models = installer.default_model()
-        )
+        selections = InstallSelections(install_models=installer.default_model())
         installer.install(selections)
     elif opt.yes_to_all:
-        selections = InstallSelections(
-            install_models = installer.recommended_models()
-        )
+        selections = InstallSelections(install_models=installer.recommended_models())
         installer.install(selections)
 
     # this is where the TUI is called
     else:
         # needed to support the probe() method running under a subprocess
         torch.multiprocessing.set_start_method("spawn")
 
         # the third argument is needed in the Windows 11 environment in
         # order to launch and resize a console window running this program
         set_min_terminal_size(MIN_COLS, MIN_LINES)
         installApp = AddModelApplication(opt)
         try:
             installApp.run()
         except KeyboardInterrupt as e:
-            if hasattr(installApp,'main_form'):
-                if installApp.main_form.subprocess \
-                   and installApp.main_form.subprocess.is_alive():
-                    logger.info('Terminating subprocesses')
+            if hasattr(installApp, "main_form"):
+                if installApp.main_form.subprocess and installApp.main_form.subprocess.is_alive():
+                    logger.info("Terminating subprocesses")
                     installApp.main_form.subprocess.terminate()
                     installApp.main_form.subprocess = None
             raise e
         process_and_execute(opt, installApp.install_selections)
 
+
 # -------------------------------------
 def main():
     parser = argparse.ArgumentParser(description="InvokeAI model downloader")
     parser.add_argument(
         "--add",
         nargs="*",
         help="List of URLs, local paths or repo_ids of models to install",
@@ -766,27 +772,25 @@
         "--root_dir",
         dest="root",
         type=str,
         default=None,
         help="path to root of install directory",
     )
     opt = parser.parse_args()
-    
+
     invoke_args = []
     if opt.root:
-        invoke_args.extend(['--root',opt.root])
+        invoke_args.extend(["--root", opt.root])
     if opt.full_precision:
-        invoke_args.extend(['--precision','float32'])
+        invoke_args.extend(["--precision", "float32"])
     config.parse_args(invoke_args)
     logger = InvokeAILogger().getLogger(config=config)
 
     if not config.model_conf_path.exists():
-        logger.info(
-            "Your InvokeAI root directory is not set up. Calling invokeai-configure."
-        )
+        logger.info("Your InvokeAI root directory is not set up. Calling invokeai-configure.")
         from invokeai.frontend.install import invokeai_configure
 
         invokeai_configure()
         sys.exit(0)
 
     try:
         select_and_download_models(opt)
@@ -796,25 +800,23 @@
     except KeyboardInterrupt:
         curses.nocbreak()
         curses.echo()
         curses.endwin()
         logger.info("Goodbye! Come back soon.")
     except widget.NotEnoughSpaceForWidget as e:
         if str(e).startswith("Height of 1 allocated"):
-            logger.error(
-                "Insufficient vertical space for the interface. Please make your window taller and try again"
-            )
-        input('Press any key to continue...')
+            logger.error("Insufficient vertical space for the interface. Please make your window taller and try again")
+        input("Press any key to continue...")
     except Exception as e:
         if str(e).startswith("addwstr"):
             logger.error(
                 "Insufficient horizontal space for the interface. Please make your window wider and try again."
             )
         else:
-            print(f'An exception has occurred: {str(e)} Details:')
+            print(f"An exception has occurred: {str(e)} Details:")
             print(traceback.format_exc(), file=sys.stderr)
-        input('Press any key to continue...')
-    
+        input("Press any key to continue...")
+
 
 # -------------------------------------
 if __name__ == "__main__":
     main()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/install/widgets.py` & `InvokeAI-3.0.1rc2/invokeai/frontend/install/widgets.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,109 +10,116 @@
 import subprocess
 import sys
 import npyscreen
 import textwrap
 import npyscreen.wgmultiline as wgmultiline
 from npyscreen import fmPopup
 from shutil import get_terminal_size
-from curses import BUTTON2_CLICKED,BUTTON3_CLICKED
+from curses import BUTTON2_CLICKED, BUTTON3_CLICKED
 
 # minimum size for UIs
 MIN_COLS = 130
 MIN_LINES = 38
 
+
 # -------------------------------------
 def set_terminal_size(columns: int, lines: int):
     ts = get_terminal_size()
-    width = max(columns,ts.columns)
-    height = max(lines,ts.lines)
+    width = max(columns, ts.columns)
+    height = max(lines, ts.lines)
 
     OS = platform.uname().system
     if OS == "Windows":
         pass
         # not working reliably - ask user to adjust the window
-        #_set_terminal_size_powershell(width,height)
+        # _set_terminal_size_powershell(width,height)
     elif OS in ["Darwin", "Linux"]:
-        _set_terminal_size_unix(width,height)
+        _set_terminal_size_unix(width, height)
 
     # check whether it worked....
     ts = get_terminal_size()
     pause = False
     if ts.columns < columns:
-        print('\033[1mThis window is too narrow for the user interface.\033[0m')
+        print("\033[1mThis window is too narrow for the user interface.\033[0m")
         pause = True
     if ts.lines < lines:
-        print('\033[1mThis window is too short for the user interface.\033[0m')
+        print("\033[1mThis window is too short for the user interface.\033[0m")
         pause = True
     if pause:
-        input('Maximize the window then press any key to continue..')
+        input("Maximize the window then press any key to continue..")
+
 
 def _set_terminal_size_powershell(width: int, height: int):
-    script=f'''
+    script = f"""
 $pshost = get-host
 $pswindow = $pshost.ui.rawui
 $newsize = $pswindow.buffersize
 $newsize.height = 3000
 $newsize.width = {width}
 $pswindow.buffersize = $newsize
 $newsize = $pswindow.windowsize
 $newsize.height = {height}
 $newsize.width = {width}
 $pswindow.windowsize = $newsize
-'''
-    subprocess.run(["powershell","-Command","-"],input=script,text=True)
+"""
+    subprocess.run(["powershell", "-Command", "-"], input=script, text=True)
+
 
 def _set_terminal_size_unix(width: int, height: int):
     import fcntl
     import termios
 
     # These terminals accept the size command and report that the
     # size changed, but they lie!!!
-    for bad_terminal in ['TERMINATOR_UUID', 'ALACRITTY_WINDOW_ID']:
+    for bad_terminal in ["TERMINATOR_UUID", "ALACRITTY_WINDOW_ID"]:
         if os.environ.get(bad_terminal):
             return
-    
+
     winsize = struct.pack("HHHH", height, width, 0, 0)
     fcntl.ioctl(sys.stdout.fileno(), termios.TIOCSWINSZ, winsize)
     sys.stdout.write("\x1b[8;{height};{width}t".format(height=height, width=width))
     sys.stdout.flush()
 
+
 def set_min_terminal_size(min_cols: int, min_lines: int):
     # make sure there's enough room for the ui
     term_cols, term_lines = get_terminal_size()
     if term_cols >= min_cols and term_lines >= min_lines:
         return
     cols = max(term_cols, min_cols)
     lines = max(term_lines, min_lines)
     set_terminal_size(cols, lines)
 
+
 class IntSlider(npyscreen.Slider):
     def translate_value(self):
         stri = "%2d / %2d" % (self.value, self.out_of)
         l = (len(str(self.out_of))) * 2 + 4
         stri = stri.rjust(l)
         return stri
 
+
 # -------------------------------------
 # fix npyscreen form so that cursor wraps both forward and backward
 class CyclingForm(object):
     def find_previous_editable(self, *args):
         done = False
-        n = self.editw-1
+        n = self.editw - 1
         while not done:
             if self._widgets__[n].editable and not self._widgets__[n].hidden:
                 self.editw = n
                 done = True
             n -= 1
-            if n<0:
+            if n < 0:
                 if self.cycle_widgets:
-                    n = len(self._widgets__)-1
+                    n = len(self._widgets__) - 1
                 else:
                     done = True
-                    
+
+
 # -------------------------------------
 class CenteredTitleText(npyscreen.TitleText):
     def __init__(self, *args, **keywords):
         super().__init__(*args, **keywords)
         self.resize()
 
     def resize(self):
@@ -155,15 +162,16 @@
         stri = stri.rjust(l)
         return stri
 
 
 class FloatTitleSlider(npyscreen.TitleText):
     _entry_type = FloatSlider
 
-class SelectColumnBase():
+
+class SelectColumnBase:
     def make_contained_widgets(self):
         self._my_widgets = []
         column_width = self.width // self.columns
         for h in range(self.value_cnt):
             self._my_widgets.append(
                 self._contained_widgets(
                     self.parent,
@@ -213,97 +221,102 @@
         mouse_id, rel_x, rel_y, z, bstate = self.interpret_mouse_event(mouse_event)
         column_width = self.width // self.columns
         column_height = math.ceil(self.value_cnt / self.columns)
         column_no = rel_x // column_width
         row_no = rel_y // self._contained_widget_height
         self.cursor_line = column_no * column_height + row_no
         if bstate & curses.BUTTON1_DOUBLE_CLICKED:
-            if hasattr(self,'on_mouse_double_click'):
+            if hasattr(self, "on_mouse_double_click"):
                 self.on_mouse_double_click(self.cursor_line)
         self.display()
 
-class MultiSelectColumns( SelectColumnBase, npyscreen.MultiSelect):
+
+class MultiSelectColumns(SelectColumnBase, npyscreen.MultiSelect):
     def __init__(self, screen, columns: int = 1, values: list = [], **keywords):
         self.columns = columns
         self.value_cnt = len(values)
         self.rows = math.ceil(self.value_cnt / self.columns)
         super().__init__(screen, values=values, **keywords)
 
     def on_mouse_double_click(self, cursor_line):
         self.h_select_toggle(cursor_line)
 
+
 class SingleSelectWithChanged(npyscreen.SelectOne):
-    def __init__(self,*args,**kwargs):
-        super().__init__(*args,**kwargs)
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
 
-    def h_select(self,ch):
+    def h_select(self, ch):
         super().h_select(ch)
         if self.on_changed:
             self.on_changed(self.value)
 
+
 class SingleSelectColumns(SelectColumnBase, SingleSelectWithChanged):
     def __init__(self, screen, columns: int = 1, values: list = [], **keywords):
         self.columns = columns
         self.value_cnt = len(values)
         self.rows = math.ceil(self.value_cnt / self.columns)
         self.on_changed = None
         super().__init__(screen, values=values, **keywords)
 
     def when_value_edited(self):
         self.h_select(self.cursor_line)
 
     def when_cursor_moved(self):
         self.h_select(self.cursor_line)
 
-    def h_cursor_line_right(self,ch):
-        self.h_exit_down('bye bye')
+    def h_cursor_line_right(self, ch):
+        self.h_exit_down("bye bye")
 
-class TextBoxInner(npyscreen.MultiLineEdit):
 
-    def __init__(self,*args,**kwargs):
-        super().__init__(*args,**kwargs)
+class TextBoxInner(npyscreen.MultiLineEdit):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
         self.yank = None
-        self.handlers.update({
-            "^A": self.h_cursor_to_start,
-            "^E": self.h_cursor_to_end,
-            "^K": self.h_kill,
-            "^F": self.h_cursor_right,
-            "^B": self.h_cursor_left,
-            "^Y": self.h_yank,
-            "^V": self.h_paste,
-        })
+        self.handlers.update(
+            {
+                "^A": self.h_cursor_to_start,
+                "^E": self.h_cursor_to_end,
+                "^K": self.h_kill,
+                "^F": self.h_cursor_right,
+                "^B": self.h_cursor_left,
+                "^Y": self.h_yank,
+                "^V": self.h_paste,
+            }
+        )
 
     def h_cursor_to_start(self, input):
         self.cursor_position = 0
 
     def h_cursor_to_end(self, input):
         self.cursor_position = len(self.value)
 
     def h_kill(self, input):
-        self.yank = self.value[self.cursor_position:]
-        self.value = self.value[:self.cursor_position]
+        self.yank = self.value[self.cursor_position :]
+        self.value = self.value[: self.cursor_position]
 
     def h_yank(self, input):
         if self.yank:
             self.paste(self.yank)
 
     def paste(self, text: str):
-        self.value = self.value[:self.cursor_position] + text + self.value[self.cursor_position:]
+        self.value = self.value[: self.cursor_position] + text + self.value[self.cursor_position :]
         self.cursor_position += len(text)
 
-    def h_paste(self, input: int=0):
+    def h_paste(self, input: int = 0):
         try:
             text = pyperclip.paste()
         except ModuleNotFoundError:
             text = "To paste with the mouse on Linux, please install the 'xclip' program."
         self.paste(text)
-        
+
     def handle_mouse_event(self, mouse_event):
         mouse_id, rel_x, rel_y, z, bstate = self.interpret_mouse_event(mouse_event)
-        if bstate & (BUTTON2_CLICKED|BUTTON3_CLICKED):
+        if bstate & (BUTTON2_CLICKED | BUTTON3_CLICKED):
             self.h_paste()
 
     # def update(self, clear=True):
     #     if clear:
     #         self.clear()
 
     #     HEIGHT = self.height
@@ -316,116 +329,126 @@
     #     self.parent.curses_pad.vline(
     #         self.rely, self.relx, curses.ACS_VLINE, self.height
     #     )
     #     self.parent.curses_pad.vline(
     #         self.rely, self.relx + WIDTH, curses.ACS_VLINE, HEIGHT
     #     )
 
-        # # draw corners
-        # self.parent.curses_pad.addch(
-        #     self.rely,
-        #     self.relx,
-        #     curses.ACS_ULCORNER,
-        # )
-        # self.parent.curses_pad.addch(
-        #     self.rely,
-        #     self.relx + WIDTH,
-        #     curses.ACS_URCORNER,
-        # )
-        # self.parent.curses_pad.addch(
-        #     self.rely + HEIGHT,
-        #     self.relx,
-        #     curses.ACS_LLCORNER,
-        # )
-        # self.parent.curses_pad.addch(
-        #     self.rely + HEIGHT,
-        #     self.relx + WIDTH,
-        #     curses.ACS_LRCORNER,
-        # )
-
-        # # fool our superclass into thinking drawing area is smaller - this is really hacky but it seems to work
-        # (relx, rely, height, width) = (self.relx, self.rely, self.height, self.width)
-        # self.relx += 1
-        # self.rely += 1
-        # self.height -= 1
-        # self.width -= 1
-        # super().update(clear=False)
-        # (self.relx, self.rely, self.height, self.width) = (relx, rely, height, width)
+    # # draw corners
+    # self.parent.curses_pad.addch(
+    #     self.rely,
+    #     self.relx,
+    #     curses.ACS_ULCORNER,
+    # )
+    # self.parent.curses_pad.addch(
+    #     self.rely,
+    #     self.relx + WIDTH,
+    #     curses.ACS_URCORNER,
+    # )
+    # self.parent.curses_pad.addch(
+    #     self.rely + HEIGHT,
+    #     self.relx,
+    #     curses.ACS_LLCORNER,
+    # )
+    # self.parent.curses_pad.addch(
+    #     self.rely + HEIGHT,
+    #     self.relx + WIDTH,
+    #     curses.ACS_LRCORNER,
+    # )
+
+    # # fool our superclass into thinking drawing area is smaller - this is really hacky but it seems to work
+    # (relx, rely, height, width) = (self.relx, self.rely, self.height, self.width)
+    # self.relx += 1
+    # self.rely += 1
+    # self.height -= 1
+    # self.width -= 1
+    # super().update(clear=False)
+    # (self.relx, self.rely, self.height, self.width) = (relx, rely, height, width)
+
 
 class TextBox(npyscreen.BoxTitle):
     _contained_widget = TextBoxInner
 
+
 class BufferBox(npyscreen.BoxTitle):
     _contained_widget = npyscreen.BufferPager
 
+
 class ConfirmCancelPopup(fmPopup.ActionPopup):
     DEFAULT_COLUMNS = 100
+
     def on_ok(self):
         self.value = True
+
     def on_cancel(self):
         self.value = False
-        
+
+
 class FileBox(npyscreen.BoxTitle):
     _contained_widget = npyscreen.Filename
-    
+
+
 class PrettyTextBox(npyscreen.BoxTitle):
     _contained_widget = TextBox
-    
+
+
 def _wrap_message_lines(message, line_length):
     lines = []
-    for line in message.split('\n'):
+    for line in message.split("\n"):
         lines.extend(textwrap.wrap(line.rstrip(), line_length))
     return lines
-    
+
+
 def _prepare_message(message):
     if isinstance(message, list) or isinstance(message, tuple):
-        return "\n".join([ s.rstrip() for s in message])
-        #return "\n".join(message)
+        return "\n".join([s.rstrip() for s in message])
+        # return "\n".join(message)
     else:
         return message
-    
+
+
 def select_stable_diffusion_config_file(
-        form_color: str='DANGER',
-        wrap:bool =True,
-        model_name:str='Unknown',
+    form_color: str = "DANGER",
+    wrap: bool = True,
+    model_name: str = "Unknown",
 ):
     message = f"Please select the correct base model for the V2 checkpoint named '{model_name}'. Press <CANCEL> to skip installation."
     title = "CONFIG FILE SELECTION"
-    options=[
+    options = [
         "An SD v2.x base model (512 pixels; no 'parameterization:' line in its yaml file)",
         "An SD v2.x v-predictive model (768 pixels; 'parameterization: \"v\"' line in its yaml file)",
         "Skip installation for now and come back later",
     ]
 
     F = ConfirmCancelPopup(
         name=title,
         color=form_color,
         cycle_widgets=True,
         lines=16,
     )
     F.preserve_selected_widget = True
-    
+
     mlw = F.add(
         wgmultiline.Pager,
         max_height=4,
         editable=False,
     )
-    mlw_width = mlw.width-1
+    mlw_width = mlw.width - 1
     if wrap:
         message = _wrap_message_lines(message, mlw_width)
     mlw.values = message
 
     choice = F.add(
         npyscreen.SelectOne,
-        values = options,
-        value = [0],
-        max_height = len(options)+1,
+        values=options,
+        value=[0],
+        max_height=len(options) + 1,
         scroll_exit=True,
     )
 
     F.editw = 1
     F.edit()
     if not F.value:
         return None
-    assert choice.value[0] in range(0,3),'invalid choice'
-    choices = ['epsilon','v','abort']
+    assert choice.value[0] in range(0, 3), "invalid choice"
+    choices = ["epsilon", "v", "abort"]
     return choices[choice.value[0]]
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/merge/merge_diffusers.py` & `InvokeAI-3.0.1rc2/invokeai/frontend/merge/merge_diffusers.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,21 +16,25 @@
 from diffusers import logging as dlogging
 from npyscreen import widget
 from omegaconf import OmegaConf
 
 import invokeai.backend.util.logging as logger
 from invokeai.app.services.config import InvokeAIAppConfig
 from invokeai.backend.model_management import (
-    ModelMerger, MergeInterpolationMethod,
-    ModelManager, ModelType, BaseModelType,
+    ModelMerger,
+    MergeInterpolationMethod,
+    ModelManager,
+    ModelType,
+    BaseModelType,
 )
 from invokeai.frontend.install.widgets import FloatTitleSlider, TextBox, SingleSelectColumns
 
 config = InvokeAIAppConfig.get_config()
 
+
 def _parse_args() -> Namespace:
     parser = argparse.ArgumentParser(description="InvokeAI model merging")
     parser.add_argument(
         "--root_dir",
         type=Path,
         default=config.root,
         help="Path to the invokeai runtime directory",
@@ -130,22 +134,22 @@
             value="Use up and down arrows to move, <space> to select an item, <tab> and <shift-tab> to move from one field to the next.",
             editable=False,
         )
         self.nextrely += 1
         self.base_select = self.add_widget_intelligent(
             SingleSelectColumns,
             values=[
-                'Models Built on SD-1.x',
-                'Models Built on SD-2.x',
+                "Models Built on SD-1.x",
+                "Models Built on SD-2.x",
             ],
             value=[self.current_base],
-            columns = 4,
-            max_height = 2,
+            columns=4,
+            max_height=2,
             relx=8,
-            scroll_exit = True,
+            scroll_exit=True,
         )
         self.base_select.on_changed = self._populate_models
         self.add_widget_intelligent(
             npyscreen.FixedText,
             value="MODEL 1",
             color="GOOD",
             editable=False,
@@ -296,54 +300,51 @@
             return npyscreen.notify_yes_no(
                 f"The chosen merged model destination, {model_out}, is already in use. Overwrite?"
             )
 
     def validate_field_values(self) -> bool:
         bad_fields = []
         model_names = self.model_names
-        selected_models = set(
-            (model_names[self.model1.value[0]], model_names[self.model2.value[0]])
-        )
+        selected_models = set((model_names[self.model1.value[0]], model_names[self.model2.value[0]]))
         if self.model3.value[0] > 0:
             selected_models.add(model_names[self.model3.value[0] - 1])
         if len(selected_models) < 2:
-            bad_fields.append(
-                f"Please select two or three DIFFERENT models to compare. You selected {selected_models}"
-            )
+            bad_fields.append(f"Please select two or three DIFFERENT models to compare. You selected {selected_models}")
         if len(bad_fields) > 0:
             message = "The following problems were detected and must be corrected:"
             for problem in bad_fields:
                 message += f"\n* {problem}"
             npyscreen.notify_confirm(message)
             return False
         else:
             return True
 
-    def get_model_names(self, base_model: BaseModelType=None) -> List[str]:
+    def get_model_names(self, base_model: BaseModelType = None) -> List[str]:
         model_names = [
             info["name"]
             for info in self.model_manager.list_models(model_type=ModelType.Main, base_model=base_model)
             if info["model_format"] == "diffusers"
         ]
         return sorted(model_names)
 
-    def _populate_models(self,value=None):
+    def _populate_models(self, value=None):
         base_model = tuple(BaseModelType)[value[0]]
         self.model_names = self.get_model_names(base_model)
-        
+
         models_plus_none = self.model_names.copy()
         models_plus_none.insert(0, "None")
         self.model1.values = self.model_names
         self.model2.values = self.model_names
         self.model3.values = models_plus_none
-                  
+
         self.display()
 
+
 class Mergeapp(npyscreen.NPSAppManaged):
-    def __init__(self, model_manager:ModelManager):
+    def __init__(self, model_manager: ModelManager):
         super().__init__()
         self.model_manager = model_manager
 
     def onStart(self):
         npyscreen.setTheme(npyscreen.Themes.ElegantTheme)
         self.main = self.addForm("MAIN", mergeModelsForm, name="Merge Models Settings")
 
@@ -363,46 +364,40 @@
     assert args.alpha >= 0 and args.alpha <= 1.0, "alpha must be between 0 and 1"
     assert (
         args.model_names and len(args.model_names) >= 1 and len(args.model_names) <= 3
     ), "Please provide the --models argument to list 2 to 3 models to merge. Use --help for full usage."
 
     if not args.merged_model_name:
         args.merged_model_name = "+".join(args.model_names)
-        logger.info(
-            f'No --merged_model_name provided. Defaulting to "{args.merged_model_name}"'
-        )
+        logger.info(f'No --merged_model_name provided. Defaulting to "{args.merged_model_name}"')
 
     model_manager = ModelManager(config.model_conf_path)
     assert (
         not model_manager.model_exists(args.merged_model_name, args.base_model, ModelType.Main) or args.clobber
     ), f'A model named "{args.merged_model_name}" already exists. Use --clobber to overwrite.'
 
     merger = ModelMerger(model_manager)
     merger.merge_diffusion_models_and_save(**vars(args))
     logger.info(f'Models merged into new model: "{args.merged_model_name}".')
 
 
 def main():
     args = _parse_args()
-    config.parse_args(['--root',str(args.root_dir)])
+    config.parse_args(["--root", str(args.root_dir)])
 
     try:
         if args.front_end:
             run_gui(args)
         else:
             run_cli(args)
     except widget.NotEnoughSpaceForWidget as e:
         if str(e).startswith("Height of 1 allocated"):
-            logger.error(
-                "You need to have at least two diffusers models defined in models.yaml in order to merge"
-            )
+            logger.error("You need to have at least two diffusers models defined in models.yaml in order to merge")
         else:
-            logger.error(
-                "Not enough room for the user interface. Try making this window larger."
-            )
+            logger.error("Not enough room for the user interface. Try making this window larger.")
         sys.exit(-1)
     except Exception as e:
         logger.error(e)
         sys.exit(-1)
     except KeyboardInterrupt:
         sys.exit(-1)
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/training/textual_inversion.py` & `InvokeAI-3.0.1rc2/invokeai/frontend/training/textual_inversion.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,24 +19,22 @@
 import npyscreen
 from npyscreen import widget
 from omegaconf import OmegaConf
 
 import invokeai.backend.util.logging as logger
 
 from invokeai.app.services.config import InvokeAIAppConfig
-from ...backend.training import (
-    do_textual_inversion_training,
-    parse_args
-)
+from ...backend.training import do_textual_inversion_training, parse_args
 
 TRAINING_DATA = "text-inversion-training-data"
 TRAINING_DIR = "text-inversion-output"
 CONF_FILE = "preferences.conf"
 config = None
 
+
 class textualInversionForm(npyscreen.FormMultiPageAction):
     resolutions = [512, 768, 1024]
     lr_schedulers = [
         "linear",
         "cosine",
         "cosine_with_restarts",
         "polynomial",
@@ -107,17 +105,15 @@
             value=False,
             scroll_exit=True,
         )
         self.learnable_property = self.add_widget_intelligent(
             npyscreen.TitleSelectOne,
             name="Learnable property:",
             values=self.learnable_properties,
-            value=self.learnable_properties.index(
-                saved_args.get("learnable_property", "object")
-            ),
+            value=self.learnable_properties.index(saved_args.get("learnable_property", "object")),
             max_height=4,
             scroll_exit=True,
         )
         self.train_data_dir = self.add_widget_intelligent(
             npyscreen.TitleFilename,
             name="Data Training Directory:",
             select_dir=True,
@@ -239,44 +235,36 @@
             scroll_exit=True,
         )
         self.model.editing = True
 
     def initializer_changed(self):
         placeholder = self.placeholder_token.value
         self.prompt_token.value = f"(Trigger by using <{placeholder}> in your prompts)"
-        self.train_data_dir.value = str(
-            config.root_dir / TRAINING_DATA / placeholder
-        )
+        self.train_data_dir.value = str(config.root_dir / TRAINING_DATA / placeholder)
         self.output_dir.value = str(config.root_dir / TRAINING_DIR / placeholder)
         self.resume_from_checkpoint.value = Path(self.output_dir.value).exists()
 
     def on_ok(self):
         if self.validate_field_values():
             self.parentApp.setNextForm(None)
             self.editing = False
             self.parentApp.ti_arguments = self.marshall_arguments()
-            npyscreen.notify(
-                "Launching textual inversion training. This will take a while..."
-            )
+            npyscreen.notify("Launching textual inversion training. This will take a while...")
         else:
             self.editing = True
 
     def ok_cancel(self):
         sys.exit(0)
 
     def validate_field_values(self) -> bool:
         bad_fields = []
         if self.model.value is None:
-            bad_fields.append(
-                "Model Name must correspond to a known model in models.yaml"
-            )
+            bad_fields.append("Model Name must correspond to a known model in models.yaml")
         if not re.match("^[a-zA-Z0-9.-]+$", self.placeholder_token.value):
-            bad_fields.append(
-                "Trigger term must only contain alphanumeric characters, the dot and hyphen"
-            )
+            bad_fields.append("Trigger term must only contain alphanumeric characters, the dot and hyphen")
         if self.train_data_dir.value is None:
             bad_fields.append("Data Training Directory cannot be empty")
         if self.output_dir.value is None:
             bad_fields.append("The Output Destination Directory cannot be empty")
         if len(bad_fields) > 0:
             message = "The following problems were detected and must be corrected:"
             for problem in bad_fields:
@@ -284,39 +272,29 @@
             npyscreen.notify_confirm(message)
             return False
         else:
             return True
 
     def get_model_names(self) -> Tuple[List[str], int]:
         conf = OmegaConf.load(config.root_dir / "configs/models.yaml")
-        model_names = [
-            idx
-            for idx in sorted(list(conf.keys()))
-            if conf[idx].get("format", None) == "diffusers"
-        ]
-        defaults = [
-            idx
-            for idx in range(len(model_names))
-            if "default" in conf[model_names[idx]]
-        ]
+        model_names = [idx for idx in sorted(list(conf.keys())) if conf[idx].get("format", None) == "diffusers"]
+        defaults = [idx for idx in range(len(model_names)) if "default" in conf[model_names[idx]]]
         default = defaults[0] if len(defaults) > 0 else 0
         return (model_names, default)
 
     def marshall_arguments(self) -> dict:
         args = dict()
 
         # the choices
         args.update(
             model=self.model_names[self.model.value[0]],
             resolution=self.resolutions[self.resolution.value[0]],
             lr_scheduler=self.lr_schedulers[self.lr_scheduler.value[0]],
             mixed_precision=self.precisions[self.mixed_precision.value[0]],
-            learnable_property=self.learnable_properties[
-                self.learnable_property.value[0]
-            ],
+            learnable_property=self.learnable_properties[self.learnable_property.value[0]],
         )
 
         # all the strings and booleans
         for attr in (
             "initializer_token",
             "placeholder_token",
             "train_data_dir",
@@ -370,17 +348,15 @@
     """
     source = Path(args["output_dir"], "learned_embeds.bin")
     dest_dir_name = args["placeholder_token"].strip("<>")
     destination = config.root_dir / "embeddings" / dest_dir_name
     os.makedirs(destination, exist_ok=True)
     logger.info(f"Training completed. Copying learned_embeds.bin into {str(destination)}")
     shutil.copy(source, destination)
-    if (
-        input("Delete training logs and intermediate checkpoints? [y] ") or "y"
-    ).startswith(("y", "Y")):
+    if (input("Delete training logs and intermediate checkpoints? [y] ") or "y").startswith(("y", "Y")):
         shutil.rmtree(Path(args["output_dir"]))
     else:
         logger.info(f'Keeping {args["output_dir"]}')
 
 
 def save_args(args: dict):
     """
@@ -419,52 +395,48 @@
         if not re.match("^<.+>$", args["placeholder_token"]):
             args["placeholder_token"] = f"<{args['placeholder_token']}>"
 
         args["only_save_embeds"] = True
         save_args(args)
 
         try:
-            do_textual_inversion_training(InvokeAIAppConfig.get_config(),**args)
+            do_textual_inversion_training(InvokeAIAppConfig.get_config(), **args)
             copy_to_embeddings_folder(args)
         except Exception as e:
             logger.error("An exception occurred during training. The exception was:")
             logger.error(str(e))
             logger.error("DETAILS:")
             logger.error(traceback.format_exc())
 
 
 def main():
     global config
-    
+
     args = parse_args()
     config = InvokeAIAppConfig.get_config()
 
     # change root if needed
     if args.root_dir:
         config.root = args.root_dir
-    
+
     try:
         if args.front_end:
             do_front_end(args)
         else:
-            do_textual_inversion_training(config,**vars(args))
+            do_textual_inversion_training(config, **vars(args))
     except AssertionError as e:
         logger.error(e)
         sys.exit(-1)
     except KeyboardInterrupt:
         pass
     except (widget.NotEnoughSpaceForWidget, Exception) as e:
         if str(e).startswith("Height of 1 allocated"):
-            logger.error(
-                "You need to have at least one diffusers models defined in models.yaml in order to train"
-            )
+            logger.error("You need to have at least one diffusers models defined in models.yaml in order to train")
         elif str(e).startswith("addwstr"):
-            logger.error(
-                "Not enough window space for the interface. Please make your window larger and try again."
-            )
+            logger.error("Not enough window space for the interface. Please make your window larger and try again.")
         else:
             logger.error(e)
         sys.exit(-1)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/App-6125620a.css` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/App-6125620a.css`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/App-69e5ea36.js` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/App-58b095d3.js`

 * *Files 0% similar despite different names*

#### js-beautify {}

```diff
@@ -422,15 +422,15 @@
     fL as jD,
     fM as ID,
     fN as ED,
     fO as OD,
     fP as RD,
     fQ as MD,
     fR as DD
-} from "./index-89941396.js";
+} from "./index-5a784cdd.js";
 import {
     I as Kr,
     u as TD,
     c as AD,
     a as Rn,
     b as rr,
     d as Ba,
@@ -442,15 +442,15 @@
     g as Fa,
     h as $D,
     r as Ue,
     i as zD,
     j as hw,
     k as Wt,
     l as Sr
-} from "./MantineProvider-8184f020.js";
+} from "./MantineProvider-ea42d3d1.js";
 
 function LD(e, t) {
     if (e == null) return {};
     var n = {},
         r = Object.keys(e),
         o, s;
     for (s = 0; s < r.length; s++) o = r[s], !(t.indexOf(o) >= 0) && (n[o] = e[o]);
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/MantineProvider-8184f020.js` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/MantineProvider-ea42d3d1.js`

 * *Files 0% similar despite different names*

#### js-beautify {}

```diff
@@ -16,15 +16,15 @@
     Y as Ue,
     g2 as Ve,
     g3 as Ze,
     fU as qe,
     ab as j,
     fS as B,
     f_ as Xe
-} from "./index-89941396.js";
+} from "./index-5a784cdd.js";
 
 function Ye(e, t) {
     return `${e} returned \`undefined\`. Seems you forgot to wrap component within ${t}`
 }
 
 function M(e = {}) {
     const {
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/ThemeLocaleProvider-5b992bc7.css` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/ThemeLocaleProvider-5b992bc7.css`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/ThemeLocaleProvider-9ac72450.js` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/ThemeLocaleProvider-13e3db3d.js`

 * *Files 0% similar despite different names*

#### js-beautify {}

```diff
@@ -14,15 +14,15 @@
     fY as ba,
     fZ as ja,
     f_ as Ha,
     aI as Wa,
     f$ as Va,
     ad as La,
     g0 as qa
-} from "./index-89941396.js";
+} from "./index-5a784cdd.js";
 import {
     n,
     o as Sr,
     p as Oa,
     T as Na,
     q as Ga,
     s as Ua,
@@ -36,15 +36,15 @@
     B as rt,
     D as at,
     E as tt,
     F as ot,
     G as nt,
     e as it,
     M as lt
-} from "./MantineProvider-8184f020.js";
+} from "./MantineProvider-ea42d3d1.js";
 var va = String.raw,
     ua = va`
   :root,
   :host {
     --chakra-vh: 100vh;
   }
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/favicon-0d253ced.ico` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/favicon-0d253ced.ico`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/index-89941396.js` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/index-5a784cdd.js`

 * *Files 1% similar despite different names*

#### js-beautify {}

```diff
@@ -427,18 +427,18 @@
 };
 Re.useTransition = function() {
     return nr.current.useTransition()
 };
 Re.version = "18.2.0";
 I8.exports = Re;
 var k = I8.exports;
-const We = al(k),
+const Xe = al(k),
     $3 = O8({
         __proto__: null,
-        default: We
+        default: Xe
     }, [k]);
 /**
  * @license React
  * react-jsx-runtime.production.min.js
  *
  * Copyright (c) Facebook, Inc. and its affiliates.
  *
@@ -1790,15 +1790,15 @@
     var n = e.entangledLanes |= t;
     for (e = e.entanglements; n;) {
         var r = 31 - mi(n),
             i = 1 << r;
         i & t | e[r] & t && (e[r] |= t), n &= ~i
     }
 }
-var Ke = 0;
+var Ye = 0;
 
 function gA(e) {
     return e &= -e, 1 < e ? 4 < e ? e & 268435455 ? 16 : 536870912 : 4 : 1
 }
 var mA, _w, yA, vA, bA, VS = !1,
     tp = [],
     ws = null,
@@ -1921,32 +1921,32 @@
     for (ws !== null && Dc(ws, e), xs !== null && Dc(xs, e), Cs !== null && Dc(Cs, e), Vd.forEach(t), zd.forEach(t), n = 0; n < ps.length; n++) r = ps[n], r.blockedOn === e && (r.blockedOn = null);
     for (; 0 < ps.length && (n = ps[0], n.blockedOn === null);) SA(n), n.blockedOn === null && ps.shift()
 }
 var Su = Go.ReactCurrentBatchConfig,
     Fg = !0;
 
 function JF(e, t, n, r) {
-    var i = Ke,
+    var i = Ye,
         o = Su.transition;
     Su.transition = null;
     try {
-        Ke = 1, ww(e, t, n, r)
+        Ye = 1, ww(e, t, n, r)
     } finally {
-        Ke = i, Su.transition = o
+        Ye = i, Su.transition = o
     }
 }
 
 function eB(e, t, n, r) {
-    var i = Ke,
+    var i = Ye,
         o = Su.transition;
     Su.transition = null;
     try {
-        Ke = 4, ww(e, t, n, r)
+        Ye = 4, ww(e, t, n, r)
     } finally {
-        Ke = i, Su.transition = o
+        Ye = i, Su.transition = o
     }
 }
 
 function ww(e, t, n, r) {
     if (Fg) {
         var i = zS(e, t, n, r);
         if (i === null) K1(e, t, r, Bg, n), K3(e, r);
@@ -3078,15 +3078,15 @@
     }
 }
 
 function ht(e) {
     0 > ou || (e.current = YS[ou], YS[ou] = null, ou--)
 }
 
-function at(e, t) {
+function lt(e, t) {
     ou++, YS[ou] = e.current, e.current = t
 }
 var $s = {},
     Bn = Qs($s),
     pr = Qs(!1),
     Ha = $s;
 
@@ -3107,34 +3107,34 @@
 
 function zg() {
     ht(pr), ht(Bn)
 }
 
 function p5(e, t, n) {
     if (Bn.current !== $s) throw Error(Z(168));
-    at(Bn, t), at(pr, n)
+    lt(Bn, t), lt(pr, n)
 }
 
 function BA(e, t, n) {
     var r = e.stateNode;
     if (t = t.childContextTypes, typeof r.getChildContext != "function") return n;
     r = r.getChildContext();
     for (var i in r)
         if (!(i in t)) throw Error(Z(108, OF(e) || "Unknown", i));
     return Ot({}, n, r)
 }
 
 function Ug(e) {
-    return e = (e = e.stateNode) && e.__reactInternalMemoizedMergedChildContext || $s, Ha = Bn.current, at(Bn, e), at(pr, pr.current), !0
+    return e = (e = e.stateNode) && e.__reactInternalMemoizedMergedChildContext || $s, Ha = Bn.current, lt(Bn, e), lt(pr, pr.current), !0
 }
 
 function g5(e, t, n) {
     var r = e.stateNode;
     if (!r) throw Error(Z(169));
-    n ? (e = BA(e, t, Ha), r.__reactInternalMemoizedMergedChildContext = e, ht(pr), ht(Bn), at(Bn, e)) : ht(pr), at(pr, n)
+    n ? (e = BA(e, t, Ha), r.__reactInternalMemoizedMergedChildContext = e, ht(pr), ht(Bn), lt(Bn, e)) : ht(pr), lt(pr, n)
 }
 var _o = null,
     Ey = !1,
     Y1 = !1;
 
 function jA(e) {
     _o === null ? _o = [e] : _o.push(e)
@@ -3144,26 +3144,26 @@
     Ey = !0, jA(e)
 }
 
 function Zs() {
     if (!Y1 && _o !== null) {
         Y1 = !0;
         var e = 0,
-            t = Ke;
+            t = Ye;
         try {
             var n = _o;
-            for (Ke = 1; e < n.length; e++) {
+            for (Ye = 1; e < n.length; e++) {
                 var r = n[e];
                 do r = r(!0); while (r !== null)
             }
             _o = null, Ey = !1
         } catch (i) {
             throw _o !== null && (_o = _o.slice(e + 1)), dA(bw, Zs), i
         } finally {
-            Ke = t, Y1 = !1
+            Ye = t, Y1 = !1
         }
     }
     return null
 }
 var su = [],
     au = 0,
     Gg = null,
@@ -3830,34 +3830,34 @@
 
 function Oa(e) {
     if (e === rh) throw Error(Z(174));
     return e
 }
 
 function Lw(e, t) {
-    switch (at(Yd, t), at(Xd, e), at(Xi, rh), e = t.nodeType, e) {
+    switch (lt(Yd, t), lt(Xd, e), lt(Xi, rh), e = t.nodeType, e) {
         case 9:
         case 11:
             t = (t = t.documentElement) ? t.namespaceURI : MS(null, "");
             break;
         default:
             e = e === 8 ? t.parentNode : t, t = e.namespaceURI || null, e = e.tagName, t = MS(t, e)
     }
-    ht(Xi), at(Xi, t)
+    ht(Xi), lt(Xi, t)
 }
 
 function Uu() {
     ht(Xi), ht(Xd), ht(Yd)
 }
 
 function YA(e) {
     Oa(Yd.current);
     var t = Oa(Xi.current),
         n = MS(t, e.type);
-    t !== n && (at(Xd, e), at(Xi, n))
+    t !== n && (lt(Xd, e), lt(Xi, n))
 }
 
 function $w(e) {
     Xd.current === e && (ht(Xi), ht(Xd))
 }
 var Et = Qs(0);
 
@@ -4174,22 +4174,22 @@
 }
 
 function d9(e, t, n) {
     return Wa & 21 ? (wi(n, t) || (n = pA(), kt.lanes |= n, Ka |= n, e.baseState = !0), t) : (e.baseState && (e.baseState = !1, dr = !0), e.memoizedState = n)
 }
 
 function YB(e, t) {
-    var n = Ke;
-    Ke = n !== 0 && 4 > n ? n : 4, e(!0);
+    var n = Ye;
+    Ye = n !== 0 && 4 > n ? n : 4, e(!0);
     var r = Z1.transition;
     Z1.transition = {};
     try {
         e(!1), t()
     } finally {
-        Ke = n, Z1.transition = r
+        Ye = n, Z1.transition = r
     }
 }
 
 function f9() {
     return Qr().memoizedState
 }
 
@@ -4538,28 +4538,28 @@
         i = r.children,
         o = e !== null ? e.memoizedState : null;
     if (r.mode === "hidden")
         if (!(t.mode & 1)) t.memoizedState = {
             baseLanes: 0,
             cachePool: null,
             transitions: null
-        }, at(cu, Cr), Cr |= n;
+        }, lt(cu, Cr), Cr |= n;
         else {
             if (!(n & 1073741824)) return e = o !== null ? o.baseLanes | n : n, t.lanes = t.childLanes = 1073741824, t.memoizedState = {
                 baseLanes: e,
                 cachePool: null,
                 transitions: null
-            }, t.updateQueue = null, at(cu, Cr), Cr |= e, null;
+            }, t.updateQueue = null, lt(cu, Cr), Cr |= e, null;
             t.memoizedState = {
                 baseLanes: 0,
                 cachePool: null,
                 transitions: null
-            }, r = o !== null ? o.baseLanes : n, at(cu, Cr), Cr |= r
+            }, r = o !== null ? o.baseLanes : n, lt(cu, Cr), Cr |= r
         }
-    else o !== null ? (r = o.baseLanes | n, t.memoizedState = null) : r = n, at(cu, Cr), Cr |= r;
+    else o !== null ? (r = o.baseLanes | n, t.memoizedState = null) : r = n, lt(cu, Cr), Cr |= r;
     return Yn(e, t, i, n), t.child
 }
 
 function S9(e, t) {
     var n = t.ref;
     (e === null && n !== null || e !== null && e.ref !== n) && (t.flags |= 512, t.flags |= 2097152)
 }
@@ -4630,15 +4630,15 @@
 
 function w9(e, t, n) {
     var r = t.pendingProps,
         i = Et.current,
         o = !1,
         s = (t.flags & 128) !== 0,
         a;
-    if ((a = s) || (a = e !== null && e.memoizedState === null ? !1 : (i & 2) !== 0), a ? (o = !0, t.flags &= -129) : (e === null || e.memoizedState !== null) && (i |= 1), at(Et, i & 1), e === null) return ZS(t), e = t.memoizedState, e !== null && (e = e.dehydrated, e !== null) ? (t.mode & 1 ? e.data === "$!" ? t.lanes = 8 : t.lanes = 1073741824 : t.lanes = 1, null) : (s = r.children, e = r.fallback, o ? (r = t.mode, o = t.child, s = {
+    if ((a = s) || (a = e !== null && e.memoizedState === null ? !1 : (i & 2) !== 0), a ? (o = !0, t.flags &= -129) : (e === null || e.memoizedState !== null) && (i |= 1), lt(Et, i & 1), e === null) return ZS(t), e = t.memoizedState, e !== null && (e = e.dehydrated, e !== null) ? (t.mode & 1 ? e.data === "$!" ? t.lanes = 8 : t.lanes = 1073741824 : t.lanes = 1, null) : (s = r.children, e = r.fallback, o ? (r = t.mode, o = t.child, s = {
         mode: "hidden",
         children: s
     }, !(r & 1) && o !== null ? (o.childLanes = 0, o.pendingProps = s) : o = Ry(s, r, 0, null), e = $a(e, r, n, null), o.return = t, e.return = t, o.sibling = e, t.child = o, t.child.memoizedState = s_(n), t.memoizedState = o_, e) : Gw(t, s));
     if (i = e.memoizedState, i !== null && (a = i.dehydrated, a !== null)) return ij(e, t, s, r, a, i, n);
     if (o) {
         o = r.fallback, s = t.mode, i = e.child, a = i.sibling;
         var l = {
@@ -4759,15 +4759,15 @@
                 if (e.return === null || e.return === t) break e;
                 e = e.return
             }
             e.sibling.return = e.return, e = e.sibling
         }
         r &= 1
     }
-    if (at(Et, r), !(t.mode & 1)) t.memoizedState = null;
+    if (lt(Et, r), !(t.mode & 1)) t.memoizedState = null;
     else switch (i) {
         case "forwards":
             for (n = t.child, i = null; n !== null;) e = n.alternate, e !== null && Xg(e) === null && (i = n), n = n.sibling;
             n = i, n === null ? (i = t.child, t.child = null) : (i = n.sibling, n.sibling = null), nb(t, !1, i, n, o);
             break;
         case "backwards":
             for (n = null, i = t.child, t.child = null; i !== null;) {
@@ -4815,26 +4815,26 @@
             break;
         case 4:
             Lw(t, t.stateNode.containerInfo);
             break;
         case 10:
             var r = t.type._context,
                 i = t.memoizedProps.value;
-            at(qg, r._currentValue), r._currentValue = i;
+            lt(qg, r._currentValue), r._currentValue = i;
             break;
         case 13:
-            if (r = t.memoizedState, r !== null) return r.dehydrated !== null ? (at(Et, Et.current & 1), t.flags |= 128, null) : n & t.child.childLanes ? w9(e, t, n) : (at(Et, Et.current & 1), e = Bo(e, t, n), e !== null ? e.sibling : null);
-            at(Et, Et.current & 1);
+            if (r = t.memoizedState, r !== null) return r.dehydrated !== null ? (lt(Et, Et.current & 1), t.flags |= 128, null) : n & t.child.childLanes ? w9(e, t, n) : (lt(Et, Et.current & 1), e = Bo(e, t, n), e !== null ? e.sibling : null);
+            lt(Et, Et.current & 1);
             break;
         case 19:
             if (r = (n & t.childLanes) !== 0, e.flags & 128) {
                 if (r) return x9(e, t, n);
                 t.flags |= 128
             }
-            if (i = t.memoizedState, i !== null && (i.rendering = null, i.tail = null, i.lastEffect = null), at(Et, Et.current), r) break;
+            if (i = t.memoizedState, i !== null && (i.rendering = null, i.tail = null, i.lastEffect = null), lt(Et, Et.current), r) break;
             return null;
         case 22:
         case 23:
             return t.lanes = 0, b9(e, t, n)
     }
     return Bo(e, t, n)
 }
@@ -5148,28 +5148,28 @@
                     if (rn !== 0 || e !== null && e.flags & 128)
                         for (e = t.child; e !== null;) {
                             if (s = Xg(e), s !== null) {
                                 for (t.flags |= 128, Fc(o, !1), r = s.updateQueue, r !== null && (t.updateQueue = r, t.flags |= 4), t.subtreeFlags = 0, r = n, n = t.child; n !== null;) o = n, e = r, o.flags &= 14680066, s = o.alternate, s === null ? (o.childLanes = 0, o.lanes = e, o.child = null, o.subtreeFlags = 0, o.memoizedProps = null, o.memoizedState = null, o.updateQueue = null, o.dependencies = null, o.stateNode = null) : (o.childLanes = s.childLanes, o.lanes = s.lanes, o.child = s.child, o.subtreeFlags = 0, o.deletions = null, o.memoizedProps = s.memoizedProps, o.memoizedState = s.memoizedState, o.updateQueue = s.updateQueue, o.type = s.type, e = s.dependencies, o.dependencies = e === null ? null : {
                                     lanes: e.lanes,
                                     firstContext: e.firstContext
                                 }), n = n.sibling;
-                                return at(Et, Et.current & 1 | 2), t.child
+                                return lt(Et, Et.current & 1 | 2), t.child
                             }
                             e = e.sibling
                         }
                     o.tail !== null && Bt() > Hu && (t.flags |= 128, r = !0, Fc(o, !1), t.lanes = 4194304)
                 }
             else {
                 if (!r)
                     if (e = Xg(s), e !== null) {
                         if (t.flags |= 128, r = !0, n = e.updateQueue, n !== null && (t.updateQueue = n, t.flags |= 4), Fc(o, !0), o.tail === null && o.tailMode === "hidden" && !s.alternate && !St) return Nn(t), null
                     } else 2 * Bt() - o.renderingStartTime > Hu && n !== 1073741824 && (t.flags |= 128, r = !0, Fc(o, !1), t.lanes = 4194304);
                 o.isBackwards ? (s.sibling = t.child, t.child = s) : (n = o.last, n !== null ? n.sibling = s : t.child = s, o.last = s)
             }
-            return o.tail !== null ? (t = o.tail, o.rendering = t, o.tail = t.sibling, o.renderingStartTime = Bt(), t.sibling = null, n = Et.current, at(Et, r ? n & 1 | 2 : n & 1), t) : (Nn(t), null);
+            return o.tail !== null ? (t = o.tail, o.rendering = t, o.tail = t.sibling, o.renderingStartTime = Bt(), t.sibling = null, n = Et.current, lt(Et, r ? n & 1 | 2 : n & 1), t) : (Nn(t), null);
         case 22:
         case 23:
             return Xw(), r = t.memoizedState !== null, e !== null && e.memoizedState !== null !== r && (t.flags |= 8192), r && t.mode & 1 ? Cr & 1073741824 && (Nn(t), t.subtreeFlags & 6 && (t.flags |= 8192)) : Nn(t), null;
         case 24:
             return null;
         case 25:
             return null
@@ -5937,15 +5937,15 @@
     ag = 0;
 
 function Zn() {
     return $e & 6 ? Bt() : sg !== -1 ? sg : sg = Bt()
 }
 
 function As(e) {
-    return e.mode & 1 ? $e & 2 && An !== 0 ? An & -An : KB.transition !== null ? (ag === 0 && (ag = pA()), ag) : (e = Ke, e !== 0 || (e = window.event, e = e === void 0 ? 16 : _A(e.type)), e) : 1
+    return e.mode & 1 ? $e & 2 && An !== 0 ? An & -An : KB.transition !== null ? (ag === 0 && (ag = pA()), ag) : (e = Ye, e !== 0 || (e = window.event, e = e === void 0 ? 16 : _A(e.type)), e) : 1
 }
 
 function yi(e, t, n, r) {
     if (50 < Td) throw Td = 0, h_ = null, Error(Z(185));
     eh(e, n, r), (!($e & 2) || e !== gn) && (e === gn && (!($e & 2) && (Oy |= n), rn === 4 && gs(e, An)), mr(e, r), n === 1 && $e === 0 && !(t.mode & 1) && (Hu = Bt() + 500, Ey && Zs()))
 }
 
@@ -6122,19 +6122,19 @@
 }
 
 function Xa(e) {
     bs !== null && bs.tag === 0 && !($e & 6) && wu();
     var t = $e;
     $e |= 1;
     var n = Xr.transition,
-        r = Ke;
+        r = Ye;
     try {
-        if (Xr.transition = null, Ke = 1, e) return e()
+        if (Xr.transition = null, Ye = 1, e) return e()
     } finally {
-        Ke = r, Xr.transition = n, $e = t, !($e & 6) && Zs()
+        Ye = r, Xr.transition = n, $e = t, !($e & 6) && Zs()
     }
 }
 
 function Xw() {
     Cr = cu.current, ht(cu)
 }
 
@@ -6334,20 +6334,20 @@
         }
         Yt = t = e
     } while (t !== null);
     rn === 0 && (rn = 5)
 }
 
 function _a(e, t, n) {
-    var r = Ke,
+    var r = Ye,
         i = Xr.transition;
     try {
-        Xr.transition = null, Ke = 1, gj(e, t, n, r)
+        Xr.transition = null, Ye = 1, gj(e, t, n, r)
     } finally {
-        Xr.transition = i, Ke = r
+        Xr.transition = i, Ye = r
     }
     return null
 }
 
 function gj(e, t, n, r) {
     do wu(); while (bs !== null);
     if ($e & 6) throw Error(Z(327));
@@ -6357,35 +6357,35 @@
     if (e.finishedWork = null, e.finishedLanes = 0, n === e.current) throw Error(Z(177));
     e.callbackNode = null, e.callbackPriority = 0;
     var o = n.lanes | n.childLanes;
     if (XF(e, o), e === gn && (Yt = gn = null, An = 0), !(n.subtreeFlags & 2064) && !(n.flags & 2064) || cp || (cp = !0, B9(Lg, function() {
             return wu(), null
         })), o = (n.flags & 15990) !== 0, n.subtreeFlags & 15990 || o) {
         o = Xr.transition, Xr.transition = null;
-        var s = Ke;
-        Ke = 1;
+        var s = Ye;
+        Ye = 1;
         var a = $e;
-        $e |= 4, Hw.current = null, uj(e, n), O9(n, e), DB(qS), Fg = !!HS, qS = HS = null, e.current = n, cj(n), jF(), $e = a, Ke = s, Xr.transition = o
+        $e |= 4, Hw.current = null, uj(e, n), O9(n, e), DB(qS), Fg = !!HS, qS = HS = null, e.current = n, cj(n), jF(), $e = a, Ye = s, Xr.transition = o
     } else e.current = n;
     if (cp && (cp = !1, bs = e, em = i), o = e.pendingLanes, o === 0 && (Ps = null), UF(n.stateNode), mr(e, Bt()), t !== null)
         for (r = e.onRecoverableError, n = 0; n < t.length; n++) i = t[n], r(i.value, {
             componentStack: i.stack,
             digest: i.digest
         });
     if (Jg) throw Jg = !1, e = f_, f_ = null, e;
     return em & 1 && e.tag !== 0 && wu(), o = e.pendingLanes, o & 1 ? e === h_ ? Td++ : (Td = 0, h_ = e) : Td = 0, Zs(), null
 }
 
 function wu() {
     if (bs !== null) {
         var e = gA(em),
             t = Xr.transition,
-            n = Ke;
+            n = Ye;
         try {
-            if (Xr.transition = null, Ke = 16 > e ? 16 : e, bs === null) var r = !1;
+            if (Xr.transition = null, Ye = 16 > e ? 16 : e, bs === null) var r = !1;
             else {
                 if (e = bs, bs = null, em = 0, $e & 6) throw Error(Z(331));
                 var i = $e;
                 for ($e |= 4, ae = e.current; ae !== null;) {
                     var o = ae,
                         s = o.child;
                     if (ae.flags & 16) {
@@ -6481,15 +6481,15 @@
                 if ($e = i, Zs(), Ki && typeof Ki.onPostCommitFiberRoot == "function") try {
                     Ki.onPostCommitFiberRoot(_y, e)
                 } catch {}
                 r = !0
             }
             return r
         } finally {
-            Ke = n, Xr.transition = t
+            Ye = n, Xr.transition = t
         }
     }
     return !1
 }
 
 function j5(e, t, n) {
     t = Gu(n, t), t = m9(e, t, 1), e = Es(e, t, 1), t = Zn(), e !== null && (eh(e, 1, t), mr(e, t))
@@ -6634,15 +6634,15 @@
             return Yn(e, t, t.pendingProps, n), t.child;
         case 8:
             return Yn(e, t, t.pendingProps.children, n), t.child;
         case 12:
             return Yn(e, t, t.pendingProps.children, n), t.child;
         case 10:
             e: {
-                if (r = t.type._context, i = t.pendingProps, o = t.memoizedProps, s = i.value, at(qg, r._currentValue), r._currentValue = s, o !== null)
+                if (r = t.type._context, i = t.pendingProps, o = t.memoizedProps, s = i.value, lt(qg, r._currentValue), r._currentValue = s, o !== null)
                     if (wi(o.value, s)) {
                         if (o.children === i.children && !pr.current) {
                             t = Bo(e, t, n);
                             break e
                         }
                     } else
                         for (o = t.child, o !== null && (o.return = t); o !== null;) {
@@ -7017,22 +7017,22 @@
             var r = Zn();
             yi(n, e, t, r)
         }
         Jw(e, t)
     }
 };
 vA = function() {
-    return Ke
+    return Ye
 };
 bA = function(e, t) {
-    var n = Ke;
+    var n = Ye;
     try {
-        return Ke = e, t()
+        return Ye = e, t()
     } finally {
-        Ke = n
+        Ye = n
     }
 };
 $S = function(e, t, n) {
     switch (t) {
         case "input":
             if (OS(e, n), t = n.name, n.type === "radio" && t != null) {
                 for (n = e; n.parentNode;) n = n.parentNode;
@@ -7397,15 +7397,15 @@
         }
         return e
     }, rm.apply(this, arguments)
 }
 var ek = {
         exports: {}
     },
-    Xe = {};
+    Qe = {};
 /** @license React v16.13.1
  * react-is.production.min.js
  *
  * Copyright (c) Facebook, Inc. and its affiliates.
  *
  * This source code is licensed under the MIT license found in the
  * LICENSE file in the root directory of this source tree.
@@ -7460,69 +7460,69 @@
         }
     }
 }
 
 function tk(e) {
     return Dr(e) === zy
 }
-Xe.AsyncMode = ox;
-Xe.ConcurrentMode = zy;
-Xe.ContextConsumer = Vy;
-Xe.ContextProvider = jy;
-Xe.Element = rx;
-Xe.ForwardRef = Uy;
-Xe.Fragment = $y;
-Xe.Lazy = qy;
-Xe.Memo = Hy;
-Xe.Portal = ix;
-Xe.Profiler = By;
-Xe.StrictMode = Fy;
-Xe.Suspense = Gy;
-Xe.isAsyncMode = function(e) {
+Qe.AsyncMode = ox;
+Qe.ConcurrentMode = zy;
+Qe.ContextConsumer = Vy;
+Qe.ContextProvider = jy;
+Qe.Element = rx;
+Qe.ForwardRef = Uy;
+Qe.Fragment = $y;
+Qe.Lazy = qy;
+Qe.Memo = Hy;
+Qe.Portal = ix;
+Qe.Profiler = By;
+Qe.StrictMode = Fy;
+Qe.Suspense = Gy;
+Qe.isAsyncMode = function(e) {
     return tk(e) || Dr(e) === ox
 };
-Xe.isConcurrentMode = tk;
-Xe.isContextConsumer = function(e) {
+Qe.isConcurrentMode = tk;
+Qe.isContextConsumer = function(e) {
     return Dr(e) === Vy
 };
-Xe.isContextProvider = function(e) {
+Qe.isContextProvider = function(e) {
     return Dr(e) === jy
 };
-Xe.isElement = function(e) {
+Qe.isElement = function(e) {
     return typeof e == "object" && e !== null && e.$$typeof === rx
 };
-Xe.isForwardRef = function(e) {
+Qe.isForwardRef = function(e) {
     return Dr(e) === Uy
 };
-Xe.isFragment = function(e) {
+Qe.isFragment = function(e) {
     return Dr(e) === $y
 };
-Xe.isLazy = function(e) {
+Qe.isLazy = function(e) {
     return Dr(e) === qy
 };
-Xe.isMemo = function(e) {
+Qe.isMemo = function(e) {
     return Dr(e) === Hy
 };
-Xe.isPortal = function(e) {
+Qe.isPortal = function(e) {
     return Dr(e) === ix
 };
-Xe.isProfiler = function(e) {
+Qe.isProfiler = function(e) {
     return Dr(e) === By
 };
-Xe.isStrictMode = function(e) {
+Qe.isStrictMode = function(e) {
     return Dr(e) === Fy
 };
-Xe.isSuspense = function(e) {
+Qe.isSuspense = function(e) {
     return Dr(e) === Gy
 };
-Xe.isValidElementType = function(e) {
+Qe.isValidElementType = function(e) {
     return typeof e == "string" || typeof e == "function" || e === $y || e === zy || e === By || e === Fy || e === Gy || e === nV || typeof e == "object" && e !== null && (e.$$typeof === qy || e.$$typeof === Hy || e.$$typeof === jy || e.$$typeof === Vy || e.$$typeof === Uy || e.$$typeof === iV || e.$$typeof === oV || e.$$typeof === sV || e.$$typeof === rV)
 };
-Xe.typeOf = Dr;
-ek.exports = Xe;
+Qe.typeOf = Dr;
+ek.exports = Qe;
 var aV = ek.exports,
     nk = aV,
     lV = {
         $$typeof: !0,
         render: !0,
         defaultProps: !0,
         displayName: !0,
@@ -7535,15 +7535,15 @@
         displayName: !0,
         propTypes: !0,
         type: !0
     },
     rk = {};
 rk[nk.ForwardRef] = lV;
 rk[nk.Memo] = uV;
-var Ze = {};
+var et = {};
 /**
  * @license React
  * react-is.production.min.js
  *
  * Copyright (c) Facebook, Inc. and its affiliates.
  *
  * This source code is licensed under the MIT license found in the
@@ -7592,72 +7592,72 @@
                         }
                 }
             case ax:
                 return t
         }
     }
 }
-Ze.ContextConsumer = Qy;
-Ze.ContextProvider = Yy;
-Ze.Element = sx;
-Ze.ForwardRef = Zy;
-Ze.Fragment = Wy;
-Ze.Lazy = n0;
-Ze.Memo = t0;
-Ze.Portal = ax;
-Ze.Profiler = Xy;
-Ze.StrictMode = Ky;
-Ze.Suspense = Jy;
-Ze.SuspenseList = e0;
-Ze.isAsyncMode = function() {
+et.ContextConsumer = Qy;
+et.ContextProvider = Yy;
+et.Element = sx;
+et.ForwardRef = Zy;
+et.Fragment = Wy;
+et.Lazy = n0;
+et.Memo = t0;
+et.Portal = ax;
+et.Profiler = Xy;
+et.StrictMode = Ky;
+et.Suspense = Jy;
+et.SuspenseList = e0;
+et.isAsyncMode = function() {
     return !1
 };
-Ze.isConcurrentMode = function() {
+et.isConcurrentMode = function() {
     return !1
 };
-Ze.isContextConsumer = function(e) {
+et.isContextConsumer = function(e) {
     return Jr(e) === Qy
 };
-Ze.isContextProvider = function(e) {
+et.isContextProvider = function(e) {
     return Jr(e) === Yy
 };
-Ze.isElement = function(e) {
+et.isElement = function(e) {
     return typeof e == "object" && e !== null && e.$$typeof === sx
 };
-Ze.isForwardRef = function(e) {
+et.isForwardRef = function(e) {
     return Jr(e) === Zy
 };
-Ze.isFragment = function(e) {
+et.isFragment = function(e) {
     return Jr(e) === Wy
 };
-Ze.isLazy = function(e) {
+et.isLazy = function(e) {
     return Jr(e) === n0
 };
-Ze.isMemo = function(e) {
+et.isMemo = function(e) {
     return Jr(e) === t0
 };
-Ze.isPortal = function(e) {
+et.isPortal = function(e) {
     return Jr(e) === ax
 };
-Ze.isProfiler = function(e) {
+et.isProfiler = function(e) {
     return Jr(e) === Xy
 };
-Ze.isStrictMode = function(e) {
+et.isStrictMode = function(e) {
     return Jr(e) === Ky
 };
-Ze.isSuspense = function(e) {
+et.isSuspense = function(e) {
     return Jr(e) === Jy
 };
-Ze.isSuspenseList = function(e) {
+et.isSuspenseList = function(e) {
     return Jr(e) === e0
 };
-Ze.isValidElementType = function(e) {
+et.isValidElementType = function(e) {
     return typeof e == "string" || typeof e == "function" || e === Wy || e === Xy || e === Ky || e === Jy || e === e0 || e === dV || typeof e == "object" && e !== null && (e.$$typeof === n0 || e.$$typeof === t0 || e.$$typeof === Yy || e.$$typeof === Qy || e.$$typeof === Zy || e.$$typeof === ik || e.getModuleId !== void 0)
 };
-Ze.typeOf = Jr;
+et.typeOf = Jr;
 
 function fV() {
     const e = Yj();
     let t = null,
         n = null;
     return {
         clear() {
@@ -7773,15 +7773,15 @@
             subscription: u
         } = s;
         return u.onStateChange = u.notifyNestedSubs, u.trySubscribe(), a !== e.getState() && u.notifyNestedSubs(), () => {
             u.tryUnsubscribe(), u.onStateChange = void 0
         }
     }, [s, a]);
     const l = t || Fs;
-    return We.createElement(l.Provider, {
+    return Xe.createElement(l.Provider, {
         value: s
     }, n)
 }
 
 function ok(e = Fs) {
     const t = e === Fs ? Q9 : nx(e);
     return function() {
@@ -12228,17 +12228,17 @@
     uK = "[object Int8Array]",
     cK = "[object Int16Array]",
     dK = "[object Int32Array]",
     fK = "[object Uint8Array]",
     hK = "[object Uint8ClampedArray]",
     pK = "[object Uint16Array]",
     gK = "[object Uint32Array]",
-    ot = {};
-ot[KO] = ot[WW] = ot[oK] = ot[sK] = ot[KW] = ot[XW] = ot[aK] = ot[lK] = ot[uK] = ot[cK] = ot[dK] = ot[ZW] = ot[JW] = ot[YO] = ot[eK] = ot[tK] = ot[nK] = ot[rK] = ot[fK] = ot[hK] = ot[pK] = ot[gK] = !0;
-ot[YW] = ot[XO] = ot[iK] = !1;
+    st = {};
+st[KO] = st[WW] = st[oK] = st[sK] = st[KW] = st[XW] = st[aK] = st[lK] = st[uK] = st[cK] = st[dK] = st[ZW] = st[JW] = st[YO] = st[eK] = st[tK] = st[nK] = st[rK] = st[fK] = st[hK] = st[pK] = st[gK] = !0;
+st[YW] = st[XO] = st[iK] = !1;
 
 function Ad(e, t, n, r, i, o) {
     var s, a = t & GW,
         l = t & HW,
         u = t & qW;
     if (n && (s = i ? n(e, r, i, o) : n(e)), s !== void 0) return s;
     if (!vr(e)) return e;
@@ -12248,15 +12248,15 @@
     } else {
         var d = Qu(e),
             f = d == XO || d == QW;
         if (lf(e)) return jO(e, a);
         if (d == YO || d == KO || f && !i) {
             if (s = l || f ? {} : WO(e), !a) return l ? Zq(e, Gq(s, e)) : Xq(e, Uq(s, e))
         } else {
-            if (!ot[d]) return i ? e : {};
+            if (!st[d]) return i ? e : {};
             s = DW(e, d, a)
         }
     }
     o || (o = new vi);
     var h = o.get(e);
     if (h) return h;
     o.set(e, s), UW(e) ? e.forEach(function(S) {
@@ -17828,16 +17828,16 @@
                                 getState: w,
                                 extra: x,
                                 endpoint: T,
                                 forced: P,
                                 type: E
                             })];
                         case 1:
-                            Y.headers = ut.sent() || O, B = function(Je) {
-                                return typeof Je == "object" && (xi(Je) || Array.isArray(Je) || typeof Je.toJSON == "function")
+                            Y.headers = ut.sent() || O, B = function(tt) {
+                                return typeof tt == "object" && (xi(tt) || Array.isArray(tt) || typeof tt.toJSON == "function")
                             }, !X.headers.has("content-type") && B(X.body) && X.headers.set("content-type", f), B(X.body) && c(X.headers) && (X.body = JSON.stringify(X.body, h)), N && (H = ~M.indexOf("?") ? "&" : "?", Q = l ? l(N) : new URLSearchParams(oT(N)), M += H + Q), M = lZ(r, M), J = new Request(M, X), ne = J.clone(), A = {
                                 request: ne
                             }, xe = !1, ve = G && setTimeout(function() {
                                 xe = !0, b.abort()
                             }, G), ut.label = 2;
                         case 2:
                             return ut.trys.push([2, 4, 5, 6]), [4, a(J)];
@@ -17852,20 +17852,20 @@
                                 meta: A
                             }];
                         case 5:
                             return ve && clearTimeout(ve), [7];
                         case 6:
                             Ne = te.clone(), A.response = Ne, gt = "", ut.label = 7;
                         case 7:
-                            return ut.trys.push([7, 9, , 10]), [4, Promise.all([y(te, D).then(function(Je) {
-                                return se = Je
-                            }, function(Je) {
-                                return vn = Je
-                            }), Ne.text().then(function(Je) {
-                                return gt = Je
+                            return ut.trys.push([7, 9, , 10]), [4, Promise.all([y(te, D).then(function(tt) {
+                                return se = tt
+                            }, function(tt) {
+                                return vn = tt
+                            }), Ne.text().then(function(tt) {
+                                return gt = tt
                             }, function() {})])];
                         case 8:
                             if (ut.sent(), vn) throw vn;
                             return [3, 10];
                         case 9:
                             return It = ut.sent(), [2, {
                                 error: {
@@ -25605,30 +25605,30 @@
                     s(Ht, $r)
                 }
             },
             gt = qc(n, ve.getState, a),
             vn = qc(n, ve.getState, M),
             It = qc(n, ve.getState, C),
             ut = qc(n, ve.getState, O),
-            Je = qc(n, ve.getState, I),
+            tt = qc(n, ve.getState, I),
             Gt = (Ht, xt) => {
                 if (Ht.button !== 0) return;
                 const {
                     edges: wr,
                     isValidConnection: $r
-                } = ve.getState(), ri = xt ? y : v, uo = (xt ? $ : A) || null, Sn = xt ? "target" : "source", ii = $r || Nre, ca = xt, ki = wr.find(et => et.id === n);
+                } = ve.getState(), ri = xt ? y : v, uo = (xt ? $ : A) || null, Sn = xt ? "target" : "source", ii = $r || Nre, ca = xt, ki = wr.find(nt => nt.id === n);
                 xe(!0), D == null || D(Ht, ki, Sn);
-                const da = et => {
-                    xe(!1), L == null || L(et, ki, Sn)
+                const da = nt => {
+                    xe(!1), L == null || L(nt, ki, Sn)
                 };
                 PI({
                     event: Ht,
                     handleId: uo,
                     nodeId: ri,
-                    onConnect: et => R == null ? void 0 : R(ki, et),
+                    onConnect: nt => R == null ? void 0 : R(ki, nt),
                     isTarget: ca,
                     getState: ve.getState,
                     setState: ve.setState,
                     isValidConnection: ii,
                     edgeUpdaterType: Sn,
                     onEdgeUpdateEnd: da
                 })
@@ -25659,15 +25659,15 @@
                 updating: J
             }]),
             onClick: se,
             onDoubleClick: gt,
             onContextMenu: vn,
             onMouseEnter: It,
             onMouseMove: ut,
-            onMouseLeave: Je,
+            onMouseLeave: tt,
             onKeyDown: X ? Un : void 0,
             tabIndex: X ? 0 : void 0,
             role: X ? "button" : void 0,
             "data-testid": `rf__edge-${n}`,
             "aria-label": W === null ? void 0 : W || `Edge from ${v} to ${y}`,
             "aria-describedby": X ? `${LI}-${G}` : void 0,
             ref: Q,
@@ -28007,15 +28007,15 @@
     onEdgeContextMenu: ce,
     onEdgeMouseEnter: Ne,
     onEdgeMouseMove: se,
     onEdgeMouseLeave: gt,
     edgeUpdaterRadius: vn,
     onEdgeUpdateStart: It,
     onEdgeUpdateEnd: ut,
-    noDragClassName: Je,
+    noDragClassName: tt,
     noWheelClassName: Gt,
     noPanClassName: sr,
     elevateEdgesOnSelect: ni,
     disableKeyboardA11y: Lr,
     nodeOrigin: Rn,
     nodeExtent: bn,
     rfId: Un
@@ -28048,15 +28048,15 @@
     panOnDrag: B,
     defaultViewport: O,
     translateExtent: I,
     minZoom: N,
     maxZoom: R,
     onSelectionContextMenu: p,
     preventScrolling: D,
-    noDragClassName: Je,
+    noDragClassName: tt,
     noWheelClassName: Gt,
     noPanClassName: sr,
     disableKeyboardA11y: Lr,
     children: K.jsxs(Cie, {
         children: [K.jsx(wie, {
             edgeTypes: t,
             onEdgeClick: a,
@@ -28090,15 +28090,15 @@
             onNodeMouseEnter: c,
             onNodeMouseMove: d,
             onNodeMouseLeave: f,
             onNodeContextMenu: h,
             selectNodesOnDrag: C,
             onlyRenderVisibleElements: $,
             noPanClassName: sr,
-            noDragClassName: Je,
+            noDragClassName: tt,
             disableKeyboardA11y: Lr,
             nodeOrigin: Rn,
             nodeExtent: bn,
             rfId: Un
         })]
     })
 }));
@@ -28505,15 +28505,15 @@
         nodesDraggable: ce,
         nodesConnectable: Ne,
         nodesFocusable: se,
         nodeOrigin: gt = Mie,
         edgesFocusable: vn,
         edgesUpdatable: It,
         elementsSelectable: ut,
-        defaultViewport: Je = Die,
+        defaultViewport: tt = Die,
         minZoom: Gt = .5,
         maxZoom: sr = 2,
         translateExtent: ni = o2,
         preventScrolling: Lr = !0,
         nodeExtent: Rn,
         defaultMarkerColor: bn = "#b1b1b7",
         zoomOnScroll: Un = !0,
@@ -28525,15 +28525,15 @@
         panOnDrag: uo = !0,
         onPaneClick: Sn,
         onPaneMouseEnter: ii,
         onPaneMouseMove: ca,
         onPaneMouseLeave: ki,
         onPaneScroll: da,
         onPaneContextMenu: Ct,
-        children: et,
+        children: nt,
         onEdgeUpdate: _n,
         onEdgeContextMenu: ln,
         onEdgeDoubleClick: In,
         onEdgeMouseEnter: Gn,
         onEdgeMouseMove: ar,
         onEdgeMouseLeave: Oi,
         onEdgeUpdateStart: Hn,
@@ -28599,15 +28599,15 @@
                     selectionMode: B,
                     deleteKeyCode: W,
                     multiSelectionKeyCode: Q,
                     panActivationKeyCode: H,
                     zoomActivationKeyCode: J,
                     onlyRenderVisibleElements: xe,
                     selectNodesOnDrag: ve,
-                    defaultViewport: Je,
+                    defaultViewport: tt,
                     translateExtent: ni,
                     minZoom: Gt,
                     maxZoom: sr,
                     preventScrolling: Lr,
                     zoomOnScroll: Un,
                     zoomOnPinch: Ht,
                     zoomOnDoubleClick: ri,
@@ -28686,15 +28686,15 @@
                     autoPanOnConnect: b1,
                     autoPanOnNodeDrag: S1,
                     onError: w1,
                     connectionRadius: _1,
                     isValidConnection: Ac
                 }), K.jsx(Cre, {
                     onSelectionChange: $
-                }), et, K.jsx(Jne, {
+                }), nt, K.jsx(Jne, {
                     proOptions: m1,
                     position: g1
                 }), K.jsx(Rre, {
                     rfId: kc,
                     disableKeyboardA11y: Hh
                 })]
             })
@@ -37105,32 +37105,32 @@
                 }
             }
         })
     },
     Fe = "positive_conditioning",
     qe = "negative_conditioning",
     dn = "text_to_latents",
-    nt = "latents_to_image",
+    We = "latents_to_image",
     fu = "nsfw_checker",
     Qc = "invisible_watermark",
     Le = "noise",
     Fi = "rand_int",
     Co = "range_of_size",
     lr = "iterate",
     pt = "main_model_loader",
     Zc = "vae_loader",
     uce = "lora_loader",
-    rt = "clip_skip",
+    it = "clip_skip",
     bt = "image_to_latents",
     Kt = "latents_to_latents",
     Qn = "resize_image",
     Mi = "inpaint",
     Rp = "control_net_collect",
     zb = "dynamic_prompt",
-    lt = "metadata_accumulator",
+    Ke = "metadata_accumulator",
     KE = "esrgan",
     en = "sdxl_model_loader",
     is = "t2l_sdxl",
     Ni = "l2l_sdxl",
     zl = "sdxl_refiner_model_loader",
     Ip = "sdxl_refiner_positive_conditioning",
     Mp = "sdxl_refiner_negative_conditioning",
@@ -39378,47 +39378,47 @@
 });
 Se.prototype.on.call(Se.prototype, "listeningChange.konva", function() {
     this._clearSelfAndDescendantCache(g2)
 });
 Se.prototype.on.call(Se.prototype, "opacityChange.konva", function() {
     this._clearSelfAndDescendantCache(bg)
 });
-const it = Ah.Factory.addGetterSetter;
-it(Se, "zIndex");
-it(Se, "absolutePosition");
-it(Se, "position");
-it(Se, "x", 0, (0, zt.getNumberValidator)());
-it(Se, "y", 0, (0, zt.getNumberValidator)());
-it(Se, "globalCompositeOperation", "source-over", (0, zt.getStringValidator)());
-it(Se, "opacity", 1, (0, zt.getNumberValidator)());
-it(Se, "name", "", (0, zt.getStringValidator)());
-it(Se, "id", "", (0, zt.getStringValidator)());
-it(Se, "rotation", 0, (0, zt.getNumberValidator)());
+const ot = Ah.Factory.addGetterSetter;
+ot(Se, "zIndex");
+ot(Se, "absolutePosition");
+ot(Se, "position");
+ot(Se, "x", 0, (0, zt.getNumberValidator)());
+ot(Se, "y", 0, (0, zt.getNumberValidator)());
+ot(Se, "globalCompositeOperation", "source-over", (0, zt.getStringValidator)());
+ot(Se, "opacity", 1, (0, zt.getNumberValidator)());
+ot(Se, "name", "", (0, zt.getStringValidator)());
+ot(Se, "id", "", (0, zt.getStringValidator)());
+ot(Se, "rotation", 0, (0, zt.getNumberValidator)());
 Ah.Factory.addComponentsGetterSetter(Se, "scale", ["x", "y"]);
-it(Se, "scaleX", 1, (0, zt.getNumberValidator)());
-it(Se, "scaleY", 1, (0, zt.getNumberValidator)());
+ot(Se, "scaleX", 1, (0, zt.getNumberValidator)());
+ot(Se, "scaleY", 1, (0, zt.getNumberValidator)());
 Ah.Factory.addComponentsGetterSetter(Se, "skew", ["x", "y"]);
-it(Se, "skewX", 0, (0, zt.getNumberValidator)());
-it(Se, "skewY", 0, (0, zt.getNumberValidator)());
+ot(Se, "skewX", 0, (0, zt.getNumberValidator)());
+ot(Se, "skewY", 0, (0, zt.getNumberValidator)());
 Ah.Factory.addComponentsGetterSetter(Se, "offset", ["x", "y"]);
-it(Se, "offsetX", 0, (0, zt.getNumberValidator)());
-it(Se, "offsetY", 0, (0, zt.getNumberValidator)());
-it(Se, "dragDistance", null, (0, zt.getNumberValidator)());
-it(Se, "width", 0, (0, zt.getNumberValidator)());
-it(Se, "height", 0, (0, zt.getNumberValidator)());
-it(Se, "listening", !0, (0, zt.getBooleanValidator)());
-it(Se, "preventDefault", !0, (0, zt.getBooleanValidator)());
-it(Se, "filters", null, function(e) {
+ot(Se, "offsetX", 0, (0, zt.getNumberValidator)());
+ot(Se, "offsetY", 0, (0, zt.getNumberValidator)());
+ot(Se, "dragDistance", null, (0, zt.getNumberValidator)());
+ot(Se, "width", 0, (0, zt.getNumberValidator)());
+ot(Se, "height", 0, (0, zt.getNumberValidator)());
+ot(Se, "listening", !0, (0, zt.getBooleanValidator)());
+ot(Se, "preventDefault", !0, (0, zt.getBooleanValidator)());
+ot(Se, "filters", null, function(e) {
     return this._filterUpToDate = !1, e
 });
-it(Se, "visible", !0, (0, zt.getBooleanValidator)());
-it(Se, "transformsEnabled", "all", (0, zt.getStringValidator)());
-it(Se, "size");
-it(Se, "dragBoundFunc");
-it(Se, "draggable", !1, (0, zt.getBooleanValidator)());
+ot(Se, "visible", !0, (0, zt.getBooleanValidator)());
+ot(Se, "transformsEnabled", "all", (0, zt.getStringValidator)());
+ot(Se, "size");
+ot(Se, "dragBoundFunc");
+ot(Se, "draggable", !1, (0, zt.getBooleanValidator)());
 Ah.Factory.backCompat(Se, {
     rotateDeg: "rotate",
     setRotationDeg: "setRotation",
     getRotationDeg: "getRotation"
 });
 var gl = {};
 Object.defineProperty(gl, "__esModule", {
@@ -42788,16 +42788,16 @@
 Ai.Factory.addGetterSetter(an, "textDecoration", null);
 Ai.Factory.addGetterSetter(an, "kerningFunc", null);
 var Cv = {};
 Object.defineProperty(Cv, "__esModule", {
     value: !0
 });
 Cv.Transformer = void 0;
-const Qe = Rt,
-    Ye = Te,
+const Je = Rt,
+    Ze = Te,
     h6 = wt,
     yfe = sn,
     vfe = Ih,
     p6 = bc,
     si = Pe,
     la = de,
     bfe = Pe;
@@ -42815,17 +42815,17 @@
         "bottom-center": 180,
         "bottom-right": 135
     };
 const xfe = "ontouchstart" in si.Konva._global;
 
 function Cfe(e, t) {
     if (e === "rotater") return "crosshair";
-    t += Qe.Util.degToRad(wfe[e] || 0);
-    var n = (Qe.Util.radToDeg(t) % 360 + 360) % 360;
-    return Qe.Util._inRange(n, 315 + 22.5, 360) || Qe.Util._inRange(n, 0, 22.5) ? "ns-resize" : Qe.Util._inRange(n, 45 - 22.5, 45 + 22.5) ? "nesw-resize" : Qe.Util._inRange(n, 90 - 22.5, 90 + 22.5) ? "ew-resize" : Qe.Util._inRange(n, 135 - 22.5, 135 + 22.5) ? "nwse-resize" : Qe.Util._inRange(n, 180 - 22.5, 180 + 22.5) ? "ns-resize" : Qe.Util._inRange(n, 225 - 22.5, 225 + 22.5) ? "nesw-resize" : Qe.Util._inRange(n, 270 - 22.5, 270 + 22.5) ? "ew-resize" : Qe.Util._inRange(n, 315 - 22.5, 315 + 22.5) ? "nwse-resize" : (Qe.Util.error("Transformer has unknown angle for cursor detection: " + n), "pointer")
+    t += Je.Util.degToRad(wfe[e] || 0);
+    var n = (Je.Util.radToDeg(t) % 360 + 360) % 360;
+    return Je.Util._inRange(n, 315 + 22.5, 360) || Je.Util._inRange(n, 0, 22.5) ? "ns-resize" : Je.Util._inRange(n, 45 - 22.5, 45 + 22.5) ? "nesw-resize" : Je.Util._inRange(n, 90 - 22.5, 90 + 22.5) ? "ew-resize" : Je.Util._inRange(n, 135 - 22.5, 135 + 22.5) ? "nwse-resize" : Je.Util._inRange(n, 180 - 22.5, 180 + 22.5) ? "ns-resize" : Je.Util._inRange(n, 225 - 22.5, 225 + 22.5) ? "nesw-resize" : Je.Util._inRange(n, 270 - 22.5, 270 + 22.5) ? "ew-resize" : Je.Util._inRange(n, 315 - 22.5, 315 + 22.5) ? "nwse-resize" : (Je.Util.error("Transformer has unknown angle for cursor detection: " + n), "pointer")
 }
 var Qm = ["top-left", "top-center", "top-right", "middle-right", "middle-left", "bottom-left", "bottom-center", "bottom-right"],
     m6 = 1e8;
 
 function Tfe(e) {
     return {
         x: e.x + e.width / 2 * Math.cos(e.rotation) + e.height / 2 * Math.sin(-e.rotation),
@@ -42861,25 +42861,25 @@
     constructor(t) {
         super(t), this._transforming = !1, this._createElements(), this._handleMouseMove = this._handleMouseMove.bind(this), this._handleMouseUp = this._handleMouseUp.bind(this), this.update = this.update.bind(this), this.on(Sfe, this.update), this.getNode() && this.update()
     }
     attachTo(t) {
         return this.setNode(t), this
     }
     setNode(t) {
-        return Qe.Util.warn("tr.setNode(shape), tr.node(shape) and tr.attachTo(shape) methods are deprecated. Please use tr.nodes(nodesArray) instead."), this.setNodes([t])
+        return Je.Util.warn("tr.setNode(shape), tr.node(shape) and tr.attachTo(shape) methods are deprecated. Please use tr.nodes(nodesArray) instead."), this.setNodes([t])
     }
     getNode() {
         return this._nodes && this._nodes[0]
     }
     _getEventNamespace() {
         return NN + this._id
     }
     setNodes(t = []) {
         this._nodes && this._nodes.length && this.detach();
-        const n = t.filter(i => i.isAncestorOf(this) ? (Qe.Util.error("Konva.Transformer cannot be an a child of the node you are trying to attach"), !1) : !0);
+        const n = t.filter(i => i.isAncestorOf(this) ? (Je.Util.error("Konva.Transformer cannot be an a child of the node you are trying to attach"), !1) : !0);
         this._nodes = t = n, t.length === 1 && this.useSingleNodeRotation() ? this.rotation(t[0].getAbsoluteRotation()) : this.rotation(0), this._nodes.forEach(i => {
             const o = () => {
                     this.nodes().length === 1 && this.useSingleNodeRotation() && this.rotation(this.nodes()[0].getAbsoluteRotation()), this._resetTransformCache(), !this._transforming && !this.isDragging() && this.update()
                 },
                 s = i._attrsAffectingSize.map(a => a + "Change." + this._getEventNamespace()).join(" ");
             i.on(s, o), i.on(_fe.map(a => a + `.${this._getEventNamespace()}`).join(" "), o), i.on(`absoluteTransformChange.${this._getEventNamespace()}`, o), this._proxyDrag(i)
         }), this._resetTransformCache();
@@ -42976,15 +42976,15 @@
                 }],
                 f = u.getAbsoluteTransform();
             d.forEach(function(h) {
                 var p = f.point(h);
                 n.push(p)
             })
         });
-        const r = new Qe.Transform;
+        const r = new Je.Transform;
         r.rotate(-si.Konva.getAngle(this.rotation()));
         var i, o, s, a;
         n.forEach(function(u) {
             var c = r.point(u);
             i === void 0 && (i = s = c.x, o = a = c.y), i = Math.min(i, c.x), o = Math.min(o, c.y), s = Math.max(s, c.x), a = Math.max(a, c.y)
         }), r.invert();
         const l = r.point({
@@ -43046,15 +43046,15 @@
             name: "back",
             width: 0,
             height: 0,
             draggable: !0,
             sceneFunc(n) {
                 var r = this.getParent(),
                     i = r.padding();
-                n.beginPath(), n.rect(-i, -i, this.width() + i * 2, this.height() + i * 2), n.moveTo(this.width() / 2, -i), r.rotateEnabled() && n.lineTo(this.width() / 2, -r.rotateAnchorOffset() * Qe.Util._sign(this.height()) - i), n.fillStrokeShape(this)
+                n.beginPath(), n.rect(-i, -i, this.width() + i * 2, this.height() + i * 2), n.moveTo(this.width() / 2, -i), r.rotateEnabled() && n.lineTo(this.width() / 2, -r.rotateAnchorOffset() * Je.Util._sign(this.height()) - i), n.fillStrokeShape(this)
             },
             hitFunc: (n, r) => {
                 if (this.shouldOverdrawWholeArea()) {
                     var i = this.padding();
                     n.beginPath(), n.rect(-i, -i, r.width() + i * 2, r.height() + i * 2), n.fillStrokeShape(r)
                 }
             }
@@ -43234,24 +43234,24 @@
                 })
             }), this._movingAnchorName = null
         }
     }
     _fitNodesInto(t, n) {
         var r = this._getNodeRect();
         const i = 1;
-        if (Qe.Util._inRange(t.width, -this.padding() * 2 - i, i)) {
+        if (Je.Util._inRange(t.width, -this.padding() * 2 - i, i)) {
             this.update();
             return
         }
-        if (Qe.Util._inRange(t.height, -this.padding() * 2 - i, i)) {
+        if (Je.Util._inRange(t.height, -this.padding() * 2 - i, i)) {
             this.update();
             return
         }
         const o = this.flipEnabled();
-        var s = new Qe.Transform;
+        var s = new Je.Transform;
         if (s.rotate(si.Konva.getAngle(this.rotation())), this._movingAnchorName && t.width < 0 && this._movingAnchorName.indexOf("left") >= 0) {
             const d = s.point({
                 x: -this.padding() * 2,
                 y: 0
             });
             if (t.x += d.x, t.y += d.y, t.width += this.padding() * 2, this._movingAnchorName = this._movingAnchorName.replace("left", "right"), this._anchorDragOffset.x -= d.x, this._anchorDragOffset.y -= d.y, !o) {
                 this.update();
@@ -43284,48 +43284,48 @@
             if (this._movingAnchorName = this._movingAnchorName.replace("bottom", "top"), this._anchorDragOffset.x -= d.x, this._anchorDragOffset.y -= d.y, t.height += this.padding() * 2, !o) {
                 this.update();
                 return
             }
         }
         if (this.boundBoxFunc()) {
             const d = this.boundBoxFunc()(r, t);
-            d ? t = d : Qe.Util.warn("boundBoxFunc returned falsy. You should return new bound rect from it!")
+            d ? t = d : Je.Util.warn("boundBoxFunc returned falsy. You should return new bound rect from it!")
         }
         const a = 1e7,
-            l = new Qe.Transform;
+            l = new Je.Transform;
         l.translate(r.x, r.y), l.rotate(r.rotation), l.scale(r.width / a, r.height / a);
-        const u = new Qe.Transform;
+        const u = new Je.Transform;
         u.translate(t.x, t.y), u.rotate(t.rotation), u.scale(t.width / a, t.height / a);
         const c = u.multiply(l.invert());
         this._nodes.forEach(d => {
             var f;
             const h = d.getParent().getAbsoluteTransform(),
                 p = d.getTransform().copy();
             p.translate(d.offsetX(), d.offsetY());
-            const m = new Qe.Transform;
+            const m = new Je.Transform;
             m.multiply(h.copy().invert()).multiply(c).multiply(h).multiply(p);
             const S = m.decompose();
             d.setAttrs(S), this._fire("transform", {
                 evt: n,
                 target: d
             }), d._fire("transform", {
                 evt: n,
                 target: d
             }), (f = d.getLayer()) === null || f === void 0 || f.batchDraw()
-        }), this.rotation(Qe.Util._getRotation(t.rotation)), this._resetTransformCache(), this.update(), this.getLayer().batchDraw()
+        }), this.rotation(Je.Util._getRotation(t.rotation)), this._resetTransformCache(), this.update(), this.getLayer().batchDraw()
     }
     forceUpdate() {
         this._resetTransformCache(), this.update()
     }
     _batchChangeChild(t, n) {
         this.findOne(t).setAttrs(n)
     }
     update() {
         var t, n = this._getNodeRect();
-        this.rotation(Qe.Util._getRotation(n.rotation));
+        this.rotation(Je.Util._getRotation(n.rotation));
         var r = n.width,
             i = n.height,
             o = this.enabledAnchors(),
             s = this.resizeEnabled(),
             a = this.padding(),
             l = this.anchorSize();
         const u = this.find("._anchor");
@@ -43382,15 +43382,15 @@
             x: r,
             y: i,
             offsetX: l / 2 - a,
             offsetY: l / 2 - a,
             visible: s && o.indexOf("bottom-right") >= 0
         }), this._batchChangeChild(".rotater", {
             x: r / 2,
-            y: -this.rotateAnchorOffset() * Qe.Util._sign(i) - a,
+            y: -this.rotateAnchorOffset() * Je.Util._sign(i) - a,
             visible: this.rotateEnabled()
         }), this._batchChangeChild(".back", {
             width: r,
             height: i,
             visible: this.borderEnabled(),
             stroke: this.borderStroke(),
             strokeWidth: this.borderStrokeWidth(),
@@ -43431,49 +43431,49 @@
             height: 0
         }
     }
 }
 Cv.Transformer = ze;
 
 function Afe(e) {
-    return e instanceof Array || Qe.Util.warn("enabledAnchors value should be an array"), e instanceof Array && e.forEach(function(t) {
-        Qm.indexOf(t) === -1 && Qe.Util.warn("Unknown anchor name: " + t + ". Available names are: " + Qm.join(", "))
+    return e instanceof Array || Je.Util.warn("enabledAnchors value should be an array"), e instanceof Array && e.forEach(function(t) {
+        Qm.indexOf(t) === -1 && Je.Util.warn("Unknown anchor name: " + t + ". Available names are: " + Qm.join(", "))
     }), e || []
 }
 ze.prototype.className = "Transformer";
 (0, bfe._registerNode)(ze);
-Ye.Factory.addGetterSetter(ze, "enabledAnchors", Qm, Afe);
-Ye.Factory.addGetterSetter(ze, "flipEnabled", !0, (0, la.getBooleanValidator)());
-Ye.Factory.addGetterSetter(ze, "resizeEnabled", !0);
-Ye.Factory.addGetterSetter(ze, "anchorSize", 10, (0, la.getNumberValidator)());
-Ye.Factory.addGetterSetter(ze, "rotateEnabled", !0);
-Ye.Factory.addGetterSetter(ze, "rotationSnaps", []);
-Ye.Factory.addGetterSetter(ze, "rotateAnchorOffset", 50, (0, la.getNumberValidator)());
-Ye.Factory.addGetterSetter(ze, "rotationSnapTolerance", 5, (0, la.getNumberValidator)());
-Ye.Factory.addGetterSetter(ze, "borderEnabled", !0);
-Ye.Factory.addGetterSetter(ze, "anchorStroke", "rgb(0, 161, 255)");
-Ye.Factory.addGetterSetter(ze, "anchorStrokeWidth", 1, (0, la.getNumberValidator)());
-Ye.Factory.addGetterSetter(ze, "anchorFill", "white");
-Ye.Factory.addGetterSetter(ze, "anchorCornerRadius", 0, (0, la.getNumberValidator)());
-Ye.Factory.addGetterSetter(ze, "borderStroke", "rgb(0, 161, 255)");
-Ye.Factory.addGetterSetter(ze, "borderStrokeWidth", 1, (0, la.getNumberValidator)());
-Ye.Factory.addGetterSetter(ze, "borderDash");
-Ye.Factory.addGetterSetter(ze, "keepRatio", !0);
-Ye.Factory.addGetterSetter(ze, "shiftBehavior", "default");
-Ye.Factory.addGetterSetter(ze, "centeredScaling", !1);
-Ye.Factory.addGetterSetter(ze, "ignoreStroke", !1);
-Ye.Factory.addGetterSetter(ze, "padding", 0, (0, la.getNumberValidator)());
-Ye.Factory.addGetterSetter(ze, "node");
-Ye.Factory.addGetterSetter(ze, "nodes");
-Ye.Factory.addGetterSetter(ze, "boundBoxFunc");
-Ye.Factory.addGetterSetter(ze, "anchorDragBoundFunc");
-Ye.Factory.addGetterSetter(ze, "anchorStyleFunc");
-Ye.Factory.addGetterSetter(ze, "shouldOverdrawWholeArea", !1);
-Ye.Factory.addGetterSetter(ze, "useSingleNodeRotation", !0);
-Ye.Factory.backCompat(ze, {
+Ze.Factory.addGetterSetter(ze, "enabledAnchors", Qm, Afe);
+Ze.Factory.addGetterSetter(ze, "flipEnabled", !0, (0, la.getBooleanValidator)());
+Ze.Factory.addGetterSetter(ze, "resizeEnabled", !0);
+Ze.Factory.addGetterSetter(ze, "anchorSize", 10, (0, la.getNumberValidator)());
+Ze.Factory.addGetterSetter(ze, "rotateEnabled", !0);
+Ze.Factory.addGetterSetter(ze, "rotationSnaps", []);
+Ze.Factory.addGetterSetter(ze, "rotateAnchorOffset", 50, (0, la.getNumberValidator)());
+Ze.Factory.addGetterSetter(ze, "rotationSnapTolerance", 5, (0, la.getNumberValidator)());
+Ze.Factory.addGetterSetter(ze, "borderEnabled", !0);
+Ze.Factory.addGetterSetter(ze, "anchorStroke", "rgb(0, 161, 255)");
+Ze.Factory.addGetterSetter(ze, "anchorStrokeWidth", 1, (0, la.getNumberValidator)());
+Ze.Factory.addGetterSetter(ze, "anchorFill", "white");
+Ze.Factory.addGetterSetter(ze, "anchorCornerRadius", 0, (0, la.getNumberValidator)());
+Ze.Factory.addGetterSetter(ze, "borderStroke", "rgb(0, 161, 255)");
+Ze.Factory.addGetterSetter(ze, "borderStrokeWidth", 1, (0, la.getNumberValidator)());
+Ze.Factory.addGetterSetter(ze, "borderDash");
+Ze.Factory.addGetterSetter(ze, "keepRatio", !0);
+Ze.Factory.addGetterSetter(ze, "shiftBehavior", "default");
+Ze.Factory.addGetterSetter(ze, "centeredScaling", !1);
+Ze.Factory.addGetterSetter(ze, "ignoreStroke", !1);
+Ze.Factory.addGetterSetter(ze, "padding", 0, (0, la.getNumberValidator)());
+Ze.Factory.addGetterSetter(ze, "node");
+Ze.Factory.addGetterSetter(ze, "nodes");
+Ze.Factory.addGetterSetter(ze, "boundBoxFunc");
+Ze.Factory.addGetterSetter(ze, "anchorDragBoundFunc");
+Ze.Factory.addGetterSetter(ze, "anchorStyleFunc");
+Ze.Factory.addGetterSetter(ze, "shouldOverdrawWholeArea", !1);
+Ze.Factory.addGetterSetter(ze, "useSingleNodeRotation", !0);
+Ze.Factory.backCompat(ze, {
     lineEnabled: "borderEnabled",
     rotateHandlerOffset: "rotateAnchorOffset",
     enabledHandlers: "enabledAnchors"
 });
 var Tv = {};
 Object.defineProperty(Tv, "__esModule", {
     value: !0
@@ -44358,15 +44358,15 @@
             isFullyTransparent: r
         } = ype(e.data), i = vpe(t.data);
         return n ? r ? "txt2img" : "outpaint" : i ? "inpaint" : "img2img"
     }, Spe = e => oR(e, n => n.isEnabled && (!!n.processedControlImage || n.processorType === "none" && !!n.controlImage)), Kv = (e, t, n) => {
         const {
             isEnabled: r,
             controlNets: i
-        } = e.controlNet, o = Spe(i), s = t.nodes[lt];
+        } = e.controlNet, o = Spe(i), s = t.nodes[Ke];
         if (r && o.length && o.length) {
             const a = {
                 id: Rp,
                 type: "collect",
                 is_intermediate: !0
             };
             t.nodes[Rp] = a, t.edges.push({
@@ -44430,15 +44430,15 @@
             iterations: r,
             seed: i,
             shouldRandomizeSeed: o
         } = e.generation, {
             combinatorial: s,
             isEnabled: a,
             maxPrompts: l
-        } = e.dynamicPrompts, u = t.nodes[lt];
+        } = e.dynamicPrompts, u = t.nodes[Ke];
         if (a) {
             rY(t.nodes[Fe], "prompt");
             const c = {
                     id: zb,
                     type: "dynamic_prompt",
                     is_intermediate: !0,
                     max_prompts: s ? l : r,
@@ -44470,15 +44470,15 @@
                     }
                 }), u && t.edges.push({
                     source: {
                         node_id: lr,
                         field: "item"
                     },
                     destination: {
-                        node_id: lt,
+                        node_id: Ke,
                         field: "positive_prompt"
                     }
                 }), o) {
                 const f = {
                     id: Fi,
                     type: "rand_int",
                     is_intermediate: !0
@@ -44494,15 +44494,15 @@
                     }
                 }), u && t.edges.push({
                     source: {
                         node_id: Fi,
                         field: "a"
                     },
                     destination: {
-                        node_id: lt,
+                        node_id: Ke,
                         field: "seed"
                     }
                 })
             } else t.nodes[Le].seed = i, u && (u.seed = i)
         } else {
             u && (u.positive_prompt = n);
             const c = {
@@ -44537,15 +44537,15 @@
                     }
                 }), u && t.edges.push({
                     source: {
                         node_id: lr,
                         field: "item"
                     },
                     destination: {
-                        node_id: lt,
+                        node_id: Ke,
                         field: "seed"
                     }
                 }), o) {
                 const f = {
                     id: Fi,
                     type: "rand_int",
                     is_intermediate: !0
@@ -44561,16 +44561,16 @@
                     }
                 })
             } else c.start = i
         }
     }, Mh = (e, t, n) => {
         const {
             loras: r
-        } = e.lora, i = hR(r), o = t.nodes[lt];
-        i > 0 && (t.edges = t.edges.filter(l => !(l.source.node_id === pt && ["unet"].includes(l.source.field))), t.edges = t.edges.filter(l => !(l.source.node_id === rt && ["clip"].includes(l.source.field))));
+        } = e.lora, i = hR(r), o = t.nodes[Ke];
+        i > 0 && (t.edges = t.edges.filter(l => !(l.source.node_id === pt && ["unet"].includes(l.source.field))), t.edges = t.edges.filter(l => !(l.source.node_id === it && ["clip"].includes(l.source.field))));
         let s = "",
             a = 0;
         Qa(r, l => {
             const {
                 model_name: u,
                 base_model: c,
                 weight: d
@@ -44597,15 +44597,15 @@
                 },
                 destination: {
                     node_id: f,
                     field: "unet"
                 }
             }), t.edges.push({
                 source: {
-                    node_id: rt,
+                    node_id: it,
                     field: "clip"
                 },
                 destination: {
                     node_id: f,
                     field: "clip"
                 }
             })) : (t.edges.push({
@@ -44663,18 +44663,18 @@
         memoizeOptions: {
             equalityCheck: S0
         }
     }), i4e = Jn(e => e.ui, e => e, {
         memoizeOptions: {
             equalityCheck: S0
         }
-    }), xl = (e, t, n = nt) => {
+    }), xl = (e, t, n = We) => {
         const i = jN(e) === "unifiedCanvas" ? !e.canvas.shouldAutoSave : !1,
             o = t.nodes[n],
-            s = t.nodes[lt];
+            s = t.nodes[Ke];
         if (!o) return;
         o.is_intermediate = !0;
         const a = {
             id: fu,
             type: "img_nsfw",
             is_intermediate: i
         };
@@ -44685,38 +44685,38 @@
             },
             destination: {
                 node_id: fu,
                 field: "image"
             }
         }), s && t.edges.push({
             source: {
-                node_id: lt,
+                node_id: Ke,
                 field: "metadata"
             },
             destination: {
                 node_id: fu,
                 field: "metadata"
             }
         })
     }, Nh = (e, t) => {
         const {
             vae: n
-        } = e.generation, r = !n, i = t.nodes[lt];
+        } = e.generation, r = !n, i = t.nodes[Ke];
         r || (t.nodes[Zc] = {
             type: "vae_loader",
             id: Zc,
             is_intermediate: !0,
             vae_model: n
         }), (t.id === bC || t.id === Xm) && t.edges.push({
             source: {
                 node_id: r ? pt : Zc,
                 field: "vae"
             },
             destination: {
-                node_id: nt,
+                node_id: We,
                 field: "vae"
             }
         }), t.id === Xm && t.edges.push({
             source: {
                 node_id: r ? pt : Zc,
                 field: "vae"
             },
@@ -44730,19 +44730,19 @@
                 field: "vae"
             },
             destination: {
                 node_id: Mi,
                 field: "vae"
             }
         }), n && i && (i.vae = n)
-    }, Cl = (e, t, n = nt) => {
+    }, Cl = (e, t, n = We) => {
         const i = jN(e) === "unifiedCanvas" ? !e.canvas.shouldAutoSave : !1,
             o = t.nodes[n],
             s = t.nodes[fu],
-            a = t.nodes[lt];
+            a = t.nodes[Ke];
         if (!o) return;
         const l = {
             id: Qc,
             type: "img_watermark",
             is_intermediate: i
         };
         t.nodes[Qc] = l, o.is_intermediate = !0, s ? (s.is_intermediate = !0, t.edges.push({
@@ -44761,15 +44761,15 @@
             },
             destination: {
                 node_id: Qc,
                 field: "image"
             }
         }), a && t.edges.push({
             source: {
-                node_id: lt,
+                node_id: Ke,
                 field: "metadata"
             },
             destination: {
                 node_id: Qc,
                 field: "metadata"
             }
         })
@@ -44819,17 +44819,17 @@
                     },
                     [pt]: {
                         type: "main_model_loader",
                         id: pt,
                         is_intermediate: !0,
                         model: o
                     },
-                    [rt]: {
+                    [it]: {
                         type: "clip_skip",
-                        id: rt,
+                        id: it,
                         is_intermediate: !0,
                         skipped_layers: c
                     },
                     [Kt]: {
                         type: "l2l",
                         id: Kt,
                         is_intermediate: !0,
@@ -44839,54 +44839,54 @@
                         strength: u
                     },
                     [bt]: {
                         type: "i2l",
                         id: bt,
                         is_intermediate: !0
                     },
-                    [nt]: {
+                    [We]: {
                         type: "l2i",
-                        id: nt,
+                        id: We,
                         is_intermediate: !m
                     }
                 },
                 edges: [{
                     source: {
                         node_id: pt,
                         field: "clip"
                     },
                     destination: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     }
                 }, {
                     source: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     },
                     destination: {
                         node_id: Fe,
                         field: "clip"
                     }
                 }, {
                     source: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     },
                     destination: {
                         node_id: qe,
                         field: "clip"
                     }
                 }, {
                     source: {
                         node_id: Kt,
                         field: "latents"
                     },
                     destination: {
-                        node_id: nt,
+                        node_id: We,
                         field: "latents"
                     }
                 }, {
                     source: {
                         node_id: bt,
                         field: "latents"
                     },
@@ -44988,16 +44988,16 @@
                 field: "height"
             },
             destination: {
                 node_id: Le,
                 field: "height"
             }
         });
-        return v.nodes[lt] = {
-            id: lt,
+        return v.nodes[Ke] = {
+            id: Ke,
             type: "metadata_accumulator",
             generation_mode: "img2img",
             cfg_scale: s,
             height: p,
             width: h,
             positive_prompt: "",
             negative_prompt: i,
@@ -45008,15 +45008,24 @@
             scheduler: a,
             vae: void 0,
             controlnets: [],
             loras: [],
             clip_skip: c,
             strength: u,
             init_image: t.image_name
-        }, Mh(e, v, Kt), Nh(e, v), xc(e, v), Kv(e, v, Kt), e.system.shouldUseNSFWChecker && xl(e, v), e.system.shouldUseWatermarker && Cl(e, v), v
+        }, v.edges.push({
+            source: {
+                node_id: Ke,
+                field: "metadata"
+            },
+            destination: {
+                node_id: We,
+                field: "metadata"
+            }
+        }), Mh(e, v, Kt), Nh(e, v), xc(e, v), Kv(e, v, Kt), e.system.shouldUseNSFWChecker && xl(e, v), e.system.shouldUseWatermarker && Cl(e, v), v
     }, wpe = (e, t, n) => {
         const r = fe("nodes"),
             {
                 positivePrompt: i,
                 negativePrompt: o,
                 model: s,
                 cfgScale: a,
@@ -45086,17 +45095,17 @@
                 },
                 [pt]: {
                     type: "main_model_loader",
                     id: pt,
                     is_intermediate: !0,
                     model: s
                 },
-                [rt]: {
+                [it]: {
                     type: "clip_skip",
-                    id: rt,
+                    id: it,
                     is_intermediate: !0,
                     skipped_layers: _
                 },
                 [Co]: {
                     type: "range_of_size",
                     id: Co,
                     is_intermediate: !0,
@@ -45120,29 +45129,29 @@
                 }
             }, {
                 source: {
                     node_id: pt,
                     field: "clip"
                 },
                 destination: {
-                    node_id: rt,
+                    node_id: it,
                     field: "clip"
                 }
             }, {
                 source: {
-                    node_id: rt,
+                    node_id: it,
                     field: "clip"
                 },
                 destination: {
                     node_id: Fe,
                     field: "clip"
                 }
             }, {
                 source: {
-                    node_id: rt,
+                    node_id: it,
                     field: "clip"
                 },
                 destination: {
                     node_id: qe,
                     field: "clip"
                 }
             }, {
@@ -45255,23 +45264,23 @@
                     },
                     [pt]: {
                         type: "main_model_loader",
                         id: pt,
                         is_intermediate: !0,
                         model: i
                     },
-                    [rt]: {
+                    [it]: {
                         type: "clip_skip",
-                        id: rt,
+                        id: it,
                         is_intermediate: !0,
                         skipped_layers: l
                     },
-                    [nt]: {
+                    [We]: {
                         type: "l2i",
-                        id: nt,
+                        id: We,
                         is_intermediate: !h
                     }
                 },
                 edges: [{
                     source: {
                         node_id: qe,
                         field: "conditioning"
@@ -45291,29 +45300,29 @@
                     }
                 }, {
                     source: {
                         node_id: pt,
                         field: "clip"
                     },
                     destination: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     }
                 }, {
                     source: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     },
                     destination: {
                         node_id: Fe,
                         field: "clip"
                     }
                 }, {
                     source: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     },
                     destination: {
                         node_id: qe,
                         field: "clip"
                     }
                 }, {
@@ -45327,30 +45336,30 @@
                     }
                 }, {
                     source: {
                         node_id: dn,
                         field: "latents"
                     },
                     destination: {
-                        node_id: nt,
+                        node_id: We,
                         field: "latents"
                     }
                 }, {
                     source: {
                         node_id: Le,
                         field: "noise"
                     },
                     destination: {
                         node_id: dn,
                         field: "noise"
                     }
                 }]
             };
-        return m.nodes[lt] = {
-            id: lt,
+        return m.nodes[Ke] = {
+            id: Ke,
             type: "metadata_accumulator",
             generation_mode: "txt2img",
             cfg_scale: o,
             height: f,
             width: d,
             positive_prompt: "",
             negative_prompt: r,
@@ -45359,15 +45368,24 @@
             steps: a,
             rand_device: p ? "cpu" : "cuda",
             scheduler: s,
             vae: void 0,
             controlnets: [],
             loras: [],
             clip_skip: l
-        }, Mh(e, m, dn), Nh(e, m), xc(e, m), Kv(e, m, dn), e.system.shouldUseNSFWChecker && xl(e, m), e.system.shouldUseWatermarker && Cl(e, m), m
+        }, m.edges.push({
+            source: {
+                node_id: Ke,
+                field: "metadata"
+            },
+            destination: {
+                node_id: We,
+                field: "metadata"
+            }
+        }), Mh(e, m, dn), Nh(e, m), xc(e, m), Kv(e, m, dn), e.system.shouldUseNSFWChecker && xl(e, m), e.system.shouldUseWatermarker && Cl(e, m), m
     }, Cpe = (e, t, n, r) => {
         let i;
         if (t === "txt2img") i = xpe(e);
         else if (t === "img2img") {
             if (!n) throw new Error("Missing canvas init image");
             i = _pe(e, n)
         } else {
@@ -45479,17 +45497,17 @@
                 id: Xm,
                 nodes: {
                     [pt]: {
                         type: "main_model_loader",
                         id: pt,
                         model: i
                     },
-                    [rt]: {
+                    [it]: {
                         type: "clip_skip",
-                        id: rt,
+                        id: it,
                         skipped_layers: h
                     },
                     [Fe]: {
                         type: "compel",
                         id: Fe,
                         prompt: n
                     },
@@ -45499,17 +45517,17 @@
                         prompt: r
                     },
                     [Le]: {
                         type: "noise",
                         id: Le,
                         use_cpu: v
                     },
-                    [nt]: {
+                    [We]: {
                         type: "l2i",
-                        id: nt,
+                        id: We,
                         fp32: S === "fp32"
                     },
                     [Kt]: {
                         type: "l2l",
                         id: Kt,
                         cfg_scale: o,
                         scheduler: s,
@@ -45533,42 +45551,42 @@
                     }
                 }, {
                     source: {
                         node_id: pt,
                         field: "clip"
                     },
                     destination: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     }
                 }, {
                     source: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     },
                     destination: {
                         node_id: Fe,
                         field: "clip"
                     }
                 }, {
                     source: {
-                        node_id: rt,
+                        node_id: it,
                         field: "clip"
                     },
                     destination: {
                         node_id: qe,
                         field: "clip"
                     }
                 }, {
                     source: {
                         node_id: Kt,
                         field: "latents"
                     },
                     destination: {
-                        node_id: nt,
+                        node_id: We,
                         field: "latents"
                     }
                 }, {
                     source: {
                         node_id: bt,
                         field: "latents"
                     },
@@ -45661,16 +45679,16 @@
                 field: "height"
             },
             destination: {
                 node_id: Le,
                 field: "height"
             }
         });
-        return y.nodes[lt] = {
-            id: lt,
+        return y.nodes[Ke] = {
+            id: Ke,
             type: "metadata_accumulator",
             generation_mode: "img2img",
             cfg_scale: o,
             height: f,
             width: d,
             positive_prompt: "",
             negative_prompt: r,
@@ -45681,15 +45699,24 @@
             scheduler: s,
             vae: void 0,
             controlnets: [],
             loras: [],
             clip_skip: h,
             strength: u,
             init_image: l.imageName
-        }, Mh(e, y, Kt), Nh(e, y), xc(e, y), Kv(e, y, Kt), e.system.shouldUseNSFWChecker && xl(e, y), e.system.shouldUseWatermarker && Cl(e, y), y
+        }, y.edges.push({
+            source: {
+                node_id: Ke,
+                field: "metadata"
+            },
+            destination: {
+                node_id: We,
+                field: "metadata"
+            }
+        }), Mh(e, y, Kt), Nh(e, y), xc(e, y), Kv(e, y, Kt), e.system.shouldUseNSFWChecker && xl(e, y), e.system.shouldUseWatermarker && Cl(e, y), y
     }, VN = (e, t, n) => {
         const {
             positivePrompt: r,
             negativePrompt: i
         } = e.generation, {
             refinerModel: o,
             refinerAestheticScore: s,
@@ -45697,15 +45724,15 @@
             negativeStylePrompt: l,
             refinerSteps: u,
             refinerScheduler: c,
             refinerCFGScale: d,
             refinerStart: f
         } = e.sdxl;
         if (!o) return;
-        const h = t.nodes[lt];
+        const h = t.nodes[Ke];
         h && (h.refiner_model = o, h.refiner_aesthetic_store = s, h.refiner_cfg_scale = d, h.refiner_scheduler = c, h.refiner_start = f, h.refiner_steps = u), t.edges = t.edges.filter(p => !(p.source.node_id === n && ["latents"].includes(p.source.field))), t.edges = t.edges.filter(p => !(p.source.node_id === en && ["vae"].includes(p.source.field))), n === Ni && t.edges.push({
             source: {
                 node_id: en,
                 field: "vae"
             },
             destination: {
                 node_id: bt,
@@ -45744,15 +45771,15 @@
             }
         }, {
             source: {
                 node_id: zl,
                 field: "vae"
             },
             destination: {
-                node_id: nt,
+                node_id: We,
                 field: "vae"
             }
         }, {
             source: {
                 node_id: zl,
                 field: "clip2"
             },
@@ -45798,15 +45825,15 @@
             }
         }, {
             source: {
                 node_id: ma,
                 field: "latents"
             },
             destination: {
-                node_id: nt,
+                node_id: We,
                 field: "latents"
             }
         })
     }, Ppe = e => {
         const t = fe("nodes"),
             {
                 positivePrompt: n,
@@ -45856,17 +45883,17 @@
                         style: y ? `${r} ${v}` : v
                     },
                     [Le]: {
                         type: "noise",
                         id: Le,
                         use_cpu: w
                     },
-                    [nt]: {
+                    [We]: {
                         type: "l2i",
-                        id: nt,
+                        id: We,
                         fp32: m === "fp32"
                     },
                     [Ni]: {
                         type: "l2l_sdxl",
                         id: Ni,
                         cfg_scale: o,
                         scheduler: s,
@@ -45891,15 +45918,15 @@
                     }
                 }, {
                     source: {
                         node_id: en,
                         field: "vae"
                     },
                     destination: {
-                        node_id: nt,
+                        node_id: We,
                         field: "vae"
                     }
                 }, {
                     source: {
                         node_id: en,
                         field: "vae"
                     },
@@ -45945,15 +45972,15 @@
                     }
                 }, {
                     source: {
                         node_id: Ni,
                         field: "latents"
                     },
                     destination: {
-                        node_id: nt,
+                        node_id: We,
                         field: "latents"
                     }
                 }, {
                     source: {
                         node_id: bt,
                         field: "latents"
                     },
@@ -46046,16 +46073,16 @@
                 field: "height"
             },
             destination: {
                 node_id: Le,
                 field: "height"
             }
         });
-        return x.nodes[lt] = {
-            id: lt,
+        return x.nodes[Ke] = {
+            id: Ke,
             type: "metadata_accumulator",
             generation_mode: "sdxl_img2img",
             cfg_scale: o,
             height: d,
             width: c,
             positive_prompt: "",
             negative_prompt: r,
@@ -46070,19 +46097,19 @@
             clip_skip: f,
             strength: _,
             init_image: l.imageName,
             positive_style_prompt: S,
             negative_style_prompt: v
         }, x.edges.push({
             source: {
-                node_id: lt,
+                node_id: Ke,
                 field: "metadata"
             },
             destination: {
-                node_id: nt,
+                node_id: We,
                 field: "metadata"
             }
         }), g && VN(e, x, Ni), xc(e, x), e.system.shouldUseNSFWChecker && xl(e, x), e.system.shouldUseWatermarker && Cl(e, x), x
     }, Ape = () => {
         le({
             predicate: e => Ph.match(e) && e.payload === "img2img",
             effect: async (e, {
@@ -46275,17 +46302,17 @@
                     type: "t2l_sdxl",
                     id: is,
                     cfg_scale: o,
                     scheduler: s,
                     steps: a,
                     denoising_end: v ? y : 1
                 },
-                [nt]: {
+                [We]: {
                     type: "l2i",
-                    id: nt,
+                    id: We,
                     fp32: h === "fp32"
                 }
             },
             edges: [{
                 source: {
                     node_id: en,
                     field: "unet"
@@ -46296,15 +46323,15 @@
                 }
             }, {
                 source: {
                     node_id: en,
                     field: "vae"
                 },
                 destination: {
-                    node_id: nt,
+                    node_id: We,
                     field: "vae"
                 }
             }, {
                 source: {
                     node_id: en,
                     field: "clip"
                 },
@@ -46368,21 +46395,21 @@
                 }
             }, {
                 source: {
                     node_id: is,
                     field: "latents"
                 },
                 destination: {
-                    node_id: nt,
+                    node_id: We,
                     field: "latents"
                 }
             }]
         };
-        return b.nodes[lt] = {
-            id: lt,
+        return b.nodes[Ke] = {
+            id: Ke,
             type: "metadata_accumulator",
             generation_mode: "sdxl_txt2img",
             cfg_scale: o,
             height: u,
             width: l,
             positive_prompt: "",
             negative_prompt: r,
@@ -46395,19 +46422,19 @@
             controlnets: [],
             loras: [],
             clip_skip: c,
             positive_style_prompt: p,
             negative_style_prompt: m
         }, b.edges.push({
             source: {
-                node_id: lt,
+                node_id: Ke,
                 field: "metadata"
             },
             destination: {
-                node_id: nt,
+                node_id: We,
                 field: "metadata"
             }
         }), v && VN(e, b, is), xc(e, b), e.system.shouldUseNSFWChecker && xl(e, b), e.system.shouldUseWatermarker && Cl(e, b), b
     },
     Fpe = e => {
         const t = fe("nodes"),
             {
@@ -46430,17 +46457,17 @@
             id: bC,
             nodes: {
                 [pt]: {
                     type: "main_model_loader",
                     id: pt,
                     model: i
                 },
-                [rt]: {
+                [it]: {
                     type: "clip_skip",
-                    id: rt,
+                    id: it,
                     skipped_layers: c
                 },
                 [Fe]: {
                     type: "compel",
                     id: Fe,
                     prompt: n
                 },
@@ -46459,50 +46486,50 @@
                 [dn]: {
                     type: "t2l",
                     id: dn,
                     cfg_scale: o,
                     scheduler: s,
                     steps: a
                 },
-                [nt]: {
+                [We]: {
                     type: "l2i",
-                    id: nt,
+                    id: We,
                     fp32: h === "fp32"
                 }
             },
             edges: [{
                 source: {
                     node_id: pt,
                     field: "clip"
                 },
                 destination: {
-                    node_id: rt,
+                    node_id: it,
                     field: "clip"
                 }
             }, {
                 source: {
                     node_id: pt,
                     field: "unet"
                 },
                 destination: {
                     node_id: dn,
                     field: "unet"
                 }
             }, {
                 source: {
-                    node_id: rt,
+                    node_id: it,
                     field: "clip"
                 },
                 destination: {
                     node_id: Fe,
                     field: "clip"
                 }
             }, {
                 source: {
-                    node_id: rt,
+                    node_id: it,
                     field: "clip"
                 },
                 destination: {
                     node_id: qe,
                     field: "clip"
                 }
             }, {
@@ -46525,30 +46552,30 @@
                 }
             }, {
                 source: {
                     node_id: dn,
                     field: "latents"
                 },
                 destination: {
-                    node_id: nt,
+                    node_id: We,
                     field: "latents"
                 }
             }, {
                 source: {
                     node_id: Le,
                     field: "noise"
                 },
                 destination: {
                     node_id: dn,
                     field: "noise"
                 }
             }]
         };
-        return m.nodes[lt] = {
-            id: lt,
+        return m.nodes[Ke] = {
+            id: Ke,
             type: "metadata_accumulator",
             generation_mode: "txt2img",
             cfg_scale: o,
             height: u,
             width: l,
             positive_prompt: "",
             negative_prompt: r,
@@ -46557,15 +46584,24 @@
             steps: a,
             rand_device: p ? "cpu" : "cuda",
             scheduler: s,
             vae: void 0,
             controlnets: [],
             loras: [],
             clip_skip: c
-        }, Mh(e, m, dn), Nh(e, m), xc(e, m), Kv(e, m, dn), e.system.shouldUseNSFWChecker && xl(e, m), e.system.shouldUseWatermarker && Cl(e, m), m
+        }, m.edges.push({
+            source: {
+                node_id: Ke,
+                field: "metadata"
+            },
+            destination: {
+                node_id: We,
+                field: "metadata"
+            }
+        }), Mh(e, m, dn), Nh(e, m), xc(e, m), Kv(e, m, dn), e.system.shouldUseNSFWChecker && xl(e, m), e.system.shouldUseWatermarker && Cl(e, m), m
     },
     Bpe = () => {
         le({
             predicate: e => Ph.match(e) && e.payload === "txt2img",
             effect: async (e, {
                 getState: t,
                 dispatch: n,
@@ -47779,23 +47815,23 @@
             return F(V(q))
         }
     }
     var gt = Array.prototype,
         vn = Function.prototype,
         It = Object.prototype,
         ut = Y["__core-js_shared__"],
-        Je = vn.toString,
+        tt = vn.toString,
         Gt = It.hasOwnProperty,
         sr = function() {
             var F = /[^.]+$/.exec(ut && ut.keys && ut.keys.IE_PROTO || "");
             return F ? "Symbol(src)_1." + F : ""
         }(),
         ni = It.toString,
-        Lr = Je.call(Object),
-        Rn = RegExp("^" + Je.call(Gt).replace(L, "\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g, "$1.*?") + "$"),
+        Lr = tt.call(Object),
+        Rn = RegExp("^" + tt.call(Gt).replace(L, "\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g, "$1.*?") + "$"),
         bn = Q ? Y.Buffer : void 0,
         Un = Y.Symbol,
         Ht = Y.Uint8Array,
         xt = bn ? bn.allocUnsafe : void 0,
         wr = se(Object.getPrototypeOf, Object),
         $r = Object.create,
         ri = It.propertyIsEnumerable,
@@ -47807,15 +47843,15 @@
                 return F({}, "", {}), F
             } catch {}
         }(),
         ca = bn ? bn.isBuffer : void 0,
         ki = Math.max,
         da = Date.now,
         Ct = T1(Y, "Map"),
-        et = T1(Object, "create"),
+        nt = T1(Object, "create"),
         _n = function() {
             function F() {}
             return function(V) {
                 if (!ha(V)) return {};
                 if ($r) return $r(V);
                 F.prototype = V;
                 var q = new F;
@@ -47829,39 +47865,39 @@
         for (this.clear(); ++V < q;) {
             var ie = F[V];
             this.set(ie[0], ie[1])
         }
     }
 
     function In() {
-        this.__data__ = et ? et(null) : {}, this.size = 0
+        this.__data__ = nt ? nt(null) : {}, this.size = 0
     }
 
     function Gn(F) {
         var V = this.has(F) && delete this.__data__[F];
         return this.size -= V ? 1 : 0, V
     }
 
     function ar(F) {
         var V = this.__data__;
-        if (et) {
+        if (nt) {
             var q = V[F];
             return q === r ? void 0 : q
         }
         return Gt.call(V, F) ? V[F] : void 0
     }
 
     function Oi(F) {
         var V = this.__data__;
-        return et ? V[F] !== void 0 : Gt.call(V, F)
+        return nt ? V[F] !== void 0 : Gt.call(V, F)
     }
 
     function Hn(F, V) {
         var q = this.__data__;
-        return this.size += this.has(F) ? 0 : 1, q[F] = et && V === void 0 ? r : V, this
+        return this.size += this.has(F) ? 0 : 1, q[F] = nt && V === void 0 ? r : V, this
     }
     ln.prototype.clear = In, ln.prototype.delete = Gn, ln.prototype.get = ar, ln.prototype.has = Oi, ln.prototype.set = Hn;
 
     function wn(F) {
         var V = -1,
             q = F == null ? 0 : F.length;
         for (this.clear(); ++V < q;) {
@@ -47970,19 +48006,19 @@
     }
     es.prototype.clear = y1, es.prototype.delete = v1, es.prototype.get = Hh, es.prototype.has = b1, es.prototype.set = S1;
 
     function _1(F, V) {
         var q = A1(F),
             ie = !q && P1(F),
             De = !q && !ie && A3(F),
-            tt = !q && !ie && !De && O3(F),
-            mt = q || ie || De || tt,
+            rt = !q && !ie && !De && O3(F),
+            mt = q || ie || De || rt,
             Ie = mt ? ve(F.length, String) : [],
             yt = Ie.length;
-        for (var Fr in F)(V || Gt.call(F, Fr)) && !(mt && (Fr == "length" || De && (Fr == "offset" || Fr == "parent") || tt && (Fr == "buffer" || Fr == "byteLength" || Fr == "byteOffset") || E3(Fr, yt))) && Ie.push(Fr);
+        for (var Fr in F)(V || Gt.call(F, Fr)) && !(mt && (Fr == "length" || De && (Fr == "offset" || Fr == "parent") || rt && (Fr == "buffer" || Fr == "byteLength" || Fr == "byteOffset") || E3(Fr, yt))) && Ie.push(Fr);
         return Ie
     }
 
     function Ac(F, V, q) {
         (q !== void 0 && !Kh(F[V], q) || q === void 0 && !(V in F)) && Ol(F, V, q)
     }
 
@@ -48030,40 +48066,40 @@
         var V = P3(F),
             q = [];
         for (var ie in F) ie == "constructor" && (V || !Gt.call(F, ie)) || q.push(ie);
         return q
     }
 
     function T3(F, V, q, ie, De) {
-        F !== V && x1(V, function(tt, mt) {
-            if (De || (De = new es), ha(tt)) k$(F, V, mt, q, T3, ie, De);
+        F !== V && x1(V, function(rt, mt) {
+            if (De || (De = new es), ha(rt)) k$(F, V, mt, q, T3, ie, De);
             else {
-                var Ie = ie ? ie(E1(F, mt), tt, mt + "", F, V, De) : void 0;
-                Ie === void 0 && (Ie = tt), Ac(F, mt, Ie)
+                var Ie = ie ? ie(E1(F, mt), rt, mt + "", F, V, De) : void 0;
+                Ie === void 0 && (Ie = rt), Ac(F, mt, Ie)
             }
         }, R3)
     }
 
-    function k$(F, V, q, ie, De, tt, mt) {
+    function k$(F, V, q, ie, De, rt, mt) {
         var Ie = E1(F, q),
             yt = E1(V, q),
             Fr = mt.get(yt);
         if (Fr) {
             Ac(F, q, Fr);
             return
         }
-        var xr = tt ? tt(Ie, yt, q + "", F, V, mt) : void 0,
+        var xr = rt ? rt(Ie, yt, q + "", F, V, mt) : void 0,
             Rc = xr === void 0;
         if (Rc) {
             var R1 = A1(yt),
                 I1 = !R1 && A3(yt),
                 M3 = !R1 && !I1 && O3(yt);
             xr = yt, R1 || I1 || M3 ? A1(Ie) ? xr = Ie : Y$(Ie) ? xr = D$(Ie) : I1 ? (Rc = !1, xr = I$(yt, !0)) : M3 ? (Rc = !1, xr = N$(yt, !0)) : xr = [] : Q$(yt) || P1(yt) ? (xr = Ie, P1(Ie) ? xr = Z$(Ie) : (!ha(Ie) || O1(Ie)) && (xr = j$(yt))) : Rc = !1
         }
-        Rc && (mt.set(yt, xr), De(xr, yt, ie, tt, mt), mt.delete(yt)), Ac(F, q, xr)
+        Rc && (mt.set(yt, xr), De(xr, yt, ie, rt, mt), mt.delete(yt)), Ac(F, q, xr)
     }
 
     function O$(F, V) {
         return W$(q$(F, V, I3), F + "")
     }
     var R$ = ii ? function(F, V) {
         return ii(F, "toString", {
@@ -48097,41 +48133,41 @@
         for (V || (V = Array(ie)); ++q < ie;) V[q] = F[q];
         return V
     }
 
     function L$(F, V, q, ie) {
         var De = !q;
         q || (q = {});
-        for (var tt = -1, mt = V.length; ++tt < mt;) {
-            var Ie = V[tt],
+        for (var rt = -1, mt = V.length; ++rt < mt;) {
+            var Ie = V[rt],
                 yt = ie ? ie(q[Ie], F[Ie], Ie, q, F) : void 0;
             yt === void 0 && (yt = F[Ie]), De ? Ol(q, Ie, yt) : w1(q, Ie, yt)
         }
         return q
     }
 
     function $$(F) {
         return O$(function(V, q) {
             var ie = -1,
                 De = q.length,
-                tt = De > 1 ? q[De - 1] : void 0,
+                rt = De > 1 ? q[De - 1] : void 0,
                 mt = De > 2 ? q[2] : void 0;
-            for (tt = F.length > 3 && typeof tt == "function" ? (De--, tt) : void 0, mt && V$(q[0], q[1], mt) && (tt = De < 3 ? void 0 : tt, De = 1), V = Object(V); ++ie < De;) {
+            for (rt = F.length > 3 && typeof rt == "function" ? (De--, rt) : void 0, mt && V$(q[0], q[1], mt) && (rt = De < 3 ? void 0 : rt, De = 1), V = Object(V); ++ie < De;) {
                 var Ie = q[ie];
-                Ie && F(V, Ie, ie, tt)
+                Ie && F(V, Ie, ie, rt)
             }
             return V
         })
     }
 
     function F$(F) {
         return function(V, q, ie) {
-            for (var De = -1, tt = Object(V), mt = ie(V), Ie = mt.length; Ie--;) {
+            for (var De = -1, rt = Object(V), mt = ie(V), Ie = mt.length; Ie--;) {
                 var yt = mt[F ? Ie : ++De];
-                if (q(tt[yt], yt, tt) === !1) break
+                if (q(rt[yt], yt, rt) === !1) break
             }
             return V
         }
     }
 
     function Wh(F, V) {
         var q = F.__data__;
@@ -48194,15 +48230,15 @@
     function H$(F) {
         return ni.call(F)
     }
 
     function q$(F, V, q) {
         return V = ki(V === void 0 ? F.length - 1 : V, 0),
             function() {
-                for (var ie = arguments, De = -1, tt = ki(ie.length - V, 0), mt = Array(tt); ++De < tt;) mt[De] = ie[V + De];
+                for (var ie = arguments, De = -1, rt = ki(ie.length - V, 0), mt = Array(rt); ++De < rt;) mt[De] = ie[V + De];
                 De = -1;
                 for (var Ie = Array(V + 1); ++De < V;) Ie[De] = ie[De];
                 return Ie[V] = q(mt), xe(F, this, Ie)
             }
     }
 
     function E1(F, V) {
@@ -48222,15 +48258,15 @@
             return F.apply(void 0, arguments)
         }
     }
 
     function X$(F) {
         if (F != null) {
             try {
-                return Je.call(F)
+                return tt.call(F)
             } catch {}
             try {
                 return F + ""
             } catch {}
         }
         return ""
     }
@@ -48274,15 +48310,15 @@
     }
 
     function Q$(F) {
         if (!Oc(F) || Rl(F) != y) return !1;
         var V = wr(F);
         if (V === null) return !0;
         var q = Gt.call(V, "constructor") && V.constructor;
-        return typeof q == "function" && q instanceof q && Je.call(q) == Lr
+        return typeof q == "function" && q instanceof q && tt.call(q) == Lr
     }
     var O3 = te ? ce(te) : kc;
 
     function Z$(F) {
         return L$(F, R3(F))
     }
 
@@ -50078,20 +50114,20 @@
         ...r
     } = t ?? {};
     r.shouldForwardProp || (r.shouldForwardProp = bye);
     const i = kye({
             baseStyle: n
         }),
         o = Aye(e, r)(i);
-    return We.forwardRef(function(l, u) {
+    return Xe.forwardRef(function(l, u) {
         const {
             colorMode: c,
             forced: d
         } = BC();
-        return We.createElement(o, {
+        return Xe.createElement(o, {
             ref: u,
             "data-theme": d ? c : void 0,
             ...l
         })
     })
 }
 
@@ -53456,15 +53492,15 @@
                 a = n.x.scale * t.x,
                 l = n.y.scale * t.y;
             i[0 + s] /= a, i[1 + s] /= l;
             const u = At(a, l, .5);
             return typeof i[2 + s] == "number" && (i[2 + s] /= u), typeof i[3 + s] == "number" && (i[3 + s] /= u), o(i)
         }
     };
-class N1e extends We.Component {
+class N1e extends Xe.Component {
     componentDidMount() {
         const {
             visualElement: t,
             layoutGroup: n,
             switchLayoutGroup: r,
             layoutId: i
         } = this.props, {
@@ -53516,15 +53552,15 @@
     render() {
         return null
     }
 }
 
 function TL(e) {
     const [t, n] = R1e(), r = k.useContext(HC);
-    return We.createElement(N1e, {
+    return Xe.createElement(N1e, {
         ...e,
         layoutGroup: r,
         switchLayoutGroup: k.useContext(SD),
         isPresent: t,
         safeToRemove: n
     })
 }
@@ -57881,15 +57917,15 @@
 };
 
 function w_e(e) {
     let {
         id: t,
         value: n
     } = e;
-    return We.createElement("div", {
+    return Xe.createElement("div", {
         id: t,
         style: __e
     }, n)
 }
 const x_e = {
     position: "fixed",
     width: 1,
@@ -57904,15 +57940,15 @@
 };
 
 function C_e(e) {
     let {
         id: t,
         announcement: n
     } = e;
-    return We.createElement("div", {
+    return Xe.createElement("div", {
         id: t,
         style: x_e,
         role: "status",
         "aria-live": "assertive",
         "aria-atomic": !0
     }, n)
 }
@@ -58044,18 +58080,18 @@
                 } = d;
                 o(t.onDragCancel({
                     active: f,
                     over: h
                 }))
             }
         }), [o, t])), !l) return null;
-    const c = We.createElement(We.Fragment, null, We.createElement(w_e, {
+    const c = Xe.createElement(Xe.Fragment, null, Xe.createElement(w_e, {
         id: r,
         value: i.draggable
-    }), We.createElement(C_e, {
+    }), Xe.createElement(C_e, {
         id: a,
         announcement: s
     }));
     return n ? zi.createPortal(c, n) : c
 }
 var nn;
 (function(e) {
@@ -58584,41 +58620,41 @@
 function S8(e) {
     e.preventDefault()
 }
 
 function r2e(e) {
     e.stopPropagation()
 }
-var st;
+var at;
 (function(e) {
     e.Space = "Space", e.Down = "ArrowDown", e.Right = "ArrowRight", e.Left = "ArrowLeft", e.Up = "ArrowUp", e.Esc = "Escape", e.Enter = "Enter"
-})(st || (st = {}));
+})(at || (at = {}));
 const y$ = {
-        start: [st.Space, st.Enter],
-        cancel: [st.Esc],
-        end: [st.Space, st.Enter]
+        start: [at.Space, at.Enter],
+        cancel: [at.Esc],
+        end: [at.Space, at.Enter]
     },
     i2e = (e, t) => {
         let {
             currentCoordinates: n
         } = t;
         switch (e.code) {
-            case st.Right:
+            case at.Right:
                 return {
                     ...n, x: n.x + 25
                 };
-            case st.Left:
+            case at.Left:
                 return {
                     ...n, x: n.x - 25
                 };
-            case st.Down:
+            case at.Down:
                 return {
                     ...n, y: n.y + 25
                 };
-            case st.Up:
+            case at.Up:
                 return {
                     ...n, y: n.y - 25
                 }
         }
     };
 class v$ {
     constructor(t) {
@@ -58690,45 +58726,45 @@
                             isLeft: g,
                             isBottom: b,
                             maxScroll: _,
                             minScroll: w
                         } = p$(m),
                         x = Z_e(m),
                         T = {
-                            x: Math.min(S === st.Right ? x.right - x.width / 2 : x.right, Math.max(S === st.Right ? x.left : x.left + x.width / 2, d.x)),
-                            y: Math.min(S === st.Down ? x.bottom - x.height / 2 : x.bottom, Math.max(S === st.Down ? x.top : x.top + x.height / 2, d.y))
+                            x: Math.min(S === at.Right ? x.right - x.width / 2 : x.right, Math.max(S === at.Right ? x.left : x.left + x.width / 2, d.x)),
+                            y: Math.min(S === at.Down ? x.bottom - x.height / 2 : x.bottom, Math.max(S === at.Down ? x.top : x.top + x.height / 2, d.y))
                         },
-                        P = S === st.Right && !y || S === st.Left && !g,
-                        E = S === st.Down && !b || S === st.Up && !v;
+                        P = S === at.Right && !y || S === at.Left && !g,
+                        E = S === at.Down && !b || S === at.Up && !v;
                     if (P && T.x !== d.x) {
                         const A = m.scrollLeft + f.x,
-                            $ = S === st.Right && A <= _.x || S === st.Left && A >= w.x;
+                            $ = S === at.Right && A <= _.x || S === at.Left && A >= w.x;
                         if ($ && !f.y) {
                             m.scrollTo({
                                 left: A,
                                 behavior: a
                             });
                             return
                         }
-                        $ ? h.x = m.scrollLeft - A : h.x = S === st.Right ? m.scrollLeft - _.x : m.scrollLeft - w.x, h.x && m.scrollBy({
+                        $ ? h.x = m.scrollLeft - A : h.x = S === at.Right ? m.scrollLeft - _.x : m.scrollLeft - w.x, h.x && m.scrollBy({
                             left: -h.x,
                             behavior: a
                         });
                         break
                     } else if (E && T.y !== d.y) {
                         const A = m.scrollTop + f.y,
-                            $ = S === st.Down && A <= _.y || S === st.Up && A >= w.y;
+                            $ = S === at.Down && A <= _.y || S === at.Up && A >= w.y;
                         if ($ && !f.x) {
                             m.scrollTo({
                                 top: A,
                                 behavior: a
                             });
                             return
                         }
-                        $ ? h.y = m.scrollTop - A : h.y = S === st.Down ? m.scrollTop - _.y : m.scrollTop - w.y, h.y && m.scrollBy({
+                        $ ? h.y = m.scrollTop - A : h.y = S === at.Down ? m.scrollTop - _.y : m.scrollTop - w.y, h.y && m.scrollBy({
                             top: -h.y,
                             behavior: a
                         });
                         break
                     }
                 }
                 this.handleMove(t, Fu(yy(d, this.referenceCoordinates), h))
@@ -58860,15 +58896,15 @@
     handleCancel() {
         const {
             onCancel: t
         } = this.props;
         this.detach(), t()
     }
     handleKeydown(t) {
-        t.code === st.Esc && this.handleCancel()
+        t.code === at.Esc && this.handleCancel()
     }
     removeTextSelection() {
         var t;
         (t = this.document.getSelection()) == null || t.removeAllRanges()
     }
 }
 const o2e = {
@@ -59798,15 +59834,15 @@
             ce = (r = ve.nodeRef.current) != null ? r : Y,
             Ne = w ? (i = ve.rect) != null ? i : J : null,
             se = !!(ve.nodeRef.current && ve.rect),
             gt = m2e(se ? null : J),
             vn = w$(ce ? or(ce) : null),
             It = y2e(w ? xe ?? Y : null),
             ut = _2e(It),
-            Je = E$(h, {
+            tt = E$(h, {
                 transform: {
                     x: P.x - gt.x,
                     y: P.y - gt.y,
                     scaleX: 1,
                     scaleY: 1
                 },
                 activatorEvent: N,
@@ -59820,32 +59856,32 @@
                 scrollableAncestorRects: ut,
                 windowRect: vn
             }),
             Gt = B ? Fu(B, P) : null,
             sr = v2e(It),
             ni = E8(sr),
             Lr = E8(sr, [J]),
-            Rn = Fu(Je, ni),
-            bn = Ne ? G_e(Ne, Je) : null,
+            Rn = Fu(tt, ni),
+            bn = Ne ? G_e(Ne, tt) : null,
             Un = M && bn ? d({
                 active: M,
                 collisionRect: bn,
                 droppableRects: G,
                 droppableContainers: j,
                 pointerCoordinates: Gt
             }) : null,
             Ht = $_e(Un, "id"),
             [xt, wr] = k.useState(null),
-            $r = se ? Je : Fu(Je, Lr),
+            $r = se ? tt : Fu(tt, Lr),
             ri = z_e($r, (o = xt == null ? void 0 : xt.rect) != null ? o : null, J),
-            uo = k.useCallback((Ct, et) => {
+            uo = k.useCallback((Ct, nt) => {
                 let {
                     sensor: _n,
                     options: ln
-                } = et;
+                } = nt;
                 if (C.current == null) return;
                 const In = T.get(C.current);
                 if (!In) return;
                 const Gn = Ct.nativeEvent,
                     ar = new _n({
                         active: C.current,
                         activeNode: In,
@@ -59923,40 +59959,40 @@
                                     event: fo
                                 })
                             }
                         })
                     }
                 }
             }, [T]),
-            Sn = k.useCallback((Ct, et) => (_n, ln) => {
+            Sn = k.useCallback((Ct, nt) => (_n, ln) => {
                 const In = _n.nativeEvent,
                     Gn = T.get(ln);
                 if (C.current !== null || !Gn || In.dndKit || In.defaultPrevented) return;
                 const ar = {
                     active: Gn
                 };
-                Ct(_n, et.options, ar) === !0 && (In.dndKit = {
-                    capturedBy: et.sensor
-                }, C.current = ln, uo(_n, et))
+                Ct(_n, nt.options, ar) === !0 && (In.dndKit = {
+                    capturedBy: nt.sensor
+                }, C.current = ln, uo(_n, nt))
             }, [T, uo]),
             ii = d2e(c, Sn);
         b2e(c), io(() => {
             J && b === hs.Initializing && _(hs.Initialized)
         }, [J, b]), k.useEffect(() => {
             const {
                 onDragMove: Ct
             } = D.current, {
-                active: et,
+                active: nt,
                 activatorEvent: _n,
                 collisions: ln,
                 over: In
             } = te.current;
-            if (!et || !_n) return;
+            if (!nt || !_n) return;
             const Gn = {
-                active: et,
+                active: nt,
                 activatorEvent: _n,
                 collisions: ln,
                 delta: {
                     x: Rn.x,
                     y: Rn.y
                 },
                 over: In
@@ -59966,30 +60002,30 @@
                     type: "onDragMove",
                     event: Gn
                 })
             })
         }, [Rn.x, Rn.y]), k.useEffect(() => {
             const {
                 active: Ct,
-                activatorEvent: et,
+                activatorEvent: nt,
                 collisions: _n,
                 droppableContainers: ln,
                 scrollAdjustedTranslate: In
             } = te.current;
-            if (!Ct || C.current == null || !et || !In) return;
+            if (!Ct || C.current == null || !nt || !In) return;
             const {
                 onDragOver: Gn
             } = D.current, ar = ln.get(Ht), Oi = ar && ar.rect.current ? {
                 id: ar.id,
                 rect: ar.rect.current,
                 data: ar.data,
                 disabled: ar.disabled
             } : null, Hn = {
                 active: Ct,
-                activatorEvent: et,
+                activatorEvent: nt,
                 collisions: _n,
                 delta: {
                     x: In.x,
                     y: In.y
                 },
                 over: Oi
             };
@@ -60054,33 +60090,33 @@
                     draggable: L
                 },
                 dispatch: v,
                 draggableNodes: T,
                 over: xt,
                 measureDroppableContainers: W
             }), [N, ii, M, J, v, L, T, xt, W]);
-        return We.createElement(a$.Provider, {
+        return Xe.createElement(a$.Provider, {
             value: g
-        }, We.createElement(Gh.Provider, {
+        }, Xe.createElement(Gh.Provider, {
             value: ki
-        }, We.createElement(T$.Provider, {
+        }, Xe.createElement(T$.Provider, {
             value: ca
-        }, We.createElement(f1.Provider, {
+        }, Xe.createElement(f1.Provider, {
             value: ri
-        }, u)), We.createElement(A2e, {
+        }, u)), Xe.createElement(A2e, {
             disabled: (a == null ? void 0 : a.restoreFocus) === !1
-        })), We.createElement(O_e, {
+        })), Xe.createElement(O_e, {
             ...a,
             hiddenTextDescribedById: L
         }));
 
         function da() {
             const Ct = (O == null ? void 0 : O.autoScrollEnabled) === !1,
-                et = typeof l == "object" ? l.enabled === !1 : l === !1,
-                _n = w && !Ct && !et;
+                nt = typeof l == "object" ? l.enabled === !1 : l === !1,
+                _n = w && !Ct && !nt;
             return typeof l == "object" ? {
                 ...l,
                 enabled: _n
             } : {
                 enabled: _n
             }
         }
@@ -60251,32 +60287,32 @@
         if (l == null || u == null) {
             i(null);
             return
         }
         Promise.resolve(t(u, o)).then(() => {
             i(null)
         })
-    }, [t, r, o]), We.createElement(We.Fragment, null, n, r ? k.cloneElement(r, {
+    }, [t, r, o]), Xe.createElement(Xe.Fragment, null, n, r ? k.cloneElement(r, {
         ref: s
     }) : null)
 }
 const j2e = {
     x: 0,
     y: 0,
     scaleX: 1,
     scaleY: 1
 };
 
 function V2e(e) {
     let {
         children: t
     } = e;
-    return We.createElement(Gh.Provider, {
+    return Xe.createElement(Gh.Provider, {
         value: C$
-    }, We.createElement(f1.Provider, {
+    }, Xe.createElement(f1.Provider, {
         value: j2e
     }, t))
 }
 const z2e = {
         position: "fixed",
         touchAction: "none"
     },
@@ -60306,15 +60342,15 @@
                 top: a.top,
                 left: a.left,
                 transform: Qf.Transform.toString(d),
                 transformOrigin: i && r ? M_e(r, a) : void 0,
                 transition: typeof c == "function" ? c(r) : c,
                 ...l
             };
-        return We.createElement(n, {
+        return Xe.createElement(n, {
             className: s,
             style: f,
             ref: t
         }, o)
     }),
     H2e = e => t => {
         let {
@@ -60464,15 +60500,15 @@
 let A8 = 0;
 
 function Y2e(e) {
     return k.useMemo(() => {
         if (e != null) return A8++, A8
     }, [e])
 }
-const Q2e = We.memo(e => {
+const Q2e = Xe.memo(e => {
         let {
             adjustScale: t = !1,
             children: n,
             dropAnimation: r,
             style: i,
             transition: o,
             modifiers: s,
@@ -60507,17 +60543,17 @@
             windowRect: _
         }), P = C3(f), E = K2e({
             config: r,
             draggableNodes: p,
             droppableContainers: m,
             measuringConfiguration: y
         }), A = P ? S.setRef : void 0;
-        return We.createElement(V2e, null, We.createElement(B2e, {
+        return Xe.createElement(V2e, null, Xe.createElement(B2e, {
             animation: E
-        }, d && x ? We.createElement(G2e, {
+        }, d && x ? Xe.createElement(G2e, {
             key: x,
             id: d.id,
             ref: A,
             as: a,
             activatorEvent: c,
             adjustScale: t,
             className: l,
@@ -60749,28 +60785,28 @@
                 onClose: a,
                 onClickAddToBoard: l,
                 handleAddToBoard: u
             },
             children: e.children
         })
     },
-    swe = k.lazy(() => G9(() => import("./App-69e5ea36.js"), ["./App-69e5ea36.js", "./MantineProvider-8184f020.js", "./App-6125620a.css"], import.meta.url)),
-    awe = k.lazy(() => G9(() => import("./ThemeLocaleProvider-9ac72450.js"), ["./ThemeLocaleProvider-9ac72450.js", "./MantineProvider-8184f020.js", "./ThemeLocaleProvider-5b992bc7.css"], import.meta.url)),
+    swe = k.lazy(() => G9(() => import("./App-58b095d3.js"), ["./App-58b095d3.js", "./MantineProvider-ea42d3d1.js", "./App-6125620a.css"], import.meta.url)),
+    awe = k.lazy(() => G9(() => import("./ThemeLocaleProvider-13e3db3d.js"), ["./ThemeLocaleProvider-13e3db3d.js", "./MantineProvider-ea42d3d1.js", "./ThemeLocaleProvider-5b992bc7.css"], import.meta.url)),
     lwe = ({
         apiUrl: e,
         token: t,
         config: n,
         headerComponent: r,
         middleware: i
     }) => (k.useEffect(() => (t && Cf.set(t), e && Tf.set(e), zM(), i && i.length > 0 ? c2(m8(), ...i) : c2(m8()), () => {
         Tf.set(void 0), Cf.set(void 0)
-    }), [e, t, i]), K.jsx(We.StrictMode, {
+    }), [e, t, i]), K.jsx(Xe.StrictMode, {
         children: K.jsx(mV, {
             store: Gpe,
-            children: K.jsx(We.Suspense, {
+            children: K.jsx(Xe.Suspense, {
                 fallback: K.jsx(rSe, {}),
                 children: K.jsx(awe, {
                     children: K.jsx(rwe, {
                         children: K.jsx(owe, {
                             children: K.jsx(swe, {
                                 config: n,
                                 headerComponent: r
@@ -60780,9 +60816,9 @@
                 })
             })
         })
     })),
     uwe = k.memo(lwe);
 xS.createRoot(document.getElementById("root")).render(K.jsx(uwe, {}));
 export {
-    fD as $, K as A, ti as B, Vt as C, Mu as D, Vn as E, fi as F, Xne as G, br as H, Is as I, $i as J, $3e as K, _I as L, cre as M, jre as N, ote as O, Zne as P, rc as Q, Tl as R, df as S, Nge as T, sl as U, oD as V, h4e as W, c4e as X, Hbe as Y, $be as Z, x4e as _, Zk as a, U_ as a$, $L as a0, mD as a1, d4e as a2, f4e as a3, Mge as a4, Ige as a5, p4e as a6, Na as a7, al as a8, rm as a9, d3 as aA, K2 as aB, o4e as aC, jN as aD, ZCe as aE, Y4 as aF, lCe as aG, zi as aH, BC as aI, nF as aJ, hwe as aK, dwe as aL, fwe as aM, Ee as aN, Ea as aO, C3e as aP, YM as aQ, w3e as aR, x3e as aS, S3e as aT, m3e as aU, Mpe as aV, k4e as aW, R4e as aX, b3e as aY, R3e as aZ, WCe as a_, IV as aa, We as ab, e8 as ac, uye as ad, w4e as ae, Io as af, rD as ag, R1e as ah, y4e as ai, R2 as aj, u4e as ak, _4e as al, Jn as am, mC as an, S0 as ao, A4e as ap, B3e as aq, Th as ar, nM as as, AM as at, Z0 as au, P$ as av, g5e as aw, Ft as ax, Ga as ay, c3 as az, Tx as b, I7 as b$, _3e as b0, u3 as b1, tSe as b2, vxe as b3, Ss as b4, A5e as b5, hp as b6, bge as b7, FC as b8, ZN as b9, ue as bA, iwe as bB, KCe as bC, dJ as bD, e3e as bE, $R as bF, Yxe as bG, RR as bH, G_ as bI, O4e as bJ, l4e as bK, O3e as bL, YCe as bM, QCe as bN, Rs as bO, Vle as bP, HCe as bQ, qCe as bR, UCe as bS, Jbe as bT, Am as bU, i4e as bV, N5e as bW, mxe as bX, ble as bY, d5e as bZ, I3e as b_, _ge as ba, $3 as bb, A3e as bc, P3e as bd, cwe as be, mwe as bf, ywe as bg, Bwe as bh, jwe as bi, wwe as bj, qwe as bk, bwe as bl, Iwe as bm, Twe as bn, Jle as bo, Swe as bp, Vwe as bq, vwe as br, Qwe as bs, xwe as bt, Mwe as bu, Cwe as bv, Nwe as bw, Awe as bx, Lwe as by, Zle as bz, AU as c, uxe as c$, eN as c0, O5e as c1, GCe as c2, t3e as c3, T7 as c4, LR as c5, _we as c6, xxe as c7, zn as c8, _5e as c9, Ph as cA, lxe as cB, Zwe as cC, ixe as cD, Fwe as cE, m5e as cF, hl as cG, v5e as cH, r5e as cI, n5e as cJ, t5e as cK, L5e as cL, e4e as cM, l5e as cN, s5e as cO, fe as cP, xf as cQ, W5e as cR, wE as cS, a5e as cT, u5e as cU, o5e as cV, Xse as cW, $we as cX, pQ as cY, j5e as cZ, PR as c_, R5e as ca, k5e as cb, Qae as cc, S5e as cd, C5e as ce, T5e as cf, vQ as cg, vae as ch, bae as ci, pxe as cj, wxe as ck, x5e as cl, _Q as cm, F3e as cn, p5e as co, ER as cp, g3e as cq, f3e as cr, h3e as cs, p3e as ct, B5e as cu, Qa as cv, pwe as cw, xo as cx, ace as cy, kwe as cz, mX as d, pe as d$, bQ as d0, Ui as d1, Dwe as d2, Kwe as d3, gwe as d4, aY as d5, Ywe as d6, i5e as d7, cxe as d8, e5e as d9, d3e as dA, C7 as dB, i3e as dC, s3e as dD, n3e as dE, Spe as dF, r3e as dG, oxe as dH, sxe as dI, nxe as dJ, rxe as dK, txe as dL, X5e as dM, G5e as dN, P5e as dO, U5e as dP, J5e as dQ, K5e as dR, E5e as dS, q5e as dT, H5e as dU, V5e as dV, z5e as dW, GI as dX, Lx as dY, Y5e as dZ, Fm as d_, hce as da, fxe as db, he as dc, Kn as dd, D5e as de, N3e as df, D3e as dg, N7 as dh, F5e as di, M3e as dj, hR as dk, axe as dl, sY as dm, Pwe as dn, $5e as dp, bT as dq, a3e as dr, Hx as ds, tJ as dt, Xl as du, ST as dv, l3e as dw, u3e as dx, c3e as dy, nJ as dz, ZO as e, bpe as e$, Q5e as e0, u2 as e1, Ewe as e2, Z5e as e3, J9 as e4, ak as e5, J3e as e6, q3e as e7, W3e as e8, K3e as e9, cCe as eA, nd as eB, sCe as eC, Vxe as eD, tv as eE, uCe as eF, $xe as eG, aCe as eH, Fxe as eI, Gxe as eJ, Pxe as eK, Exe as eL, Txe as eM, DCe as eN, MR as eO, Cf as eP, Ixe as eQ, Mxe as eR, Nxe as eS, ICe as eT, qxe as eU, Hxe as eV, FQ as eW, RCe as eX, Ele as eY, zxe as eZ, mpe as e_, RM as ea, Q3e as eb, Z3e as ec, Df as ed, _0 as ee, z3e as ef, Dpe as eg, j3e as eh, V3e as ei, G3e as ej, U3e as ek, H3e as el, Y3e as em, $ie as en, eM as eo, oN as ep, CF as eq, Pe as er, gCe as es, NCe as et, MCe as eu, Kxe as ev, CCe as ew, LCe as ex, Ple as ey, jxe as ez, sO as f, aye as f$, Zxe as f0, Jxe as f1, mCe as f2, pCe as f3, dCe as f4, Axe as f5, kxe as f6, I5e as f7, M5e as f8, oCe as f9, yQ as fA, Jwe as fB, Rxe as fC, tCe as fD, jCe as fE, zwe as fF, Uwe as fG, Gwe as fH, Hwe as fI, nCe as fJ, r4e as fK, yxe as fL, AR as fM, Sxe as fN, bxe as fO, _xe as fP, iY as fQ, yle as fR, a4e as fS, iD as fT, S4e as fU, v4e as fV, g4e as fW, b4e as fX, Wi as fY, m4e as fZ, s4e as f_, Qxe as fa, yCe as fb, xCe as fc, vCe as fd, Wxe as fe, Bxe as ff, OCe as fg, kCe as fh, _Ce as fi, bCe as fj, SCe as fk, VCe as fl, PCe as fm, zCe as fn, iCe as fo, rCe as fp, Lxe as fq, Dxe as fr, BCe as fs, Xxe as ft, Tle as fu, wle as fv, xle as fw, Cle as fx, Oxe as fy, dxe as fz, uX as g, Yme as g0, C4e as g1, E4e as g2, P4e as g3, gO as h, vr as i, mc as j, v0 as k, mn as l, y0 as m, hh as n, Ci as o, ta as p, g0 as q, oo as r, gh as s, hb as t, _x as u, iO as v, kx as w, HO as x, aO as y, k as z
+    fD as $, K as A, ti as B, Vt as C, Mu as D, Vn as E, fi as F, Xne as G, br as H, Is as I, $i as J, $3e as K, _I as L, cre as M, jre as N, ote as O, Zne as P, rc as Q, Tl as R, df as S, Nge as T, sl as U, oD as V, h4e as W, c4e as X, Hbe as Y, $be as Z, x4e as _, Zk as a, U_ as a$, $L as a0, mD as a1, d4e as a2, f4e as a3, Mge as a4, Ige as a5, p4e as a6, Na as a7, al as a8, rm as a9, d3 as aA, K2 as aB, o4e as aC, jN as aD, ZCe as aE, Y4 as aF, lCe as aG, zi as aH, BC as aI, nF as aJ, hwe as aK, dwe as aL, fwe as aM, Ee as aN, Ea as aO, C3e as aP, YM as aQ, w3e as aR, x3e as aS, S3e as aT, m3e as aU, Mpe as aV, k4e as aW, R4e as aX, b3e as aY, R3e as aZ, WCe as a_, IV as aa, Xe as ab, e8 as ac, uye as ad, w4e as ae, Io as af, rD as ag, R1e as ah, y4e as ai, R2 as aj, u4e as ak, _4e as al, Jn as am, mC as an, S0 as ao, A4e as ap, B3e as aq, Th as ar, nM as as, AM as at, Z0 as au, P$ as av, g5e as aw, Ft as ax, Ga as ay, c3 as az, Tx as b, I7 as b$, _3e as b0, u3 as b1, tSe as b2, vxe as b3, Ss as b4, A5e as b5, hp as b6, bge as b7, FC as b8, ZN as b9, ue as bA, iwe as bB, KCe as bC, dJ as bD, e3e as bE, $R as bF, Yxe as bG, RR as bH, G_ as bI, O4e as bJ, l4e as bK, O3e as bL, YCe as bM, QCe as bN, Rs as bO, Vle as bP, HCe as bQ, qCe as bR, UCe as bS, Jbe as bT, Am as bU, i4e as bV, N5e as bW, mxe as bX, ble as bY, d5e as bZ, I3e as b_, _ge as ba, $3 as bb, A3e as bc, P3e as bd, cwe as be, mwe as bf, ywe as bg, Bwe as bh, jwe as bi, wwe as bj, qwe as bk, bwe as bl, Iwe as bm, Twe as bn, Jle as bo, Swe as bp, Vwe as bq, vwe as br, Qwe as bs, xwe as bt, Mwe as bu, Cwe as bv, Nwe as bw, Awe as bx, Lwe as by, Zle as bz, AU as c, uxe as c$, eN as c0, O5e as c1, GCe as c2, t3e as c3, T7 as c4, LR as c5, _we as c6, xxe as c7, zn as c8, _5e as c9, Ph as cA, lxe as cB, Zwe as cC, ixe as cD, Fwe as cE, m5e as cF, hl as cG, v5e as cH, r5e as cI, n5e as cJ, t5e as cK, L5e as cL, e4e as cM, l5e as cN, s5e as cO, fe as cP, xf as cQ, W5e as cR, wE as cS, a5e as cT, u5e as cU, o5e as cV, Xse as cW, $we as cX, pQ as cY, j5e as cZ, PR as c_, R5e as ca, k5e as cb, Qae as cc, S5e as cd, C5e as ce, T5e as cf, vQ as cg, vae as ch, bae as ci, pxe as cj, wxe as ck, x5e as cl, _Q as cm, F3e as cn, p5e as co, ER as cp, g3e as cq, f3e as cr, h3e as cs, p3e as ct, B5e as cu, Qa as cv, pwe as cw, xo as cx, ace as cy, kwe as cz, mX as d, pe as d$, bQ as d0, Ui as d1, Dwe as d2, Kwe as d3, gwe as d4, aY as d5, Ywe as d6, i5e as d7, cxe as d8, e5e as d9, d3e as dA, C7 as dB, i3e as dC, s3e as dD, n3e as dE, Spe as dF, r3e as dG, oxe as dH, sxe as dI, nxe as dJ, rxe as dK, txe as dL, X5e as dM, G5e as dN, P5e as dO, U5e as dP, J5e as dQ, K5e as dR, E5e as dS, q5e as dT, H5e as dU, V5e as dV, z5e as dW, GI as dX, Lx as dY, Y5e as dZ, Fm as d_, hce as da, fxe as db, he as dc, Kn as dd, D5e as de, N3e as df, D3e as dg, N7 as dh, F5e as di, M3e as dj, hR as dk, axe as dl, sY as dm, Pwe as dn, $5e as dp, bT as dq, a3e as dr, Hx as ds, tJ as dt, Xl as du, ST as dv, l3e as dw, u3e as dx, c3e as dy, nJ as dz, ZO as e, bpe as e$, Q5e as e0, u2 as e1, Ewe as e2, Z5e as e3, J9 as e4, ak as e5, J3e as e6, q3e as e7, W3e as e8, K3e as e9, cCe as eA, nd as eB, sCe as eC, Vxe as eD, tv as eE, uCe as eF, $xe as eG, aCe as eH, Fxe as eI, Gxe as eJ, Pxe as eK, Exe as eL, Txe as eM, DCe as eN, MR as eO, Cf as eP, Ixe as eQ, Mxe as eR, Nxe as eS, ICe as eT, qxe as eU, Hxe as eV, FQ as eW, RCe as eX, Ele as eY, zxe as eZ, mpe as e_, RM as ea, Q3e as eb, Z3e as ec, Df as ed, _0 as ee, z3e as ef, Dpe as eg, j3e as eh, V3e as ei, G3e as ej, U3e as ek, H3e as el, Y3e as em, $ie as en, eM as eo, oN as ep, CF as eq, Pe as er, gCe as es, NCe as et, MCe as eu, Kxe as ev, CCe as ew, LCe as ex, Ple as ey, jxe as ez, sO as f, aye as f$, Zxe as f0, Jxe as f1, mCe as f2, pCe as f3, dCe as f4, Axe as f5, kxe as f6, I5e as f7, M5e as f8, oCe as f9, yQ as fA, Jwe as fB, Rxe as fC, tCe as fD, jCe as fE, zwe as fF, Uwe as fG, Gwe as fH, Hwe as fI, nCe as fJ, r4e as fK, yxe as fL, AR as fM, Sxe as fN, bxe as fO, _xe as fP, iY as fQ, yle as fR, a4e as fS, iD as fT, S4e as fU, v4e as fV, g4e as fW, b4e as fX, Wi as fY, m4e as fZ, s4e as f_, Qxe as fa, yCe as fb, xCe as fc, vCe as fd, Wxe as fe, Bxe as ff, OCe as fg, kCe as fh, _Ce as fi, bCe as fj, SCe as fk, VCe as fl, PCe as fm, zCe as fn, iCe as fo, rCe as fp, Lxe as fq, Dxe as fr, BCe as fs, Xxe as ft, Tle as fu, wle as fv, xle as fw, Cle as fx, Oxe as fy, dxe as fz, uX as g, Yme as g0, C4e as g1, E4e as g2, P4e as g3, gO as h, vr as i, mc as j, v0 as k, mn as l, y0 as m, hh as n, Ci as o, ta as p, g0 as q, oo as r, gh as s, hb as t, _x as u, iO as v, kx as w, HO as x, aO as y, k as z
 };
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-cyrillic-ext-wght-normal-848492d3.woff2` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-cyrillic-ext-wght-normal-848492d3.woff2`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-cyrillic-wght-normal-262a1054.woff2` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-cyrillic-wght-normal-262a1054.woff2`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-greek-ext-wght-normal-fe977ddb.woff2` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-greek-ext-wght-normal-fe977ddb.woff2`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-greek-wght-normal-89b4a3fe.woff2` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-greek-wght-normal-89b4a3fe.woff2`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-latin-ext-wght-normal-45606f83.woff2` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-latin-ext-wght-normal-45606f83.woff2`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-latin-wght-normal-450f3ba4.woff2` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-latin-wght-normal-450f3ba4.woff2`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/inter-vietnamese-wght-normal-ac4e131c.woff2` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/inter-vietnamese-wght-normal-ac4e131c.woff2`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/assets/logo-13003d72.png` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/assets/logo-13003d72.png`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/index.html` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/index.html`

 * *Files 24% similar despite different names*

```diff
@@ -8,15 +8,15 @@
     <style>
       html,
       body {
         padding: 0;
         margin: 0;
       }
     </style>
-    <script type="module" crossorigin src="./assets/index-89941396.js"></script>
+    <script type="module" crossorigin src="./assets/index-5a784cdd.js"></script>
   </head>
 
   <body dir="ltr">
     <div id="root"></div>
     
   </body>
 </html>
```

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ar.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ar.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/de.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/de.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/en.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/en.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/es.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/es.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/fi.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/fi.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/fr.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/fr.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/he.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/he.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/it.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/it.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ja.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ja.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ko.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ko.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/nl.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/nl.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/pl.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/pl.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/pt.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/pt.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/pt_BR.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/pt_BR.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/ru.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/ru.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/sv.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/sv.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/tr.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/tr.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/uk.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/uk.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/zh_CN.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/zh_CN.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/dist/locales/zh_Hant.json` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/dist/locales/zh_Hant.json`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/favicon.ico` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/favicon.ico`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/index.css` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/index.css`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/index.html` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/index.html`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/index.js` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/index.js`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/dream_web/test.html` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/dream_web/test.html`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/favicon.ico` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/favicon.ico`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/index.css` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/index.css`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/index.html` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/index.html`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/invokeai/frontend/web/static/legacy_web/index.js` & `InvokeAI-3.0.1rc2/invokeai/frontend/web/static/legacy_web/index.js`

 * *Files identical despite different names*

### Comparing `InvokeAI-3.0.1rc1/pyproject.toml` & `InvokeAI-3.0.1rc2/pyproject.toml`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 [build-system]
 requires = ["setuptools~=65.5", "pip~=22.3", "wheel"]
 build-backend = "setuptools.build_meta"
 
 [project]
 name = "InvokeAI"
 description = "An implementation of Stable Diffusion which provides various new features and options to aid the image generation process"
-requires-python = ">=3.9, <3.11"
+requires-python = ">=3.9, <3.12"
 readme = { content-type = "text/markdown", file = "README.md" }
 keywords = ["stable-diffusion", "AI"]
 dynamic = ["version"]
 license = { file = "LICENSE" }
 authors = [{ name = "The InvokeAI Project", email = "lincoln.stein@gmail.com" }]
 classifiers = [
   'Development Status :: 4 - Beta',
@@ -28,81 +28,81 @@
   'Topic :: Internet :: WWW/HTTP :: WSGI :: Application',
   'Topic :: Internet :: WWW/HTTP :: WSGI :: Server',
   'Topic :: Multimedia :: Graphics',
   'Topic :: Scientific/Engineering :: Artificial Intelligence',
   'Topic :: Scientific/Engineering :: Image Processing',
 ]
 dependencies = [
-  "accelerate~=0.16",
+  "accelerate~=0.21.0",
   "albumentations",
   "click",
-  "clip_anytorch",          # replacing "clip @ https://github.com/openai/CLIP/archive/eaa22acb90a5876642d0507623e859909230a52d.zip",
-  "compel==2.0.0",
+  "clip_anytorch",  # replacing "clip @ https://github.com/openai/CLIP/archive/eaa22acb90a5876642d0507623e859909230a52d.zip",
+  "compel~=2.0.0",
   "controlnet-aux>=0.0.6",
-  "timm==0.6.13",           # needed to override timm latest in controlnet_aux, see  https://github.com/isl-org/ZoeDepth/issues/26
+  "timm==0.6.13",   # needed to override timm latest in controlnet_aux, see  https://github.com/isl-org/ZoeDepth/issues/26
   "datasets",
-  "diffusers[torch]~=0.18.1",
-  "dnspython==2.2.1",
+  "diffusers[torch]~=0.19.0",
+  "dnspython~=2.4.0",
   "dynamicprompts",
   "easing-functions",
   "einops",
   "eventlet",
   "facexlib",
   "fastapi==0.88.0",
   "fastapi-events==0.8.0",
   "fastapi-socketio==0.0.10",
   "flask==2.1.3",
   "flask_cors==3.0.10",
   "flask_socketio==5.3.0",
   "flaskwebgui==1.0.3",
-  "gfpgan==1.3.8",
   "huggingface-hub>=0.11.1",
-  "invisible-watermark>=0.2.0", # needed to install SDXL base and refiner using their repo_ids
+  "invisible-watermark~=0.2.0", # needed to install SDXL base and refiner using their repo_ids
   "matplotlib",                 # needed for plotting of Penner easing functions
   "mediapipe",                  # needed for "mediapipeface" controlnet model
   "npyscreen",
-  "numpy<1.24",
+  "numpy==1.24.4",
   "omegaconf",
   "opencv-python",
   "picklescan",
   "pillow",
   "prompt-toolkit",
-  "pympler==1.0.1",
+  "pydantic==1.10.10",
+  "pympler~=1.0.1",
   "pypatchmatch",
   'pyperclip',
   "pyreadline3",
-  "python-multipart==0.0.6",
-  "pytorch-lightning==1.7.7",
+  "python-multipart",
+  "pytorch-lightning",
   "realesrgan",
-  "requests==2.28.2",
+  "requests~=2.28.2",
   "rich~=13.3",
   "safetensors~=0.3.0",
-  "scikit-image>=0.19",
+  "scikit-image~=0.21.0",
   "send2trash",
-  "test-tube>=0.7.5",
-  "torch~=2.0.0",
-  "torchvision>=0.14.1",
-  "torchmetrics==0.11.4",
-  "torchsde==0.2.5",
+  "test-tube~=0.7.5",
+  "torch~=2.0.1",
+  "torchvision~=0.15.2",
+  "torchmetrics~=1.0.1",
+  "torchsde~=0.2.5",
   "transformers~=4.31.0",
-  "uvicorn[standard]==0.21.1",
+  "uvicorn[standard]~=0.21.1",
   "windows-curses; sys_platform=='win32'",
 ]
 
 [project.optional-dependencies]
 "dist" = ["pip-tools", "pipdeptree", "twine"]
 "docs" = [
   "mkdocs-material<9.0",
   "mkdocs-git-revision-date-localized-plugin",
   "mkdocs-redirects==1.2.0",
 ]
 "dev" = [
   "pudb",
 ]
-"test" = ["pytest>6.0.0", "pytest-cov"]
+"test" = ["pytest>6.0.0", "pytest-cov", "black"]
 "xformers" = [
 	   "xformers~=0.0.19; sys_platform!='darwin'",
 	   "triton; sys_platform=='linux'",
 ]
 
 [project.scripts]
 
@@ -172,9 +172,12 @@
 pretty_print = true
 [tool.coverage.html]
 directory = "coverage/html"
 [tool.coverage.xml]
 output = "coverage/index.xml"
 #=== End: PyTest and Coverage
 
-[flake8]
+[tool.flake8]
 max-line-length = 120
+
+[tool.black]
+line-length = 120
```

